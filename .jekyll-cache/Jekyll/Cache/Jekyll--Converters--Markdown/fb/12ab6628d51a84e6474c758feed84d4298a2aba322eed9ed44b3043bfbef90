I"Ê#<p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<p>åœ¨é€²å…¥ Text clustering å‰æˆ‘æƒ³è¦å…ˆä»‹ç´¹ <strong>Clustering</strong>ã€‚</p>

<h2 id="clustring">Clustring</h2>

<ul>
  <li>
    <p>Clustering (åˆ†ç¾¤ï¼‰æ˜¯ä¸€ç¨® unsupervised learningï¼ˆéç›£ç£å­¸ç¿’ï¼‰</p>
  </li>
  <li>Typical applications
    <ol>
      <li>As a stand-alone tool to get insight into data distribution</li>
      <li>As a preprocessing step for other algorithms</li>
    </ol>
  </li>
  <li>Cluster: a collection of data objectsã€‚ä½†ä»€éº¼æ¨£çš„ data æœƒè¢«æ­¸åœ¨åŒä¸€å€‹ cluster å‘¢ï¼Ÿ é€™å°±æ˜¯ clustering æœ€é‡è¦çš„æ¦‚å¿µï¼Œ<strong>Similar</strong> å’Œ <strong>Dissimilar</strong>ã€‚</li>
  <li><strong>Similar</strong> to one another within the same cluster</li>
  <li><strong>Dissimilar</strong> to the objects in other clusters</li>
</ul>

<p>=&gt; è€Œæåˆ° Similar å’Œ Dissimilar å°±è¦å¿…é ˆè¦æåˆ° distance (or similarity) measuresã€‚æœ‰äº† distanceï¼ˆsimilarityï¼‰æ‰æœ‰è¾¦æ³•å®šç¾© Similar å’Œ Dissimilarã€‚</p>

<h3 id="distancesimilarity">Distanceï¼ˆSimilarityï¼‰</h3>

<ul>
  <li>Distances are normally used to measure the similarity or dissimilarity between two data objects</li>
</ul>

<p>Distances å¯ä»¥æœ‰å¾ˆå¤šç¨®å®šç¾©æ–¹å¼ï¼Œä½†ä¸ç®¡æ€éº¼å®šç¾©ï¼Œéƒ½ä¸€å®šè¦ç¬¦åˆä»¥ä¸‹çš„ propertiesã€‚</p>
<ul>
  <li>\(d(i, j) \ge 0\) (non-negativity)</li>
  <li>\(d(i, i) = 0\) (identity of indiscernibles)</li>
  <li>\(d(i, j) = d(j, i)\) (symmetry)</li>
  <li>\(d(i, j) \le d(i, k) + d(k, j)\) (triangle inequality)</li>
</ul>

<p><strong>Minkowski distance</strong></p>

<p>\(d(i, j) = \sqrt[q]{(|x_{i_1} - x_{j_1}|^q + |x_{i_2} - x_{j2}|^q + \ldots + |x_{i_p} - x_{j_p}|^q)}\)
, q is a positive integer</p>

<ul>
  <li>If q = 1, d is Manhattan distance</li>
  <li>If q = 2, d is Euclidean distance</li>
</ul>

<p><strong>Binary Variables</strong></p>

<ul>
  <li>symmetric binary variables: both states are equally important; 0/1</li>
  <li>asymmetric binary variables: one state is more important than the other (e.g. outcome of disease test); 1 is the important state, 0 the other</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Contingency%20tables%20for%20Binary%20Variables.png" alt="PMF_two coins.png" /></p>

<ul>
  <li>Accuracy for symmetric binary variables</li>
</ul>

\[sim(i, j) = \frac{a+d}{a+b+c+d}\]

<ul>
  <li>Jaccard similarity for asymmetric binary variables</li>
</ul>

\[sim(i, j) = \frac{a}{a+b+c}\]

<hr />

<h2 id="text-clustering">Text clustering</h2>

<p><img src="http://localhost:4000/blog/img/posts/Conceptual%20framework%20for%20text%20mining.png" alt="PMF_two coins.png" /></p>

<p>ä¸Šåœ–æ˜¯ Conceptual framework for text miningï¼Œè€Œé€™ç¯‡è¦ä»‹ç´¹çš„æ˜¯ Clustering å’Œ Topic Analysisã€‚</p>

<ul>
  <li>
    <p><strong>Text clustering</strong> is the task of grouping similar texts together. What is considered â€˜similarâ€™ depends on the application.</p>
  </li>
  <li>
    <p>Clustering is a central tool in exploratory data analysis, where it can help us to get insights into the distribution of a data set.</p>
  </li>
  <li>
    <p>Clustering is also useful as a pre-processing technique in knowledge-focused applications. Example: Brown clustering</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/The%20standard%20text%20clustering%20pipeline.png" alt="PMF_two coins.png" /></p>

<hr />

<h2 id="similarity-measures">Similarity measures</h2>

<p>å‰›å‰›å‰é¢æåˆ°çš„ Similarityï¼Œå¦‚ä½•ç”¨åœ¨ text ä¸Šå‘¢ï¼Ÿ</p>

<p><strong>Accuracy for symmetric binary vectors</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Accuracy%20for%20symmetric%20binary%20vectors.png" alt="PMF_two coins.png" /></p>

<p><strong>Jaccard similarity for asymmetric binary vectors</strong></p>

<hr />

<h2 id="hard-clustering-and-soft-clustering">Hard clustering and soft clustering</h2>

<p><strong>Hard clustering</strong></p>
<ul>
  <li>Each document either belongs to a cluster or not. Ex. Hierarchical clustering(brown clustring), Partitioning clustering(k-means), Density-Based clustering(DBSCAN)</li>
</ul>

<p><strong>Soft clustering</strong></p>
<ul>
  <li>Each document belongs to each cluster to a certain degree. Ex. LDA (topic model)</li>
</ul>

<hr />

<h2 id="an-overview-of-hard-clustering-methods">An overview of hard clustering methods</h2>

<h3 id="hierarchical-clustering">Hierarchical clustering</h3>

<p>This method does not require the number of clusters k as an input, but needs a termination condition</p>

<p><img src="http://localhost:4000/blog/img/posts/Hierarchical%20Clustering.png" alt="" /></p>

<ul>
  <li>
    <p><strong>Agglomerative</strong>: Each document starts in its own cluster. Hierarchy is created by merging pairs of clusters.
å°‡é»å€†å€†åˆä½µï¼Œæœ€å¾Œæ‰€æœ‰çš„é»æœƒå…¨éƒ¨åœ¨åŒä¸€å€‹ cluster è£¡ã€‚é‚£è‡³æ–¼è¦ç”¨ä»€éº¼æ±ºå®šè¦å°‡å…©å€‹é»åˆä½µå‘¢ï¼Ÿé€™æ™‚å€™åˆæœ‰ Linkage criteria ä¾†æ±ºå®šï¼Œeg. Single-link, Complete-link, Average-link ç­‰ç­‰ã€‚ä¸åŒçš„ linkage criteria æœƒå°è‡´ä¸åŒçš„åˆ†ç¾¤çµæœï¼Œå¦‚ä¸‹åœ–ã€‚</p>
  </li>
  <li>
    <p><strong>Divisive clustering</strong>: All documents start in one cluster. Hierarchy is created by splitting clusters recursively.</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Linkage%20criteria.png" alt="" /></p>

<ul>
  <li><strong>Brown clustring</strong></li>
</ul>

<h3 id="partitioning-clustering">Partitioning clustering</h3>
<ul>
  <li>
    <p><strong>K-means</strong></p>
  </li>
  <li>
    <p>Issues with the k-means algorithm</p>
    <ol>
      <li>The k-means algorithm always converges, but there is no guarantee that it finds a global optimum. (Solution: random restarts)</li>
      <li>The number of clusters needs to be specified in advance, or chosen based on heuristics and cross-validation. (Example: elbow method)</li>
      <li>The k-means algorithm is not good at handling outliers â€“ every document will eventually belong to some cluster.</li>
      <li>K-means is restricted to clusters with convex shapes =&gt; Density-Based clustering</li>
    </ol>
  </li>
</ul>

<h3 id="density-based-clustering">Density-Based clustering</h3>

<ul>
  <li>The basic idea behind density-based algorithms is that different regions of the vector space can be more or less densely populated.</li>
  <li>Under this view, clusters can take any shape; they are not constrained to convex clusters as in k-means.</li>
</ul>

<p><strong>Directly density-reachable</strong></p>

<ul>
  <li>DBSCAN</li>
</ul>

<hr />

<h2 id="evaluation-of-hard-clustering">Evaluation of hard clustering</h2>

<h3 id="intrinsic-and-extrinsic-evaluation">Intrinsic and extrinsic evaluation</h3>
<ul>
  <li>In <strong>intrinsic</strong> evaluation, a clustering is evaluated based on internal measures such as coherence and separation. <em>Are documents in the same cluster similar? Are clusters well-separated?</em></li>
  <li>In <strong>extrinsic</strong> evaluation, a clustering is evaluated based on data that was not used for the clustering, such as known class labels. <em>cluster purity, Rand index</em></li>
</ul>

<h3 id="rand-index">Rand index</h3>

<p>å‡è¨­ä¸€å€‹é›†åˆä¸­æœ‰Nç¯‡æ–‡ç« 
ä¸€å€‹é›†åˆä¸­æœ‰N(N-1)/2å€‹é›†åˆå°
TPï¼šåŒä¸€é¡çš„æ–‡ç« è¢«åˆ†åˆ°åŒä¸€å€‹ç°‡
TNï¼šä¸åŒé¡çš„æ–‡ç« è¢«åˆ†åˆ°ä¸åŒç°‡
FPï¼šä¸åŒé¡çš„æ–‡ç« è¢«åˆ†åˆ°åŒä¸€å€‹ç°‡
FNï¼šåŒä¸€é¡çš„æ–‡ç« è¢«åˆ†åˆ°ä¸åŒç°‡
Rand Indexåº¦é‡çš„æ­£ç¢ºçš„ç™¾åˆ†æ¯”
RI = ï¼ˆTP+TNï¼‰/ï¼ˆTP+FP+FN+TNï¼‰</p>

<hr />

<h2 id="topic-models">Topic models</h2>
<ul>
  <li>A topic model is a statistical model for representing the abstract topics that are expressed in a collection of documents.</li>
  <li>Topic models are examples of soft clustering techniques â€“ each document belongs to each cluster (topic) to a certain degree.</li>
</ul>

<h3 id="latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</h3>

<p>LDAæœ‰å…©å€‹åŸå‰‡ï¼Œ</p>
<ol>
  <li>æ¯å€‹ domcuments æ˜¯ç”±å¤šå€‹ Topic çµ„æˆï¼ˆeach document belongs to each cluster (topic) to a certain degreeï¼‰</li>
  <li>æ¯å€‹ä¸»é¡Œæœƒæœ‰ä¸åŒçš„ terms ä¾†æè¿°ï¼Œä¸”åŒæ¨£å°è©å¯ä»¥åŒæ™‚å‡ºç¾åœ¨ä¸åŒçš„ä¸»é¡Œã€‚</li>
</ol>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models.png" alt="" /></p>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models2.png" alt="" /></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://www.ida.liu.se/~732A75/info/courseinfo.en.shtml">732A75 Advanced Data Mining</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/238079-natural-language-processing-brown-clustering">è‡ªç„¶èªè¨€è™•ç† â€“ Brown Clustering</a>
<br />
<a href="https://medium.com/@tengyuanchang/ç›´è§€ç†è§£-lda-latent-dirichlet-allocation-èˆ‡æ–‡ä»¶ä¸»é¡Œæ¨¡å‹-ab4f26c27184">ç›´è§€ç†è§£ LDA (Latent Dirichlet Allocation) èˆ‡æ–‡ä»¶ä¸»é¡Œæ¨¡å‹</a>
<br />
<a href="https://www.twblogs.net/a/5b8c83e12b717718833363ae">èšé¡è©•åƒ¹æŒ‡æ¨™ Rand Index,RI,Recall,Precision,F1</a></p>
:ET