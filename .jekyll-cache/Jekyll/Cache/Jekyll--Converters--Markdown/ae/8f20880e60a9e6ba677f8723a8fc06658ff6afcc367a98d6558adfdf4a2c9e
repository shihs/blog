I"¦<p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="text-classification">Text classification</h2>

<ul>
  <li>Text classification is the task of categorising text documents into predefined classes.</li>
</ul>

<h2 id="evaluation-of-text-classifiers">Evaluation of text classifiers</h2>

<p>æœ€ç°¡å–®çš„æª¢è¦–é æ¸¬çµæœå¥½å£çš„æ–¹æ³•å°±æ˜¯å°‡ predict å‡ºä¾†çš„é¡åˆ¥èˆ‡çœŸå¯¦çš„é¡åˆ¥åšæ¯”è¼ƒã€‚ï¼ˆé æ¸¬å¿…é ˆè¦åœ¨ test dataï¼Œæ›å¥è©±èªªä¹‹å‰ä¸¦æ²’æœ‰åƒèˆ‡ä»»ä½• training çš„éç¨‹ã€‚é€™é»åœ¨åšæ‰€æœ‰ Machine Learning çš„æ–¹æ³•éƒ½å¾ˆé‡è¦ï¼Œåœ¨åš model æ¸¬è©¦å‰ä¸è¦ç¢° test dataã€‚ï¼‰</p>

<h3 id="accuracy">Accuracy</h3>
<p>The accuracy of a classifier is the proportion of documents for which the classifier predicts the gold-standard class:</p>

\[\textrm{accuarcy} = \frac{\textrm{number of correctly classified documents}}{\textrm{number of all documents}}\]

<h3 id="accuracy-and-imbalanced-data-sets">Accuracy and imbalanced data sets</h3>

<p>ä¸Šé¢çš„ accuracy çœ‹èµ·ä¾†éå¸¸åˆç†å•Šï¼Œå»è¨ˆç®—åˆ†é¡æ­£ç¢ºçš„æ¯”ä¾‹ä¾†åˆ¤æ–·é€™å€‹åˆ†é¡å™¨æ˜¯å¦é æ¸¬æº–ç¢ºã€‚ä½†ï¼Œå¦‚æœå…¶å¯¦è³‡æ–™æœ¬èº«çš„é¡åˆ¥ä¸¦ä¸å¹³å‡å‘¢ï¼Ÿ</p>

<p><img src="http://localhost:4000/blog/img/posts/imbalanced%20data%20sets.png" alt="PMF_two coins.png" /></p>

<p>æ ¹æ“šä¸Šåœ–ï¼Œæˆ‘å€‘åªè¦æŠŠæ‰€æœ‰è³‡æ–™éƒ½çŒœç¶ è‰²çš„ classï¼Œé€™æ¨£ accuracy å°±èƒ½æœ‰ 80%ã€‚å¾é€™å€‹ä¾‹å­å¯ä»¥çŸ¥é“ï¼Œ</p>

<ul>
  <li>
    <p>Evaluation measures are no absolute measures of performance. å¦‚æœä»Šå¤©å¾—åˆ° accuracy æ˜¯ 80% æˆ‘å€‘ä¸¦ç„¡æ³•ç¢ºå®šé€™æ¨£çš„æº–ç¢ºç‡çš„å¥½å£ï¼Œè¦æ ¹æ“šæ¯å€‹å•é¡Œå»åˆ¤æ–·ã€‚</p>
  </li>
  <li>
    <p>Instead, we should ask for a classifierâ€™s performance relative to other classifiers, or other points of comparison. E.g.â€™Logistic Regression has a higher accuracy than Naive Bayes.â€™</p>
  </li>
  <li>
    <p>When other classifiers are not available, a simple baseline is to always predict the most frequent class in the training data.</p>
  </li>
</ul>

<h2 id="precision-and-recall">Precision and recall</h2>

<ul>
  <li>
    <p>Precision and recall â€˜zoom inâ€™ on how good a system is at identifying documents of a specific class.</p>
  </li>
  <li>
    <p>Precision is the proportion of correctly classified documents among all documents for which the system predicts class.</p>
  </li>
</ul>

\[\textrm{precision} = \frac{\textrm{# true positives}}{\textrm{# true positives + # false positives}}\]

<ul>
  <li>Recall is the proportion of correctly classified documents among all documents with gold-standard class.</li>
</ul>

\[\textrm{recall} = \frac{\textrm{# true positives}}{\textrm{# true positives + # false negatives}}\]

<h2 id="f1-measure">F1-measure</h2>
<p>A good classifier should balance between precision and recall. â€¨</p>

\[\textrm{F1} = \frac{2 \cdot \textrm{precision} \cdot \textrm{recall}}{\textrm{precision + recall}}\]

<hr />

<h2 id="naive-bayes-classifier">Naive Bayes classifier</h2>

<h3 id="bayes-theorem">Bayesâ€™ theorem</h3>

<p>We know that \(C\) is classes, \(x_i, i = 1, \cdots,n\) is features. Using Bayesâ€™ theorem, the conditional probability can be decomposed as</p>

\[p(C|x_1,..., x_n) = \frac{p(C)~p(x_1, \cdots,x_n|C)}{p(x_1, \cdots,x_n)}\]

<p>ä¹Ÿå°±æ˜¯ï¼Œ</p>

\[\textrm{posterior} = \frac{\textrm{prior} \times \textrm{likelihood}}{\textrm{evidence}}\]

<p>æ ¹æ“šä¸Šå¼ï¼Œæˆ‘å€‘å¯ä»¥å°‡åˆ†æ¯è¦–ç‚ºå¸¸æ•¸ï¼Œå› ç‚º features \(x_i, i = 1, \cdots,n\) çš„å€¼æ˜¯çµ¦å®šçš„ï¼Œä¸”èˆ‡ \(C\) ç„¡é—œï¼Œæ‰€ä»¥å¯ä»¥å¾—åˆ°</p>

\[\begin{align}
p(C, x_1, \cdots,x_n) &amp; = p(C)~p(x_1,\cdots,x_n|C) \\
&amp; \propto p(C)~p(x_1|C)~p(x_2,\cdots,x_n|C,x_1) \\
&amp; \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3,\cdots,x_n|C,x_1,x_2) \\ 
&amp; \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3|C, x_1, x_2)~p(x_4,\cdots,x_n|C,x_1,x_2,x_3) \\
&amp; \propto \cdots\\
&amp; \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3|C, x_1, x_2) \cdots p(x_n|C,x_1,x_2,\cdots ,x_n) \\
\end{align}\]

<h3 id="naive-bayes-assumption">Naive Bayes assumption</h3>

<p>Naive Bayes å‡è¨­ 
\(p(x_i|C, x_j) = p(x_i|C), \textrm{for} ~ i \ne j\)</p>

<p>æ ¹æ“š Naive Bayes çš„å‡è¨­ï¼Œå‰é¢çš„å¼å­æˆ‘å€‘å¯ä»¥å¯«æˆï¼Œ</p>

\[\begin{align}
p(C|x_1,..., x_n) &amp; \propto p(C, x_1, \cdots,x_n) \\
&amp; \propto p(C)~p(x_1|C)~p(x_2|C)~p(x_3|C) \cdots p(x_n|C) \\
&amp; \propto p(C)~\prod_{i=1}^n p(x_i|C)
\end{align}\]

<p>æ ¹æ“šä¸Šé¢çš„æ¨å°éç¨‹ï¼Œæˆ‘å€‘å¯ä»¥å¾—åˆ°ï¼Œ</p>

\[p(C|x_1,..., x_n) = \frac{1}{Z}~p(C)~\prod_{i=1}^n p(x_i|C)\]

<h3 id="naive-bayes-classifer">Naive Bayes classifer</h3>

<p>è€Œ Naive Bayes classifer å°±æ˜¯å–å„å€‹åˆ†é¡ \(C_m, m = 1, \cdots, k\) 
ä¸­ \(p(C|x_1,..., x_n)\) å€¼æœ€å¤§çš„ç‚ºæœ€å¾Œçš„åˆ†é¡çµæœã€‚æ›å¥è©±èªªï¼Œæˆ‘å€‘å¯ç”¨é€™æ¨£çš„å¼å­è¡¨ç¤º</p>

\[C_m = \mathop{\arg\max}_C p(C|x_1,..., x_n) =  \mathop{\arg\max}_C p(C)~\prod_{i=1}^n p(x_i|C)\]

<p>è€Œ \(C_m\) å°±æ˜¯æœ€å¾Œçš„åˆ†é¡çµæœã€‚</p>

<h3 id="two-classic-naive-bayes-variants-for-text">Two Classic Naive Bayes Variants for Text</h3>

<ol>
  <li>Multinomial Naive Bayes
    <ul>
      <li>Data follows a multinomial distribution (å¤šé …åˆ†å¸ƒ)</li>
      <li>Each feature values is a count (word occurrence counts, TF-IDF weighting, â€¦)</li>
    </ul>
  </li>
  <li>Bernoulli Naive Bayes
    <ul>
      <li>Data follows a multivariate Bernoulli distribution</li>
      <li>Each feature is binary (word is present / absent)</li>
    </ul>
  </li>
</ol>

<hr />

<p><strong>Lab:</strong> <a href="https://github.com/shihs/732A92-TextMining/blob/master/Lab1/TM-L2.ipynb">Text classification Lab</a></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://www.ycc.idv.tw/confusion-matrix.html">å¦‚ä½•è¾¨åˆ¥æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„å¥½å£ï¼Ÿç§’æ‡‚Confusion Matrix</a>
<br />
<a href="https://zh.wikipedia.org/wiki/æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨">wikipeida - å–®ç´”è²æ°åˆ†é¡å™¨</a></p>

:ET