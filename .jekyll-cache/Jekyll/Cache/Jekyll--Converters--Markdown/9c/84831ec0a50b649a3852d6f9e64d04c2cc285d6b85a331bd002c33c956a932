I"’<p>é€™ä¸€ç³»åˆ—æˆ‘æ˜¯ä»¥ 
Wackerley, Mendenhall and Scheaffer çš„ <br />
Mathematical Statistics with Applications, 7th edition é€™æœ¬æ›¸ç‚ºä¸»ï¼Œ<br />
æœ‰äº›åè©èˆ‡å®šç†è§£é‡‹æœƒå†åƒè€ƒå…¶ä»–æ›¸ç±èˆ‡ç¶²è·¯ä½œç‚ºè¼”åŠ©ï¼ˆä¸‹æ–¹æœ‰åƒè€ƒé€£çµï¼‰ã€‚</p>

<p>é€™ç¯‡é—œæ–¼æœŸæœ›å€¼çš„ä»‹ç´¹åœ¨æ›¸ä¸­çš„ 3.3ï¼Œä½†é€™ç¯‡å…§å®¹æœ‰å¾ˆå¤§éƒ¨åˆ†ä¹Ÿä¾†è‡ª <a href="https://www.probabilitycourse.com/chapter3/3_2_2_expectation.php">Source</a></p>

<hr />
<h3 id="æœŸæœ›å€¼-expectation">æœŸæœ›å€¼ Expectation</h3>
<p><strong>å®šç¾© Definition</strong></p>

<ul>
  <li>Let \(X\) be a discrete random variable with range \(R_X = \{ x_1, x_2, x_3,â€¦ \} \)(finite or countably infinite). The expectation of \(X\) is defined as</li>
</ul>

\[E(X) = \sum_{\substack{x_k \in R_X}}x_kP(X=x_k)=\sum_{\substack{x_k \in R_X}}x_kP_X(x_k)\]

<p>ä¸”æœŸæœ›å€¼ \(E(X)\) æœƒç­‰æ–¼å¹³å‡æ•¸ \(\mu\)</p>

\[E(X) = \mu\]

<p>èªªæ˜ï¼š</p>

<blockquote>

  <p>There is a discrete random variable with range \(R_X = \{ x_1, x_2, x_3,â€¦ \} \). Suppose that we repeat this experiment a very large number of times \(N\), and that the trials are independent. Let \(N_1\) be the number of times we observe \(x_1\), \(N_2\) be the number of times we observe \(x_2, â€¦.\), \(N_k\) be the number of times we observe \(x_k\).</p>
</blockquote>

\[P_X(x_1) \approx\frac{N_1}{N} \\
P_X(x_2) \approx\frac{N_2}{N} \\
\cdots\\
P_X(x_k) \approx\frac{N_k}{N} \\
\cdots\]

<blockquote>

  <p>å¾ä»¥ä¸Šå¼å­æˆ‘å€‘å¯ä»¥å¾—åˆ° \(N_k = NP_X(x_k)\)</p>
</blockquote>

\[\begin{align}
\mu &amp; = \frac{N_1x_1 + N_2x_2 + N_3x_3 + ...}{N} \\
&amp; \approx \frac{x_1NP_X(x_1) + x_2NP_X(x_2) + x_3NP_X(x_3) + ...}{N} \\
&amp; = x_1P_X(x_1) + x_2P_X(x_2) + x_3P_X(x_3) + ... \\
&amp; = E(X)
\end{align}\]

<p>ä»¥ä¸Šå°±æ˜¯ \(E(X) = \mu\) çš„èªªæ˜ã€‚</p>

<p>å¦å¤–ï¼Œå¯èƒ½æœƒçœ‹åˆ°ä¸€äº›ä¸åŒçš„å¯«æ³•è¡¨ç¤ºæœŸæœ›å€¼</p>

\[E(X)=E[X]=EX=\mu=\mu_X\]

<p>é€™éƒ¨åˆ†å…§å®¹ä¸»è¦ä¾†æºç‚º <a href="https://www.probabilitycourse.com/chapter3/3_2_2_expectation.php">Source</a></p>

<hr />
<h3 id="law-of-the-unconscious-statistician-lotus">Law of the unconscious statistician (LOTUS)</h3>
<p><strong>THEOREM 3.2</strong></p>
<ul>
  <li>Let \(X\) be a discrete random variable with probability function \(P(x)\) and \(g(X)\) be a real-valued function of \(Y\). Then the expected value of \(g(X)\) is given by</li>
</ul>

\[E[g(X)] = \sum_{\substack{x \in R_X}}g(x)P(x)\]

<p><strong>Example:</strong> <br />
<a href="https://www.probabilitycourse.com/chapter3/3_2_3_functions_random_var.php">Source</a></p>
<blockquote>

  <p>Let \(X\) be a discrete random variable with range \(R_X = \{ 0, \frac{\pi}{4}, \frac{\pi}{2}, \frac{3\pi}{4}, \piâ€¦ \} \), such that \(P_X(0)=P_X(\frac{\pi}{4})=P_X(\frac{\pi}{2})=P_X(\frac{3\pi}{4})=P_X(\pi)=\frac{1}{5}\).  Find \(E[sin(X)]\).</p>
</blockquote>

<p>Solution:</p>

<blockquote>

  <p>æ ¹æ“š LOTUSï¼Œ
\(\begin{align}
E[\sin(X)] &amp; = \sum_{\substack{x \in R_X}}\sin(x)P(x) \\
&amp; = \sin(0)P_X(0) +  \sin(\frac{\pi}{4})P_X(\frac{\pi}{4}) + \sin(\frac{\pi}{2})P_X(\frac{\pi}{2}) + \sin(\frac{3\pi}{4})P_X(\frac{3\pi}{4}) + \sin(\pi)P_X(\pi)\\
&amp; = 0 \times \frac{1}{5} + \frac{\sqrt{2}}{2} \times \frac{1}{5} + 1 \times \frac{1}{5} + \frac{\sqrt{2}}{2} \times \frac{1}{5} + 0 \times \frac{1}{5} \\
&amp; = \frac{\sqrt{2} + 1}{2}
\end{align}\)</p>
</blockquote>

<hr />
<h3 id="è®Šç•°æ•¸-variance">è®Šç•°æ•¸ Variance</h3>
<p><strong>å®šç¾© Definition</strong></p>

<ul>
  <li>If \(X\) is a random variable with mean \(E[X] = \mu\), the variance of \(X\) is defined as</li>
</ul>

\[Var(X) = E[(X - \mu)^2]\]

<ul>
  <li>ä¸”æ ¹æ“š LOTUSï¼Œé€™å€‹å¼å­å¯ä»¥å†å¯«æˆ</li>
</ul>

\[Var(X) = E[(X - \mu)^2] = \sum_{\substack{x \in R_X}}(x - \mu)^2P(x)\]

<hr />
<h3 id="æœŸæœ›å€¼çš„é‹ç®—">æœŸæœ›å€¼çš„é‹ç®—</h3>
<p><strong>THEOREM 3.3</strong></p>

<ul>
  <li>Let X be a discrete random variable with probability function \(p(x)\) and \(c\) be a constant. Then</li>
</ul>

\[E(c) = c.\]

<p><strong>THEOREM 3.4</strong></p>
<ul>
  <li>Let X be a discrete random variable with probability function \(p(x)\) and \(g(X)\) be a function of X, and c be as constant. Then</li>
</ul>

\[E[cg(X)] = cE[g(X)].\]

<p><strong>THEOREM 3.5</strong></p>
<ul>
  <li>Let X be a discrete random variable with probability function \(p(x)\) and \(g_1(X), g_2(X), â€¦, g_k(X)\) be \(k\) functions of \(X\). Then</li>
</ul>

\[E[g_1(X)+g_2(X)+...+g_k(X)] = E[g_1(X)] + E[g_2(X)] + ... + E[g_k(X)]\]

<p>æ ¹æ“šä»¥ä¸Šå®šç†ï¼Œå¯ä»¥å†ç”¢ç”Ÿ</p>

<p><strong>THEOREM 3.6</strong></p>
<ul>
  <li>Let X be a discrete random variable with probability function \(p(x)\) and mean \(E(X) = \mu\). Then</li>
</ul>

\[\begin{align}
V(X) = \sigma^2 &amp; = E[(X - \mu)^2] \\
&amp; = E(X^2) - \mu^2 \\
&amp; = E(X^2) - E^2(X) \\
\end{align}\]

<p>è­‰æ˜ï¼š</p>

<blockquote>

\[\begin{align}
\sigma^2 &amp; = E[(X - \mu)^2] &amp; \\
&amp; = E[(X^2 - 2\mu X + \mu^2] \\
&amp; = E(X^2) - E(2\mu X) + E(\mu^2) \\
&amp; = E(X^2) - 2\mu E(X) + \mu^2 \\
&amp; = E(X^2) - 2\mu^2 + \mu^2 \\
&amp; = E(X^2) - \mu^2 \\
&amp; = E(X^2) - E^2(X) \\
\end{align}\]
</blockquote>

<hr />
<p>åƒè€ƒï¼š</p>

<ul>
  <li><a href="https://www.amazon.com/Mathematical-Statistics-Applications-Dennis-Wackerly/dp/0495110817">Mathematical Statistics with Applications 7th Edition</a></li>
  <li><a href="https://www.probabilitycourse.com">Introduce To Proability, Statistics and Random Process</a></li>
  <li><a href="http://ccckmit.wikidot.com/st:main">é™³é¾èª çš„ç¶²ç«™</a></li>
</ul>

:ET