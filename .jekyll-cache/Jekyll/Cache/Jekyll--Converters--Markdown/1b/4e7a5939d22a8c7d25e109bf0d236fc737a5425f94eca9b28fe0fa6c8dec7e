I"ˆ<p><a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">ä¸Šä¸€ç¯‡</a>èªªæ˜äº†ç¥ç¶“ç¶²è·¯çš„åŸºæœ¬åŸç†ï¼Œé€™ç¯‡æˆ‘å€‘è¦è¬›è§£ç¥ç¶“ç¶²è·¯æ˜¯å¦‚ä½•å­¸ç¿’çš„ - Gradient descentã€‚</p>

<p>é€™ç¯‡æ˜¯<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">Neural networks</a>çš„èª²ç¨‹ç­†è¨˜ã€‚</p>

<hr />

<p>åœ¨ Machine Learning è£¡æˆ‘å€‘ä½¿ç”¨ training data è¨“ç·´å¾—åˆ°ä¸€å€‹ modelï¼Œä½†é€™ã€Œè¨“ç·´ã€åˆ°åº•æ˜¯æ€éº¼è¨“ç·´çš„å‘¢ï¼Ÿè¨“ç·´çš„éç¨‹ï¼Œé€šå¸¸æ˜¯è¦æ‰¾åˆ°æœ€å°çš„ costï¼Œä¹Ÿå°±æ˜¯æœ€å¾Œçš„çµæœèˆ‡çœŸå¯¦å€¼å·®è·è¶Šå°è¶Šå¥½ã€‚</p>

<p>åœ¨ Neural Network æˆ‘å€‘è¦è¨“ç·´çš„è®Šæ•¸å°±æ˜¯<a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">å‰é¢ä¸€ç¯‡</a>æåˆ°çš„ weight å’Œ biasã€‚ä¸€é–‹å§‹æˆ‘å€‘éš¨æ©Ÿçš„çµ¦äºˆé€™äº›æ•¸å­—ï¼Œç„¶å¾Œåœ¨ä¸€æ¬¡æ¬¡çš„å­¸ç¿’ä¸­å»æ”¹é€² weight å’Œ bias çš„å€¼ï¼Œæ‰¾åˆ°æœ€å°çš„æˆæœ¬å‡½æ•¸ã€‚</p>

<p>åœ¨èª²ç¨‹çš„ä¾‹å­è£¡ï¼ŒNeural Network function çš„ input æ˜¯ 784 å€‹ pixel çš„æ•¸å­—ï¼Œoutput æ˜¯ 10 å€‹æ•¸å­—ï¼Œ0 åˆ° 9 çš„æ©Ÿç‡ï¼Œparameters æ˜¯ weight å’Œ biasã€‚ä½†åœ¨cost function ä¸­ï¼Œweight å’Œ bias è®Šæˆäº† inputï¼Œè€Œ output å°±æ˜¯ cost çš„å€¼ï¼Œparameters å‰‡æ˜¯å¾ˆå¤šçš„ training pairsã€‚</p>

<p>æˆ‘å€‘å¯ä»¥æŠŠ cost function é€™æ¨£è¡¨ç¤ºï¼Œ</p>

\[C(w_1, w_2,..., b_1, b_2, ....) = cost\]

<p>cost çš„ç®—æ³•æ˜¯ Neural Network ç®—å‡ºä¾†çš„å€¼ï¼Œæ‰£æ‰æ­£ç¢ºå€¼ç„¶å¾Œå¹³æ–¹ç›¸åŠ ã€‚å­¸ç¿’éç¨‹ä¸­ï¼Œæˆ‘å€‘å°‡æ‰€æœ‰ training data çš„ cost ç›¸åŠ å¹³å‡ï¼ˆaverage cost of all training dataï¼‰ï¼Œå¸Œæœ›é€™å€‹å€¼è¶Šå°è¶Šå¥½ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/cost.png" alt="" /></p>

<p>é‚£æˆ‘å€‘è¦å¦‚ä½•æ‰¾åˆ°æœ€å°çš„ cost å€¼ï¼Ÿ</p>

<p>å¦‚æœ cost function æ˜¯ä¸€å€‹åƒä¸‹åœ–çš„å‡½æ•¸åœ–å½¢ï¼Œæˆ‘å€‘çŸ¥é“å¾®åˆ†ç­‰æ–¼ 0 å¯ä»¥æ‰¾åˆ°æœ€å°å€¼ï¼Œä½†é€™ä¸¦éä¸€ä»¶å®¹æ˜“çš„äº‹ï¼Œå°¤å…¶æ˜¯ç•¶æˆ‘å€‘æœ‰å¾ˆå¤šè®Šæ•¸æ™‚ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima.png" alt="" /></p>

<p>æ‰€ä»¥æˆ‘å€‘å¯ä»¥ç”¨çš„æ–¹æ³•æ˜¯ï¼Œå…ˆéš¨æ©Ÿåˆ°å‡½æ•¸åœ–å½¢ä¸Šçš„ä¸€é»ï¼Œç„¶å¾Œæ ¹æ“šè©²é»çš„åˆ‡ç·šæ–œç‡ä¾†æ±ºå®šè©²å¾€å“ªå€‹æ–¹å‘ç§»å‹•å¯ä»¥æ‰¾åˆ°æœ€ä½é»ã€‚é€™æ¨£çš„æ–¹æ³•ï¼Œå¯ä»¥è®“æˆ‘å€‘åˆ° local minimumï¼Œæˆ‘å€‘ç„¡æ³•ç¢ºèªæ‰¾åˆ°çš„æœ€å°æ˜¯å¦æ˜¯ global minimumï¼Œå› ç‚ºé€™å–æ±ºèˆ‡ä¸€é–‹å§‹çš„ initial valueã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima%202.png" alt="" /></p>

<hr />

<p>åœ¨é€²å…¥ Gradient descent ä¹‹å‰ï¼Œæˆ‘æƒ³è¦å…ˆä»‹ç´¹ä¸€ä¸‹åŸºç¤è§€å¿µã€‚ä»¥ä¸‹å°‡æœƒè¬›è§£ï¼Œ</p>

<ol>
  <li>å°å‡½æ•¸ Derivative function èˆ‡å°æ•¸ Derivative</li>
  <li>æ–¹å‘å°æ•¸ Directional derivative</li>
  <li>åå°æ•¸ Partial derivative</li>
  <li>æ¢¯åº¦ Gradient</li>
</ol>

<h3 id="å°æ•¸-derivative">å°æ•¸ Derivative</h3>

<p>åˆ‡ç·šçš„æ–œç‡ç¨±ç‚ºã€Œå°æ•¸ã€ï¼ˆDerivativeï¼‰ï¼Œä¹Ÿå°±æ˜¯å¾®åˆ†ã€‚</p>

<p>å®šç¾©æˆï¼Œ</p>

\[m = \text{lim}_{\Delta x \to 0}\frac{f(x+\Delta x) - f(x) }{\Delta x} = f'(x) = \frac{\text{d} f(x)}{\text{d}x}\]

<h3 id="åå°æ•¸-partial-derivative">åå°æ•¸ Partial derivative</h3>

<p>æ–œç‡æ±‚çš„æ˜¯å–®è®Šæ•¸çš„å¾®åˆ†ï¼Œè€Œåå°æ•¸æ˜¯åœ¨å¤šè®Šæ•¸ä¸­çš„æŸå€‹è®Šæ•¸æ–¹å‘çš„æ–œç‡ã€‚</p>

<p>ä»¥å…©å€‹è‡ªè®Šæ•¸ç‚ºä¾‹ï¼Œ \(z = f(x, y)\)</p>

<p>\(f_x(a, b)\) ç‚ºå° \(x\) çš„åå°æ•¸ï¼Œå®šç¾©ç‚º</p>

\[f_x(a, b) = \text{lim}_{h \to 0} \frac{f(a + h, b) - f(a, b)}{h}\]

<p>\(f_y(a, b)\) ç‚ºå° \(y\) çš„åå°æ•¸ï¼Œå®šç¾©ç‚º</p>

\[f_y(a, b) = \text{lim}_{k \to 0} \frac{f(a, b + k) - f(a, b)}{k}\]

<p>ä¹Ÿå°±æ˜¯å° x æˆ– y æ–¹å‘åšåå¾®åˆ†ã€‚</p>

<h3 id="æ–¹å‘å°æ•¸-directional-derivative">æ–¹å‘å°æ•¸ Directional derivative</h3>

<p>æ–œç‡æ±‚çš„æ˜¯å–®è®Šæ•¸çš„å¾®åˆ†ï¼Œåœ¨å¤šè®Šæ•¸ä¸­è¦æ±‚æ–œç‡å¿…é ˆè¦çœ‹æ˜¯å“ªå€‹æ–¹å‘çš„æ–œç‡ï¼Œå› ç‚ºå„å€‹æ–¹å‘çš„æ–œç‡éƒ½ä¸ç›¸åŒï¼Œé€™æ™‚è¦æ±‚çš„å°±æ˜¯æ–¹å‘å°æ•¸ã€‚</p>

<p>å‡è¨­å‡½æ•¸ \(z = f(x, y)\)ï¼Œåœ¨å®šç¾©åŸŸ \(xy\) å¹³é¢ä¸Šæœ‰ä¸€é» \((x, y)\) åŠå–®ä½å‘é‡ \(\overset{\rightharpoonup}{u} = (u_1, u_2)\)ã€‚å‰‡æ›²é¢ \(z = f(x, y)\) åœ¨ \((a, b)\) è™•ï¼Œæ²¿ \(\overset{\rightharpoonup}{u}\) çš„æ–¹å‘çš„æ–œç‡å®šç¾©ç‚ºï¼Œ</p>

\[\text{D}_u f(x_0, y_0)= \text{lim}_{h \to 0} \frac{f(x_0 + h u_1, y_0 + h u_2) - f(x_0, y_0)}{h} = \nabla f \boldsymbol{u}\]

<p>æ–¹å‘å°æ•¸çš„æ„ç¾©åœ¨æ–¼ï¼Œæœ‰åŠ©æ–¼è¨ˆç®—å‡ºç©ºé–“ä¸­æŸä¸€å ´é‡ \(f(x, y, z)\) åœ¨æŸä¸€ç‰¹å®šæ–¹å‘ \(\boldsymbol{u}\) ä¹‹è®ŠåŒ–ç‡ã€‚</p>

<h3 id="æ¢¯åº¦-gradient">æ¢¯åº¦ Gradient</h3>

<p>æ¢¯åº¦çš„å®šç¾©å°±æ˜¯å°å„å€‹åˆ†é‡åšåå¾®åˆ†ã€‚</p>

<p>ä¾‹å¦‚ï¼Œç¾åœ¨æ˜¯å…©è®Šæ•¸å‡½æ•¸ \(f(x, y)\)ï¼Œå…¶æ¢¯åº¦ \(\nabla f(x, y)\) å®šç¾©ç‚º</p>

\[\nabla f(x, y) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)\]

<p>è‹¥æ˜¯è®Šæ•¸æœ‰ä¸‰å€‹ï¼Œå‰‡ \(\nabla f(x, y, y)\) å®šç¾©ç‚º</p>

\[\nabla f(x, y, z) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)\]

<p>Gernal ä¾†çœ‹ï¼Œæœ‰ n å€‹è®Šæ•¸çš„å‡½æ•¸ \(f(x_1, x_2, \cdots, x_n)\)ï¼Œæˆ‘å€‘å¯ä»¥å¯«æˆ</p>

\[\nabla f(x_1, x_2, \cdots, x_n) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}\right)\]

<p>æ¢¯åº¦ Gradient çš„å¹¾ä½•æ„ç¾©ï¼Œ</p>
<ul>
  <li>\(\nabla f\) è¡¨ç¤ºæ›²é¢ \(f(x, y, z) = C\) ä¹‹å‚ç›´å‘é‡ï¼Œå¦‚ä¸‹åœ–æ‰€ç¤ºã€‚</li>
  <li>\(\nabla f(P)\) è¡¨åœ¨ \(P\) é»å¢é‡æœ€å¤§çš„æ–¹å‘ï¼Œä¹Ÿå°±æ˜¯ the direction of steepest increase</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/gradient.png" alt="" /></p>

<hr />

<h3 id="æ¢¯åº¦ä¸‹é™æ³•-gradient-descent">æ¢¯åº¦ä¸‹é™æ³• Gradient descent</h3>

<p>Gradient descent æ˜¯ä¸€ç¨®æœ€ä½³åŒ–æ–¹æ³•ã€‚</p>

<p>å¾ä¸Šä¸€éƒ¨åˆ†ï¼Œæˆ‘å€‘çŸ¥é“ P é» gradeint è¡¨ç¤ºè©²é»å¢é‡æœ€å¤§çš„æ–¹å‘ã€‚ç¾åœ¨æˆ‘å€‘è¦æ‰¾ cost function çš„æœ€å°å€¼ï¼Œä¹Ÿå°±æ˜¯è©² function æ–œç‡ç‚º 0 çš„é»ï¼Œç•¶æˆ‘å€‘ä»Šå¤©åœ¨ P é»çŸ¥é“å“ªå€‹æ–¹å‘æ˜¯æœƒå¢åŠ æœ€å¿«é€Ÿçš„é»ï¼Œé‚£å°±åªè¦å¾€åæ–¹å‘å°±æœƒæ˜¯æ¸›å°‘æœ€å¿«é€Ÿçš„æ–¹å‘äº†ã€‚ä¹Ÿå°±æ˜¯ \(-\nabla f(P)\) æ˜¯æ‰¾å°‹ minimum cost çš„æ–¹å‘ã€‚</p>

<p>ç¸½çš„ä¾†èªªï¼Œæ¢¯åº¦ä¸‹é™æ³• Gradient descent å°±æ˜¯ä¸æ–·çš„åœ¨æŸå€‹é» P ç®—å®ƒçš„ Gradientï¼Œç„¶å¾Œå¾€ Gradient çš„åæ–¹å‘ä¸€å®šï¼Œç›´åˆ°æ‰¾åˆ° local minimum ç‚ºæ­¢ï¼ˆç„¡æ³•çŸ¥é“æ˜¯å¦æ‰¾åˆ° glocal minimumï¼Œæœƒæ ¹æ“š initial values è€Œå®šï¼‰ã€‚</p>

<p>åœ¨ neural netword ä¸­ï¼Œè®“é€™å€‹æ¢¯åº¦è¨ˆç®—æ›´æœ‰æ•ˆç‡çš„æ–¹æ³•å«åš <strong>Backpropagation</strong>ï¼Œè€Œé€™æ­£æ˜¯ neural netword çš„é‡é»ï¼ä¸‹ä¸€ç¯‡ç¹¼çºŒä»‹ç´¹ :)</p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/827/mod_resource/content/33/Summary_227.pdf">æ–¹å‘å°æ•¸(Directional Derivative)ä¹‹å®šç¾©èˆ‡æ„ç¾©</a>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/826/mod_resource/content/31/Summary_226.pdf">ç´”é‡å‡½æ•¸ä¹‹æ¢¯åº¦(Gradient)</a></p>

:ET