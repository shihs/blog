I"ô<h2 id="bagging">Bagging</h2>

<p><img src="http://localhost:4000/blog/img/posts/bagging.png" alt="bagging.png" height="450px" width="500px" /></p>

<p>From: Hastie, T., Tibshirani, R. and Friedman, J. The Elements of Statistical Learning. Springer, 2009. p.285</p>

<hr />

<p><strong>Bagging æ˜¯ä»€éº¼ï¼Ÿ</strong></p>

<p>Bootstrap aggregating (bagging) is a machine learning ensemble meta-algorithm to improve classification and regression models in terms of stability and classification accuracy. It also reduces variance and helps to avoid â€˜overfittingâ€™. Although it is usually applied to decision tree models, it can be used with any type of model. (<a href="https://docs.rapidminer.com/latest/studio/operators/modeling/predictive/trees/parallel_decision_tree.html">Decision Tree - Bagging</a>)</p>

<p>ç°¡å–®ä¾†èªªï¼ŒBaggingæ˜¯ä¸€ç¨® Ensemble learningæ–¹æ³•ï¼Œå®ƒé›†çµä¸€äº› model ä¾†åšæœ€å¾Œçš„æ±ºç­–ã€‚é€šå¸¸æœƒæ˜¯é›†çµè¡¨ç¾æ²’é‚£éº¼å¥½çš„ modelï¼Œè®“é€™äº› model ä¸€èµ·åšæ±ºå®šï¼Œä¸€ç¨®ä¸‰å€‹è‡­çš®åŒ å‹éä¸€å€‹è«¸è‘›äº®çš„æ„Ÿè¦ºã€‚</p>

<p>Decision tree æ˜¯ä¸€å€‹å¸¸è¦‹çš„ weak classifierï¼Œæ‰€ä»¥å¦‚æœåœ¨ decision tree ä¸Šä½¿ç”¨ baggingï¼Œå¯ä»¥è®“æœ€å¾Œè¡¨ç¾çµæœæ¯”è¼ƒå¥½ï¼Œä¹Ÿé¿å… overfitting çš„æƒ…æ³ã€‚</p>

<hr />

<p><strong>Boostrap</strong></p>

<p>åœ¨é–‹å§‹èªªæ˜ bagging æ­¥é©Ÿå‰å…ˆä»‹ç´¹ bootstrapã€‚</p>

<p>bootstrap æ˜¯ä¸€ç¨®æŠ½æ¨£æ–¹æ³•ã€‚å‡è¨­ä»Šå¤©æœ‰ä¸€çµ„è³‡æ–™ï¼Œè£¡é ­å…±æœ‰ N å€‹æ¨£æœ¬ï¼Œæˆ‘å€‘æƒ³è¦æœ‰ m å€‹å¤§å°ç‚º N çš„æ¨£æœ¬ä½œç‚ºè¨“ç·´è³‡æ–™ã€‚æ–¹æ³•æ˜¯ï¼Œæ¯æ¬¡å¾é€™ N å€‹æ¨£æœ¬éš¨æ©ŸæŠ½å–ï¼Œä¸”æ¯æ¬¡éƒ½æ˜¯å–å¾Œæ”¾å›(ä¹Ÿå°±æ˜¯æœ‰äº›æ¨£æœ¬å¯èƒ½è¢«æŠ½åˆ°ä¸€æ¬¡ä»¥ä¸Šï¼Œæœ‰äº›æ¨£æœ¬å¯èƒ½æ²’è¢«æŠ½åˆ°)ï¼ŒåŒæ¨£çš„æ–¹æ³•é‡è¤‡ m æ¬¡ï¼Œé€™æ¨£æˆ‘å€‘å°±æœƒæœ‰ m çµ„æ¨£æœ¬æ•¸ç‚º N çš„ y è³‡æ–™é›†ã€‚</p>

<p>é€™æ¨£çš„æ–¹æ³•åœ¨æ¨£æœ¬æ•¸é‡å°‘æ™‚å¾ˆæœ‰ç”¨ã€‚å¦‚æœæ¨£æœ¬å°ï¼Œä½†æˆ‘å€‘ç”¨ train-validation-test é€™æ¨£çš„æ–¹å¼è¨“ç·´è³‡æ–™ï¼Œè¨“ç·´çš„æ¨£æœ¬è³‡æ–™éå¸¸å°ï¼Œæœƒé€ æˆ bias è¼ƒå¤§çš„å•é¡Œã€‚è€Œä½¿ç”¨ bootstrap ä¸æœƒæ¸›å°‘æ¨£æœ¬çš„æ•¸é‡ï¼Œä¹Ÿèƒ½ä¿ç•™ test dataã€‚</p>

<hr />

<p><strong>å¦‚ä½•æ“ä½œï¼Ÿ</strong></p>

<ol>
  <li>ä½¿ç”¨ boostrap æ–¹æ³•å¾ training data ä¸­æ¡é›† B çµ„æ¨£æœ¬æ•¸èˆ‡ training data ç›¸åŒçš„è³‡æ–™é›†ã€‚</li>
  <li>é€™ B çµ„è³‡æ–™é›†éƒ½å»ºç«‹ä¸€å€‹ modelï¼Œ \(f_b(x)\)ï¼Œå…±ç”¢ç”Ÿ B å€‹ modelã€‚</li>
  <li>æœ€å¾Œé æ¸¬çš„çµæœå°±æ˜¯å°‡é€™ B å€‹ model åšçµ±åˆã€‚åœ¨åˆ†é¡å•é¡Œä¸Šï¼Œå¯ä»¥å¹³å‡å„å€‹ model çš„ posterior class probabilitiesï¼Œæˆ–æ˜¯ä½¿ç”¨ majority votingã€‚ï¼ˆé¸æ©Ÿç‡æ¯”è¼ƒå¤§çš„çµæœæˆ–æ˜¯å¤šæ•¸æ±ºï¼‰ï¼›åœ¨å›æ­¸ä¸Šï¼Œå‰‡å–å¹³å‡å€¼ã€‚</li>
</ol>

<p>å¾æœ€ä¸Šåœ–å¯ä»¥çœ‹åˆ°ï¼Œä¸è«–æ˜¯ä½¿ç”¨æ©Ÿç‡æˆ–æ˜¯æŠ•ç¥¨æ±ºå®šï¼Œbagging é¸å‡ºçš„çµæœçš„ test error éƒ½è¦æ¯”å€‹åˆ¥çš„ model è¡¨ç¾è¼ƒå¥½ã€‚</p>

<hr />

<p>é™¤äº† Bagging å¤–ï¼ŒEnsemble learning é‚„æœ‰å¦ä¸€ç¨®å¸¸è¦‹çš„æ–¹æ³• Boostingï¼Œé€™ç¯‡å…ˆåˆ°é€™è£¡ä¹‹å¾Œç¹¼çºŒä»‹ç´¹ã€‚</p>

<hr />

<p>Reference:
<br />
<a href="https://medium.com/@chih.sheng.huang821/æ©Ÿå™¨å­¸ç¿’-ensemble-learningä¹‹bagging-boostingå’Œadaboost-af031229ebc3">æ©Ÿå™¨å­¸ç¿’: Ensemble learningä¹‹Baggingã€Boostingå’ŒAdaBoost</a>
<br />
<a href="https://blog.csdn.net/edogawachia/article/details/79357844">éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰ç®—æ³•åŸç†</a>
<br />
The Elements of Statistical Learning
<br />
Pattern Recognition and Machine Learning</p>

:ET