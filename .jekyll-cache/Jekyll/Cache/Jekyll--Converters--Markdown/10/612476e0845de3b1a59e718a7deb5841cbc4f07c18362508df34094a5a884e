I"C#<h2 id="decision-tree">Decision Tree</h2>
<p><img src="http://localhost:4000/blog/img/posts/decision_tree.png" alt="decision_tree.png" height="190px" width="500px" /></p>

<hr />

<p><strong>Decision Tree是什麼？</strong></p>

<p>簡單來說，decision tree 是一個分類模型。</p>

<ul>
  <li>Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one.</li>
</ul>

<p>這是 Pattern Recognition and Machine Learning 裡頭的定義。</p>

<p>根據上圖，想像一下，在右邊的二維座標平面上有一堆散布的點，而 decision tree，就是將這個平面一一分割，每一個框框都是一個分類結果。但我們可以用右邊那樣一層一層的樹枝結構狀來表示右邊難懂的圖。（這裡的例子是二維平面，當變數 \(x_i\) 增加時，這樣的概念可以推廣到多維空間）</p>

<p>每一個分支都像是一個 if-else 問題，如果是就選某一邊，不是就選另外一邊。</p>

<p>上圖左，我們可以看到有兩個變數，\(x_1\), \(x_2\)，第一關是 \(x_1 &gt; \theta_1 \)，如果小於就往左邊分，如果大於就往右邊分，以此類推往下繼續細分，最後給予分類結果。最後共分成 A, B, C, D, E 五個類別，也就是右邊的五個框框。</p>

<p>Decision Tree 又可以分為</p>
<ol>
  <li>
    <p>Regression trees - 最後分類結果為連續變數</p>
  </li>
  <li>
    <p>Classification trees - 最後分類結果為類別變數</p>
  </li>
</ol>

<hr />

<p><strong>名詞</strong></p>

<ul>
  <li>Root node (The Root) - 第一個起始的點</li>
  <li>(Internal) Nodes - 中間的節點。上方會有箭頭指向 node，且 node 也會往下指向其他點。</li>
  <li>Leaves (terminal nodes) - 最後的節點，也就是最後的分類結果。</li>
</ul>

<hr />

<p><strong>如何分類？</strong></p>

<p>假如今天我有一份 data，decision tree 是如何決定要先使用哪一個變數與什麼值作為分割呢？</p>

<p><em>分割的原則是，這樣的分割要能得到最大的資訊增益 (Information gain, IG)</em> (<a href="https://medium.com/@yehjames/資料分析-機器學習-第3-5講-決策樹-decision-tree-以及隨機森林-random-forest-介紹-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林</a>)</p>

<p>資訊量根據最後的分類結果可以使用，</p>
<ol>
  <li>Regression trees:
    <ul>
      <li>MSE (mean-squared error)</li>
    </ul>
  </li>
  <li>Classification trees:
    <ul>
      <li>Entropy (Deviance)</li>
      <li>Gini impurity</li>
      <li>Missclassification error</li>
    </ul>
  </li>
</ol>

<p>在 Classification tree 裡，Entropy 和 Gini impurity 是常用的兩種方式，
詳細的公式請參考 <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Wikipedia</a></p>

<p>假如今天碰到兩種 model 算出來的資訊量都ㄧ樣，請選擇比較簡單的那個 model。（分支、leaves node 較少）</p>

<hr />

<p><strong>R 範例</strong>
這裡使用 tree 這個 library 做範例，另外還有像是 rpart 也是做 decision 常用的 package。</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="p">(</span><span class="s2">"iris"</span><span class="p">)</span><span class="w">

</span><span class="c1"># 使用 Entropy (Deviance)</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">Species</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sepal.Length</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Petal.Length</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="w"> </span><span class="n">split</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"deviance"</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="o">=</span><span class="m">0</span><span class="p">)</span><span class="w">

</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">


</span><span class="c1"># 使用 Gini impurity</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">Species</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sepal.Length</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Petal.Length</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="w"> </span><span class="n">split</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"gini"</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="o">=</span><span class="m">0</span><span class="p">)</span><span class="w">

</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<hr />

<p><strong>優缺點</strong></p>

<ul>
  <li>優點：
    <ol>
      <li>Simple to understand and interpret. 容易理解與解釋</li>
      <li>Able to handle both numerical and categorical data. 可以用在類別與 numerical 資料</li>
    </ol>
  </li>
  <li>缺點
    <ol>
      <li>Low bias and high variance with respect to the training data.
（當 decision tree model 變得太過複雜時，太多 nodes，就會導致 overfitting (<a href="https://en.wikipedia.org/wiki/Bias–variance_tradeoff">bias-variance trade off</a> 的狀況）</li>
    </ol>
  </li>
</ul>

<p>所以為了讓 model 不要 overfitting，我們可以使用 pruning 的方式，砍掉底下的樹枝。</p>

<p>在 R 可以使用，<code class="language-plaintext highlighter-rouge">prune.tree()</code>，並且可以使用 <code class="language-plaintext highlighter-rouge">best</code> 這個參數來決定最後要保留多少 leaves。</p>

<p>使用剛剛最後的 <code class="language-plaintext highlighter-rouge">fit</code>示範</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prune_fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prune.tree</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">prune_fit</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">prune_fit</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="o">=</span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">prune_fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>這時候 leaves 由原本的 9，修剪到了剩下 5，但 Misclassification error rate 卻是相同的。修剪過後的 model 較簡單，且 variance 也會比較小，在預測上表現也會較佳。</p>

<hr />

<p>Reference:
<br />
<a href="https://medium.com/@yehjames/資料分析-機器學習-第3-5講-決策樹-decision-tree-以及隨機森林-random-forest-介紹-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林</a>
<br />
<a href="https://www.youtube.com/watch?v=7VeUPuFGJHk">StatQuest: Decision Trees</a>
<br />
The Elements of Statistical Learning
<br />
Pattern Recognition and Machine Learning</p>
:ET