<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- 數學符號 -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2022. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">[Neural Network]Neural Network 2 - Gradient descent</h1>
  <div class="post-info">
    <i class="fas fa-calendar-alt"></i>
    <span class="post-date">January 23, 2020</span>
    <span>
      <i class="fas fa-comments"></i>
      <a
        class="comment-count"
        href="/blog//neural%20network/2020/01/23/Neural-Network-Neural-Network-2/#disqus_thread"
        data-disqus-identifier="/neural%20network/2020/01/23/Neural-Network-Neural-Network-2/"
        >0 Comments</a
      >
    </span>
    <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
       
      <a
        target="_blank"
        href="/blog/category/#Neural Network"
        >Neural Network</a
      >
       
    </span>
  </div>
  <div><p><a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">上一篇</a>說明了神經網路的基本原理，這篇我們要講解神經網路是如何學習的 - Gradient descent。</p>

<p>這篇是<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">Neural networks</a>的課程筆記。</p>

<hr />

<p>在 Machine Learning 裡我們使用 training data 訓練得到一個 model，但這「訓練」到底是怎麼訓練的呢？訓練的過程，通常是要找到最小的 cost，也就是最後的結果與真實值差距越小越好。</p>

<p>在 Neural Network 我們要訓練的變數就是<a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">前面一篇</a>提到的 weight 和 bias。一開始我們隨機的給予這些數字，然後在一次次的學習中去改進 weight 和 bias 的值，找到最小的成本函數。</p>

<p>在課程的例子裡，Neural Network function 的 input 是 784 個 pixel 的數字，output 是 10 個數字，0 到 9 的機率，parameters 是 weight 和 bias。但在cost function 中，weight 和 bias 變成了 input，而 output 就是 cost 的值，parameters 則是很多的 training pairs。</p>

<p>我們可以把 cost function 這樣表示，</p>

<script type="math/tex; mode=display">C(w_1, w_2,..., b_1, b_2, ....) = cost</script>

<p>cost 的算法是 Neural Network 算出來的值，扣掉正確值然後平方相加。學習過程中，我們將所有 training data 的 cost 相加平均（average cost of all training data），希望這個值越小越好。</p>

<p><img src="http://localhost:4000/blog/img/posts/cost.png" alt="" /></p>

<p>那我們要如何找到最小的 cost 值？</p>

<p>如果 cost function 是一個像下圖的函數圖形，我們知道微分等於 0 可以找到最小值，但這並非一件容易的事，尤其是當我們有很多變數時。</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima.png" alt="" /></p>

<p>所以我們可以用的方法是，先隨機到函數圖形上的一點，然後根據該點的切線斜率來決定該往哪個方向移動可以找到最低點。這樣的方法，可以讓我們到 local minimum，我們無法確認找到的最小是否是 global minimum，因為這取決與一開始的 initial value。</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima%202.png" alt="" /></p>

<hr />

<p>在進入 Gradient descent 之前，我想要先介紹一下基礎觀念。以下將會講解，</p>

<ol>
  <li>導函數 Derivative function 與導數 Derivative</li>
  <li>方向導數 Directional derivative</li>
  <li>偏導數 Partial derivative</li>
  <li>梯度 Gradient</li>
</ol>

<h3 id="導數-derivative">導數 Derivative</h3>

<p>切線的斜率稱為「導數」（Derivative），也就是微分。</p>

<p>定義成，</p>

<script type="math/tex; mode=display">m = \text{lim}_{\Delta x \to 0}\frac{f(x+\Delta x) - f(x) }{\Delta x} = f'(x) = \frac{\text{d} f(x)}{\text{d}x}</script>

<h3 id="偏導數-partial-derivative">偏導數 Partial derivative</h3>

<p>斜率求的是單變數的微分，而偏導數是在多變數中的某個變數方向的斜率。</p>

<p>以兩個字變數為例， <script type="math/tex">z = f(x, y)</script></p>

<p><script type="math/tex">f_x(a, b)</script> 為對 <script type="math/tex">x</script> 的偏導數，定義為</p>

<script type="math/tex; mode=display">f_x(a, b) = \text{lim}_{h \to 0} \frac{f(a + h, b) - f(a, b)}{h}</script>

<p><script type="math/tex">f_y(a, b)</script> 為對 <script type="math/tex">y</script> 的偏導數，定義為</p>

<script type="math/tex; mode=display">f_y(a, b) = \text{lim}_{k \to 0} \frac{f(a, b + k) - f(a, b)}{k}</script>

<p>也就是對 x 或 y 方向做偏微分。</p>

<h3 id="方向導數-directional-derivative">方向導數 Directional derivative</h3>

<p>斜率求的是單變數的微分，在多變數中要求斜率必須要看是哪個方向的斜率，因為各個方向的斜率都不相同，這時要求的就是方向導數。</p>

<p>假設函數 <script type="math/tex">z = f(x, y)</script>，在定義域 <script type="math/tex">xy</script> 平面上有一點 <script type="math/tex">(x, y)</script> 及單位向量 <script type="math/tex">\overset{\rightharpoonup}{u} = (u_1, u_2)</script>。則曲面 <script type="math/tex">z = f(x, y)</script> 在 <script type="math/tex">(a, b)</script> 處，沿 <script type="math/tex">\overset{\rightharpoonup}{u}</script> 的方向的斜率定義為，</p>

<script type="math/tex; mode=display">\text{D}_u f(x_0, y_0)= \text{lim}_{h \to 0} \frac{f(x_0 + h u_1, y_0 + h u_2) - f(x_0, y_0)}{h} = \nabla f \boldsymbol{u}</script>

<p>方向導數的意義在於，有助於計算出空間中某一場量 <script type="math/tex">f(x, y, z)</script> 在某一特定方向 <script type="math/tex">\boldsymbol{u}</script> 之變化率。</p>

<h3 id="梯度-gradient">梯度 Gradient</h3>

<p>梯度的定義就是對各個分量做偏微分。</p>

<p>例如，現在是兩變數函數 <script type="math/tex">f(x, y)</script>，其梯度 <script type="math/tex">\nabla f(x, y)</script> 定義為</p>

<script type="math/tex; mode=display">\nabla f(x, y) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)</script>

<p>若是變數有三個，則 <script type="math/tex">\nabla f(x, y, y)</script> 定義為</p>

<script type="math/tex; mode=display">\nabla f(x, y, z) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)</script>

<p>Gernal 來看，有 n 個變數的函數 <script type="math/tex">f(x_1, x_2, \cdots, x_n)</script>，我們可以寫成</p>

<script type="math/tex; mode=display">\nabla f(x_1, x_2, \cdots, x_n) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}\right)</script>

<p>梯度 Gradient 的幾何意義，</p>
<ul>
  <li><script type="math/tex">\nabla f</script> 表示曲面 <script type="math/tex">f(x, y, z) = C</script> 之垂直向量，如下圖所示。</li>
  <li><script type="math/tex">\nabla f(P)</script> 表在 P 點增量最大的方向，也就是 the direction of steepest increase</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/gradient.png" alt="" /></p>

<hr />

<h3 id="梯度下降法-gradient-descent">梯度下降法 Gradient descent</h3>

<p>Gradient descent 是一種最佳化方法。</p>

<p>從上一部分，我們知道 P 點 gradeint 表示該點增量最大的方向。現在我們要找 cost function 的最小值，也就是該 function 斜率為 0 的點，當我們今天在 P 點知道哪個方向是會增加最快速的點，那就只要往反方向就會是減少最快速的方向了。也就是 <script type="math/tex">-\nabla f(P)</script> 是找尋 minimum cost 的方向。</p>

<p>總的來說，梯度下降法 Gradient descent 就是不斷的在某個點 P 算它的 Gradient，然後往 Gradient 的反方向一定，直到找到 local minimum 為止（無法知道是否找到 glocal minimum，會根據 initial values 而定）。</p>

<p>在 neural netword 中，讓這個梯度計算更有效率的方法叫做 <strong>Backpropagation</strong>，而這正是 neural netword 的重點！下一篇繼續介紹 :)</p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/827/mod_resource/content/33/Summary_227.pdf">方向導數(Directional Derivative)之定義與意義</a>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/826/mod_resource/content/31/Summary_226.pdf">純量函數之梯度(Gradient)</a></p>

</div>
</div>

<div class="likecoin">
  
  <iframe
    src="https://button.like.co/in/embed/shihs_tw/button?referrer=http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fneural%2520network%2F2020%2F01%2F23%2FNeural-Network-Neural-Network-2%2F" height="200" width="560" allowfullscreen="" frameborder="0"
  >
  </iframe>
  
</div>

  
<a name="comments"></a>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog/#disqus_thread';
        this.page.identifier = '/neural%20network/2020/01/23/Neural-Network-Neural-Network-2/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



  <div class="related">
    <h2>Latest Posts</h2>
    <ul class="related-posts">
      
      <li>
        <h3>
          <a href="/blog/python/flask/2022/04/06/Flask-%E5%9F%BA%E6%9C%AC%E5%AF%A6%E5%81%9A/"> [Python][Flask]Flask 基本實做 </a>
          <small>Apr 6, 2022</small>
        </h3>
      </li>
      
      <li>
        <h3>
          <a href="/blog/python/2022/04/05/Python-%E5%9C%A8-Mac-%E4%B8%8B%E8%BC%89%E8%88%87%E5%AE%89%E8%A3%9D-Miniconda/"> [Python]在 Mac 下載與安裝 Miniconda </a>
          <small>Apr 5, 2022</small>
        </h3>
      </li>
      
      <li>
        <h3>
          <a href="/blog/python/2022/04/05/Python-%E8%99%9B%E6%93%AC%E7%92%B0%E5%A2%83-(virtual-environment)-conda-create-%E8%88%87-virtualenv-%E6%AF%94%E8%BC%83/"> [Python]Python 虛擬環境 (virtual environment) - conda create 與 virtualenv 比較 </a>
          <small>Apr 5, 2022</small>
        </h3>
      </li>
      
    </ul>
  </div>
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
