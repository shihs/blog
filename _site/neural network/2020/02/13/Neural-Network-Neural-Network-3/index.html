<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- 數學符號 -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2022. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">[Neural Network]Neural Network 3 - Backpropagation</h1>
  <div class="post-info">
    <i class="fas fa-calendar-alt"></i>
    <span class="post-date">February 13, 2020</span>
    <span>
      <i class="fas fa-comments"></i>
      <a
        class="comment-count"
        href="/blog//neural%20network/2020/02/13/Neural-Network-Neural-Network-3/#disqus_thread"
        data-disqus-identifier="/neural%20network/2020/02/13/Neural-Network-Neural-Network-3/"
        >0 Comments</a
      >
    </span>
    <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
       
      <a
        target="_blank"
        href="/blog/category/#Neural Network"
        >Neural Network</a
      >
       
    </span>
  </div>
  <div><p><a href="https://shihs.github.io/blog/neural%20network/2020/01/23/Neural-Network-Neural-Network-2/">前面</a>介紹了 neural network 使用 Gradient descent 來找 cost function 的最小值，那這篇要來介紹 neural network 如何使用 <strong>Backpropagation</strong> 這個演算法讓 neural network 訓練過程中更有效率。</p>

<p>這篇是<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">Neural networks</a>的課程筆記。另外，覺得李宏毅老師的<a href="https://www.youtube.com/watch?v=ibJpTrp5mcE">課程</a>也講解的很清楚，所以我會搭配老師的課程內容。李宏毅老師和主要課程使用的符號不太ㄧ樣，但我還是使用他們原本各自的符號。</p>

<hr />

<h3 id="gradient-descent">Gradient Descent</h3>

<p>Gradient descent 的目的是要找到 neural network 的 parameters 的最佳解。</p>

<p>Network parameters: 
<script type="math/tex">\theta = {\{w_1, w_2,..., b_1, b_2,...\}}</script></p>

<p>Gradient: 
<script type="math/tex">\nabla L(\theta) = \begin{bmatrix}
    \partial L(\theta)/\partial w_1 \\
    \partial L(\theta)/\partial w_2 \\
    \vdots \\
    \partial L(\theta)/\partial b_1 \\
    \partial L(\theta)/\partial b_2 \\
    \vdots 
\end{bmatrix}</script></p>

<p>初始 parameters: <script type="math/tex">\theta^0</script>，計算 <script type="math/tex">\nabla L(\theta^0)</script>，則 <script type="math/tex">\theta^1 = \theta^0 - \eta \nabla L(\theta^0)</script>，接著可以再繼續算出 <script type="math/tex">\theta_2, \theta_3...</script></p>

<p>在 neural network 裡 Gradient 的最大問題就是，neural network 有太多的參數，所以這裡我們要介紹的 Backpropagation 就是一個比較有效的計算 Gradient 的演算法。也就是說，Backpropagation 基本上就是 Gradient Descent，但是是一個有效率的演算法。</p>

<hr />

<p>前面我們舉的例子是手寫辨識，input 加上兩層的 hideen layers，總共的 weight 和 bias 加起來有 13,002 個參數。如下圖，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%208.png" alt="" /></p>

<p>為了講解方便，我們現在把整個 nerual network 簡化成每層只有一個神經元。如下圖，第一個神經元是 input，最後一個神經元是 outpu，中間是兩層的 hidden layer，所以 cost function 可以寫成，<script type="math/tex">C(w_1, b_1, w_2, b_2, w_3, b_3)</script></p>

<p><img src="http://localhost:4000/blog/img/posts/simple%20neural%20network.png" alt="" /></p>

<p>為了說明 Backpropagation 現在我們再把範圍縮小到最後一層的 hidden layer 和 output，如下圖。</p>

<p><img src="http://localhost:4000/blog/img/posts/Backpropagation.png" alt="" /></p>

<p>我們令最後的 output 為 <script type="math/tex">a^{(L)}</script> 表示它是在第 L 層，而 hidden layer 為  <script type="math/tex">a^{(L-1)}</script> 表示在第 L-1 層，所以上標就只是用來表達它們各自在哪一層而已。</p>

<p>在 <script type="math/tex">a^{(L-1)}</script> 後的 <script type="math/tex">y</script> 是我們期望的輸出結果，我們希望 <script type="math/tex">y=1</script>，也就是說，這裡的 cost 值會是，</p>

<script type="math/tex; mode=display">C_0(...) = (a^{(L)} - y)^ 2 .......... (1)</script>

<p>以上圖的例子就會是，<script type="math/tex">(0.66 - 1)^2</script></p>

<p>接著我們來看每一個神經元的值。根據前面學過的，我們知道每一個神經元都是由前面的 weight 和 bias 算出來了，也就是說</p>

<script type="math/tex; mode=display">a^{(L)} = \sigma (w^{(L)} a^{(L-1)} + b^{(L)})</script>

<p>那為了方便，我們把 sigmoid function 裡的式子用一個符號表示，</p>

<script type="math/tex; mode=display">z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)} .......... (2)</script>

<p>而 <script type="math/tex">a^{(L)}</script> 就可以寫成，</p>

<script type="math/tex; mode=display">a^{(L)} = \sigma (z^{(L)}) .......... (3)</script>

<hr />

<p>我們現在有了上面三個式子，</p>

<p><script type="math/tex">C_0(...) = (a^{(L)} - y)^ 2 .......... (1)</script>
<br />
<script type="math/tex">z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)} .......... (2)</script>
<br />
<script type="math/tex">a^{(L)} = \sigma (z^{(L)}) .......... (3)</script></p>

<p>那我們現在要如何計算權重 <script type="math/tex">w^{(L)}</script> 對於 cost function 的影響程度呢？換句話說，我們想要知道 cost function 對 <script type="math/tex">w^{(L)}</script> 的偏微分 <script type="math/tex">\frac{\partial C_0}{\partial w^{(L)}}</script>。</p>

<p>根據 <strong>chain rule</strong>，我們可以再將上式改寫成，</p>

<script type="math/tex; mode=display">\frac{\partial C_0}{\partial w^{(L)}} = \frac{\partial z^{(L)}}{\partial w^{(L)}} \frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial C_0}{\partial a^{(L)}}</script>

<p>下一個步驟要來計算每一個值，我們可以根據上面式子得到的 (1)(2)(3) 得到，</p>

<script type="math/tex; mode=display">\frac{\partial C_0}{\partial a^{(L)}} = 2(a^{(L)} - y)</script>

<script type="math/tex; mode=display">\frac{\partial a^{(L)}}{\partial z^{(L)}} = \sigma' (z^{(L)})</script>

<script type="math/tex; mode=display">\frac{\partial z^{(L)}}{\partial w^{(L)}} = a^{(L-1)}</script>

<p>所以可以將式子整理成，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{\partial C_0}{\partial w^{(L)}} & = \frac{\partial z^{(L)}}{\partial w^{(L)}} \frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial C_0}{\partial a^{(L)}} \\
& = a^{(L-1)} \times \sigma' (z^{(L)}) \times 2(a^{(L)} - y)
\end{align} %]]></script>

<p>而這只是其中一個 training sample 算出來的 cost，而總代價函數應該是所有 training data 的平均，也就是</p>

<script type="math/tex; mode=display">\frac{\partial C}{\partial w^{(L)}} = \frac{1}{n} \sum_{k=0}^{n-1} \frac{\partial C_k}{\partial w^{(L)}}</script>

<p>而這其實只是 gradient 裡其中的一個 compenent，</p>

<p>Gradient:</p>

<script type="math/tex; mode=display">\nabla C = \begin{bmatrix}
    \frac{\partial C}{\partial w^{(1)}} \\
    \frac{\partial C}{\partial b^{(1)}} \\
    \frac{\partial C}{\partial w^{(2)}} \\
    \frac{\partial C}{\partial b^{(2)}} \\
    \vdots \\
    \frac{\partial C}{\partial w^{(L)}} \\
    \frac{\partial C}{\partial b^{(L)}} \\
\end{bmatrix}</script>

<p>如果要計算的是 bias 的偏微分，式子就會是，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{\partial C_0}{\partial b^{(L)}} & = \frac{\partial z^{(L)}}{\partial b^{(L)}} \frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial C_0}{\partial a^{(L)}} \\
& = 1 \times \sigma' (z^{(L)}) \times 2(a^{(L)} - y)
\end{align} %]]></script>

<p>接下來，我們來看這個 cost function 對上一層 activation 的敏感度，</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{\partial C_0}{\partial a^{(L-1)}} & = \frac{\partial z^{(L)}}{\partial a^{(L-1)}} \frac{\partial a^{(L)}}{\partial z^{(L)}} \frac{\partial C_0}{\partial a^{(L)}} \\
& = w^{(L)} \times \sigma' (z^{(L)}) \times 2(a^{(L)} - y)
\end{align} %]]></script>

<hr />

<p>前面我們看的是只有一個神經元的狀況，回到真實的 neural network 結構。一個神經元是來自前面很多神經元與 weight 和 bias 的計算，且一個 training data 的 cost 值是每個 output 減 expected value 平方的和。</p>

<p><img src="http://localhost:4000/blog/img/posts/Backpropagation%20cost.png" alt="" /></p>

<p><img src="http://localhost:4000/blog/img/posts/Backpropagation%202.png" alt="" /></p>

<p>基本上，所有的式子都和只有一個神經元是一樣的，唯一有變化的是，<script type="math/tex">\frac{\partial C_0}{\partial a_k^{(L-1)}}</script></p>

<script type="math/tex; mode=display">\frac{\partial C_0}{\partial a_k^{(L-1)}} = \sum_{j=0}^{n_L-1}\frac{\partial z_j^{(L)}}{\partial a_k^{(L-1)}} \frac{\partial a_j^{(L)}}{\partial z_j^{(L)}} \frac{\partial C_0}{\partial a_j^{(L)}}</script>

<p>因為，一個神經元會向下影響其他所有與它連接的神經元，所以必須將它們全部加起來。</p>

<p>根據以上，只要不斷往回算就可以了。</p>

<hr />

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.youtube.com/watch?v=ibJpTrp5mcE">ML Lecture 7: Backpropagation</a>
<br />
<a href="https://medium.com/ai-academy-taiwan/back-propagation-3946e8ed8c55">Back-propagation</a></p>

</div>
</div>

<div class="likecoin">
  
  <iframe
    src="https://button.like.co/in/embed/shihs_tw/button?referrer=http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fneural%2520network%2F2020%2F02%2F13%2FNeural-Network-Neural-Network-3%2F" height="200" width="560" allowfullscreen="" frameborder="0"
  >
  </iframe>
  
</div>

  
<a name="comments"></a>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog/#disqus_thread';
        this.page.identifier = '/neural%20network/2020/02/13/Neural-Network-Neural-Network-3/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



  <div class="related">
    <h2>Latest Posts</h2>
    <ul class="related-posts">
      
      <li>
        <h3>
          <a href="/blog/python/2022/04/05/Python-%E8%99%9B%E6%93%AC%E7%92%B0%E5%A2%83-(virtual-environment)-conda-create-%E8%88%87-virtualenv-%E6%AF%94%E8%BC%83/"> [Python]Python 虛擬環境 (virtual environment) - conda create 與 virtualenv 比較 </a>
          <small>Apr 5, 2022</small>
        </h3>
      </li>
      
      <li>
        <h3>
          <a href="/blog/web/2021/08/22/Web-MVC-%E6%9E%B6%E6%A7%8B/"> [Web]MVC 架構 </a>
          <small>Aug 22, 2021</small>
        </h3>
      </li>
      
      <li>
        <h3>
          <a href="/blog/python/2021/08/21/Python-%E5%9C%A8-Mac-%E4%B8%8B%E8%BC%89%E8%88%87%E5%AE%89%E8%A3%9D-Miniconda/"> [Python]在 Mac 下載與安裝 Miniconda </a>
          <small>Aug 21, 2021</small>
        </h3>
      </li>
      
    </ul>
  </div>
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
