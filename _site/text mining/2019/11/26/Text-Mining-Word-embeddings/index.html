<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- 數學符號 -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2019. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">[Text Mining]Word embeddings</h1>
  <div class="post-info">
    <i class="fas fa-calendar-alt"></i>
    <span class="post-date">November 26, 2019</span> 
    <span>
        <i class="fas fa-comments"></i>
        <a class="comment-count" href="/blog//text%20mining/2019/11/26/Text-Mining-Word-embeddings/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/26/Text-Mining-Word-embeddings/">0 Comments</a>
    </span>
    <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>
  </div>
  <div>
    <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="word-embeddings">Word embeddings</h2>

<p>對於人來說，要理解文字並不是件困難的事，但對電腦來說，每個字不過是一串 string，所以當我們要做 text mining 時就必須要將這些 string 轉化成電腦可以理解的方式。
而 word embedding(word vector) 的概念就是將文字轉換成 vector ，好讓電腦可以讀懂文字間的關係。</p>

<p>譬如說，人類可以理解 pretty 和 beautiful 是相近詞，但如果只是給電腦這兩個單字，對於電腦來說，這只是兩個不同長度的 string 罷了。word embedding 會將這兩個字轉換成不同的 vector 映射到一個高維空間，當這兩個 vector 越接近（可以使用 consine similarity）就表示這兩個詞越相近。這就是 word embedding 主要的概念。</p>

<ul>
  <li>A word embedding is a mapping of words to points in a vector space such that nearby words (points) are similar in terms of their distributional properties.</li>
</ul>

<h3 id="the-distributional-principle">The distributional principle</h3>

<p>word embedding 方法可以使用最重要的就是因為有 distributional hypothesis 這個假設。</p>

<ul>
  <li>The distributional principle states that words that occur in similar contexts tend to have similar meanings.</li>
</ul>

<p>這裡的概念是說，詞(target words)出現在類似的上下文中(context words)，則它們很有可能有相似的意思。
譬如說，
<br />
-「那隻『貓』好可愛」
<br />
-「那隻『狗』好可愛」
<br />
這時候除了『貓』和『狗』外，這兩句話的上下文是一樣的，根據 distributional principle，這兩個詞應該是相似的。</p>

<h3 id="co-occurrence-matrix">Co-occurrence matrix</h3>

<p><img src="http://localhost:4000/blog/img/posts/Co-occurrence%20matrix.png" alt="" /></p>

<p>上圖中，context words 就是上下文，而 target words 就是我們想要分析的字詞。
<br />
現在來看 cheese 這個字，可以看到和 butter, cake, cow, deer 這幾個字一起出現的次數分別是，12, 2, 1和0次。看起來和 butter 還有 cake 連結性比較強。
再來看 bread 這個字，同樣的在 butter 和 cake 上的連結也比較強。如果我們把這兩個單字用向量表示就會是，(12, 2, 1, 0) 和 (5, 5, 0, 0)，可以去比較和其他兩個單字的 cosine similarity，這兩個的關係是比較強的。</p>

<p>現在我們把它們畫出來（因為它們被映射到的是四維空間，所以老師的 slide 只看 cow 和 cake 這兩個 context words）</p>

<p><img src="http://localhost:4000/blog/img/posts/From%20co-occurrences%20to%20word%20vectors.png" alt="" /></p>

<p>如果還是不太明白，<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">這篇部落格</a>應該可以看懂。</p>

<p><strong>補充：</strong>
從上面的 matrix 可能會想到一件事，<em>context words</em> 等於是決定了 target words 的向量啊！
換句話說，當我們分析不同的文本的時候，會需要不同的 context words來算出 word embeddings。想像，如果今天要分析新聞報導和 ptt 內容，不太可能使用一樣的 context words，畢竟ptt用語和新聞用語會有很大的不同。（蛤？你說記者都抄 ptt 內容嗎？XDDD）</p>

<hr />

<h3 id="simple-applications-of-word-embeddings">Simple applications of word embeddings</h3>

<p>word embeddings 的應用</p>

<ul>
  <li>finding similar words. 找到相似的字，像是上面的例子，找出哪一個字和 cheese 比較相似。</li>
  <li>answering ‘odd one out’ questions. 找出不一樣的詞，譬如說  lunch, breakfast, dinner, car 哪一個詞屬於不同類？ (根據上面提到的概念，lunch, breakfast, dinner 這三個的 vector 應該會比較接近，會在比較接近的上下文中出現)</li>
</ul>

<h3 id="limitations-of-word-embeddings">Limitations of word embeddings</h3>

<ul>
  <li>
    <p>There are many different facets of ‘similarity’. Ex. Is a cat more similar to a dog or to a tiger? (在不同情境下，cat 和 dog 可能比較相似。譬如說，貓和狗都是寵物，但如果以生物的角度來看，cat 和 tiger 都屬於貓科動物，這時候 cat 和 tiger 會比較相似)</p>
  </li>
  <li>
    <p>Text data does not reflect many ‘trivial’ properties of words. Ex. more ‘black sheep’ than ‘white sheep’ (如果只分析文本，因為大部分的羊都是白色的，所以在提到羊的時候並不會特別提到顏色，但當提到比較稀少的黑羊時，反而會特別說到 black，這會導致在分析時好像黑羊出現的頻率比白羊出現的頻率高)</p>
  </li>
  <li>
    <p>Word vectors reflect social biases in the data used to train them. Ex. including gender and ethnic stereotypes (<a href="https://www.pnas.org/content/115/16/E3635">論文</a>參考) 很多詞語上的用法其實帶有非常多的社會偏見和刻板印象，而這也會導致分析出的結果有所偏差。</p>
  </li>
</ul>

<hr />

<p><strong>還有什麼問題？</strong></p>

<p>到目前為止，看起來都非常合理，那還會有什麼問題呢？</p>

<p>這裡會碰到和之前提到過的，矩陣<em>稀疏性</em>的問題。如果今天 context words 有十萬個字，那麼 target words 就會是在十萬維度的空間的 vectors，而且可能會有很多的值都是 0 的狀況發生。那這樣要用什麼方法解決矩陣的稀疏性並產生 word embeddings（也就是每個詞的向量） 呢？</p>

<p>從不同的面向來看幾個常見的 word embedding 方法，</p>
<ul>
  <li>Learning word embeddings via <em>matrix factorization</em>
    <ol>
      <li>Singular Value Decomposition(SVD)</li>
      <li>Positive Pointwise mutual information(PPMI)</li>
    </ol>
  </li>
  <li>Learning word embeddings via <em>language models</em>
    <ol>
      <li>N-gram</li>
      <li>Neural language models(Ex. word2vec)</li>
    </ol>
  </li>
</ul>

<p>以下就要來介紹這幾種方法。</p>

<hr />

<h2 id="singular-value-decompositionsvd">Singular Value Decomposition(SVD)</h2>

<ul>
  <li>The rows of co-occurrence matrices are long and sparse. Instead, we would like to have word vectors that are short and dense. 簡單來說，co-occurrence matrices 會有稀疏性的問題。</li>
  <li>One idea is to approximate the co-occurrence matrix by another matrix with fewer columns. Singular Value Decomposition 的想法是，將這個又長又臭的 co-occurrence matrix 用另比較少 columns 的 matrix 取代。</li>
</ul>

<h3 id="什麼是-singular-value-decomposition奇異值分解">什麼是 Singular value decomposition（奇異值分解）?</h3>

<p>推薦<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">李宏毅老師的線性代數</a></p>
<ul>
  <li><em>Singular value decomposition(SVD)</em> can be applied on any matrix. (不需要是方陣。比較：PCA 也是一個可降維的方法，但它的矩陣就必須要是方陣。)</li>
</ul>

<p>SVD 的概念就是，任一一個矩陣 <script type="math/tex">A_{m \times n}</script>，它都可以拆解成三個矩陣（<script type="math/tex">U_{m \times n}, \Sigma_{m \times n}, V^T_{n \times n}</script>）的相乘。</p>

<p>其中，<script type="math/tex">U_{m \times n}</script> 的 columns 是 <em>Orthonormal</em>，而 <script type="math/tex">V^T_{n \times n}</script> 的 rows 是 <em>Orthonormal</em>，<script type="math/tex">\Sigma_{m \times n}</script> 是 <em>Diagonal</em>(只有對角線有非負的值，且由大到小)。</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD.png" alt="" /></p>

<ul>
  <li><script type="math/tex">\Sigma_{m \times n}</script> 會是一個長得像這樣的矩陣，且 <script type="math/tex">\sigma_1 \ge \sigma_1 \ge \ldots \ge \sigma_k</script></li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Sigma.png" alt="" /></p>

<p>而 <script type="math/tex">\sigma_r, ~~where~~1 \le r \le k</script> 是奇異值（singular value），而 r 越小也代表了該值越重要，換句話說，含有越多訊息，因此我們可以只保留 <script type="math/tex">\Sigma</script> 較重要的前面幾行得到一個相似的矩陣 <script type="math/tex">A</script>。</p>

<script type="math/tex; mode=display">A_{m \times n} = U_{m \times r} \times \Sigma_{r \times r} \times V^T_{r \times n}</script>

<p>參考<a href="https://ccjou.wordpress.com/2009/09/01/奇異值分解-svd/">線代啟示錄-奇異值分解 (SVD)</a>的圖，</p>

<p><img src="http://localhost:4000/blog/img/posts/svd2.jpg" alt="" /></p>

<p>回到我們的 word-embedding。也就是說，使用 SVD 可以利用減少 <script type="math/tex">\Sigma</script> 的維度來處理稀疏性的問題，雖然刪除了一些詞仍舊保留重要的詞。</p>

<ul>
  <li>Each row of the (truncated) matrix 𝑼 is a k-dimensional vector that represents the ‘most important’ information about a word.</li>
  <li>A practical problem is that computing the singular value decomposition for large matrices is expensive.</li>
</ul>

<hr />

<h2 id="positive-pointwise-mutual-informationppmi">Positive Pointwise mutual information(PPMI)</h2>

<h3 id="pointwise-mutual-informationpmi">Pointwise mutual information(PMI)</h3>

<ul>
  <li>Raw counts favour pairs that involve very common contexts. Ex.the cat, a cat will receive higher weight than cute cat, small cat.</li>
  <li>We want a measure that favours contexts in which the target word occurs more often than other words.</li>
  <li>A suitable measure is pointwise mutual information (PMI):</li>
</ul>

<script type="math/tex; mode=display">PMI(x, y) = log \frac{P(x, y)}{P(x) \times P(y)}</script>

<p>簡單來說，我們可以用 PMI 公式來看兩個字之間的關係。</p>

<p>現在我們把 <script type="math/tex">x</script> 看成我們的 target word，<script type="math/tex">y</script> 看成我們的 context word，</p>

<ul>
  <li>We want to use PMI to measure the associative strength between a word <script type="math/tex">w</script> and a context <script type="math/tex">c</script> in a data set <script type="math/tex">D</script>:</li>
</ul>

<script type="math/tex; mode=display">PMI(w, c) = log \frac{P(w, c)}{P(w) \times P(c)} = log \frac{\#(w, c)/|D|}{\#(w)/|D| \cdot \#(c)/|D|} = log \frac{\#(w,c) \cdot |D|}{\#(w) \cdot \#(c)}</script>

<p>但根據上面的公式，會發現一個問題，PMI is infinitely small for unseen word–context pairs, and undefined for unseen target words. (如果 <script type="math/tex">w</script> 和 <script type="math/tex">c</script> 並沒有共同出現過，再取 log，整個值會變成 -Inf)</p>

<p>所以這時候就有了 <strong>Positive Pointwise mutual information(PPMI)</strong>。</p>

<ul>
  <li>In positive pointwise mutual information (PPMI), all negative and undefined values are replaced by zero:</li>
</ul>

<script type="math/tex; mode=display">􏰜􏰜􏰝􏰞􏰍􏰂􏰠PPMI(w, c) = max(PMI(w, c), 0)</script>

<ul>
  <li>PPMI assigns high values to rare events, it is advisable to apply a count threshold or smooth the probabilities.</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Computing%20PPMI%20on%20a%20word%E2%80%93context%20matrix.png" alt="" /></p>

<hr />

<h2 id="language-models">Language models</h2>

<ul>
  <li>A <em>probabilistic language model</em> is a probability distribution over sequences of words in some language.</li>
  <li>Recent years have seen the rise of <em>neural language models</em>, which are based on distributed representations of words.</li>
  <li>By the chain rule, the probability of a sequence of 𝑁 words can be computed using conditional probabilities as</li>
</ul>

<script type="math/tex; mode=display">p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_1 \ldots w_{k-1})</script>

<ul>
  <li>To make probability estimates more robust, we can approximate the full history <script type="math/tex">w_1 \ldots w_N</script> by the last few words(馬可夫鍊):</li>
</ul>

<script type="math/tex; mode=display">p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_{k-n+1} \ldots w_{k-1})</script>

<h2 id="n-gram-models">N-gram models</h2>

<ul>
  <li>An n-gram is a contiguous sequence of n words or characters. Ex. unigram (Text), bigram (Text Mining), trigram (Text Mining course)</li>
  <li>An n-gram model is a language model defined on n-grams –  a probability distribution over sequences of n words.</li>
  <li>n-gram 是一種語言機率模型。一句話出現的機率是一個聯合模型。如果一個詞的出現只考慮前面一個字，那就是 bi-gram；如果一個詞的出現考慮前面兩個字，那就是 tri-gram。</li>
</ul>

<p><strong>Formal definition of an n-gram model</strong></p>

<ul>
  <li><script type="math/tex">n</script>: the model’s order (1 = unigram, 2 = bigram, …)</li>
  <li><script type="math/tex">V</script>: a set of possible words (character); the vocabulary</li>
  <li><script type="math/tex">P(w\mid u)</script>: a probability that specifies how likely it is to observe  the word <script type="math/tex">w</script> after the context 
<br />(n − 1)-gram <script type="math/tex">u</script></li>
</ul>

<hr />

<p><strong>Unigram model</strong></p>

<p><em>n = 1</em> 不考慮前面出現的字。
<img src="http://localhost:4000/blog/img/posts/Unigram%20model.png" alt="" /></p>

<p>Thus contexts are empty.</p>

<p><strong>MLE of unigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/MLE%20of%20unigram%20probabilities.png" alt="" /></p>

<hr />

<p><strong>Bigram models</strong></p>

<p><em>n = 2</em> 考慮前面出現的一個字。</p>

<p><img src="http://localhost:4000/blog/img/posts/Bigram%20models.png" alt="" /></p>

<p>Thus contexts are unigrams.</p>

<p><strong>Estimating bigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Estimating%20bigram%20probabilities.png" alt="" /></p>

<p><strong>Example</strong>
(<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">source</a>)
<img src="http://localhost:4000/blog/img/posts/bigram%20example.png" alt="" /></p>

<hr />

<p><strong>Smoothing</strong></p>

<p>當在計算 bigram 時可能會碰到兩個字完全沒有相鄰的狀況，這會導致算出來的機率等於 0。（如下圖）</p>

<p><img src="http://localhost:4000/blog/img/posts/bigram%20smoothing.png" alt="" /></p>

<p>這種時候就需要用到，smoothing。</p>

<p><strong>Smoothing methods</strong></p>
<ul>
  <li>Additive smoothing</li>
  <li>Good-Turing estimate</li>
  <li>Jelinek-Mercer smoothing (interpolation) • Katz smoothing (backoff)</li>
  <li>Witten-Bell smoothing</li>
  <li>Absolute discounting</li>
  <li>Kneser-Ney smoothing</li>
</ul>

<hr />

<p>上面的狀況碰到的是，”CHER” 後面沒有出現 “READ” 的狀況，而導致機率等於0，但如果現在是 “CHER” 這個字從未出現在資料集中呢？這種狀況時，smoothing 便派不上用場了。</p>

<ul>
  <li>A simple way to deal with this is to introduce a special word type UNK, and to smooth it like any other word type in the vocabulary.</li>
  <li>When we compute the probability of a document, then we first replace every unknown word with UNK.</li>
</ul>

<hr />

<h2 id="neural-networks-as-language-models">Neural networks as language models</h2>

<h3 id="advantages-of-neural-language-models">Advantages of neural language models</h3>
<ul>
  <li>Neural models can achieve better perplexity than probabilistic models, and scale to much larger values of n.</li>
  <li>Words in different positions share parameters, making them share statistical strength. (Everything must pass through the hidden layer.)</li>
  <li>The network can learn that in some contexts, only parts of the  n-gram are informative. (implicit smoothing, helps with unknown words)</li>
</ul>

<h3 id="word2vec">word2vec</h3>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1">詞向量介紹</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">自然語言處理 – Vector Space of Semantics</a>
<br />
<a href="https://zhuanlan.zhihu.com/p/26306795">[NLP] 秒懂词向量Word2vec的本质</a>
<br />
<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">李宏毅老師的線性代數 - SVD</a>
<br />
<a href="http://www.shuang0420.com/2017/03/21/NLP%20笔记%20-%20再谈词向量/">NLP 笔记 - 再谈词向量</a>
<br />
<a href="https://www.twblogs.net/a/5c7bdd70bd9eee339918b4da">機器學習筆記之二十二——PCA與SVD</a>
<br />
<a href="https://ccjou.wordpress.com/2009/09/01/奇異值分解-svd/">線代啟示錄-奇異值分解 (SVD)</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">自然語言處理 – Pointwise Mutual Information</a>
<br />
<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing</a>
<br />
<a href="https://kknews.cc/tech/83yx3qn.html">機器學習五分鐘：自然語言處理（NLP）的N-gram模型是什麼？</a></p>

  </div>
</div>


<a name="comments"></a>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/26/Text-Mining-Word-embeddings/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



<div class="related">
  <h2>Latest Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/blog/nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/">
            [NLP]NLTK 筆記        
          </a>
            <small>Dec 17, 2019</small>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/blog/text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/">
            [Text Mining]Text clustering and topic modelling        
          </a>
            <small>Nov 21, 2019</small>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/blog/text%20mining/2019/11/13/Text-Mining-Text-classification/">
            [Text Mining]Text classification        
          </a>
            <small>Nov 13, 2019</small>
        </h3>
      </li>
    
  </ul>
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
