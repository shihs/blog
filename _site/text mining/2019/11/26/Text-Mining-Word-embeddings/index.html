<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- æ•¸å­¸ç¬¦è™Ÿ -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2022. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">[Text Mining]Word embeddings</h1>
  <div class="post-info">
    <i class="fas fa-calendar-alt"></i>
    <span class="post-date">November 26, 2019</span>
    <span>
      <i class="fas fa-comments"></i>
      <a
        class="comment-count"
        href="/blog//text%20mining/2019/11/26/Text-Mining-Word-embeddings/#disqus_thread"
        data-disqus-identifier="/text%20mining/2019/11/26/Text-Mining-Word-embeddings/"
        >0 Comments</a
      >
    </span>
    <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
       
      <a
        target="_blank"
        href="/blog/category/#Text Mining"
        >Text Mining</a
      >
       
    </span>
  </div>
  <div><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="word-embeddings">Word embeddings</h2>

<p>å°æ–¼äººä¾†èªªï¼Œè¦ç†è§£æ–‡å­—ä¸¦ä¸æ˜¯ä»¶å›°é›£çš„äº‹ï¼Œä½†å°é›»è…¦ä¾†èªªï¼Œæ¯å€‹å­—ä¸éæ˜¯ä¸€ä¸² stringï¼Œæ‰€ä»¥ç•¶æˆ‘å€‘è¦åš text mining æ™‚å°±å¿…é ˆè¦å°‡é€™äº› string è½‰åŒ–æˆé›»è…¦å¯ä»¥ç†è§£çš„æ–¹å¼ã€‚
è€Œ word embedding(word vector or word representation) çš„æ¦‚å¿µå°±æ˜¯å°‡æ–‡å­—è½‰æ›æˆ vector ï¼Œå¥½è®“é›»è…¦å¯ä»¥è®€æ‡‚æ–‡å­—é–“çš„é—œä¿‚ã€‚</p>

<p>è­¬å¦‚èªªï¼Œäººé¡å¯ä»¥ç†è§£ pretty å’Œ beautiful æ˜¯ç›¸è¿‘è©ï¼Œä½†å¦‚æœåªæ˜¯çµ¦é›»è…¦é€™å…©å€‹å–®å­—ï¼Œå°æ–¼é›»è…¦ä¾†èªªï¼Œé€™åªæ˜¯å…©å€‹ä¸åŒé•·åº¦çš„ string ç½·äº†ã€‚word embedding æœƒå°‡é€™å…©å€‹å­—è½‰æ›æˆä¸åŒçš„ vector æ˜ å°„åˆ°ä¸€å€‹é«˜ç¶­ç©ºé–“ï¼Œç•¶é€™å…©å€‹ vector è¶Šæ¥è¿‘ï¼ˆå¯ä»¥ä½¿ç”¨ consine similarityï¼‰å°±è¡¨ç¤ºé€™å…©å€‹è©è¶Šç›¸è¿‘ã€‚é€™å°±æ˜¯ word embedding ä¸»è¦çš„æ¦‚å¿µã€‚</p>

<ul>
  <li>A word embedding is a mapping of words to points in a vector space such that nearby words (points) are similar in terms of their distributional properties.</li>
</ul>

<h3 id="the-distributional-principle">The distributional principle</h3>

<p>word embedding æ–¹æ³•å¯ä»¥ä½¿ç”¨æœ€é‡è¦çš„å°±æ˜¯å› ç‚ºæœ‰ distributional hypothesis é€™å€‹å‡è¨­ã€‚</p>

<ul>
  <li>The distributional principle states that words that occur in similar contexts tend to have similar meanings.</li>
</ul>

<p>é€™è£¡çš„æ¦‚å¿µæ˜¯èªªï¼Œè©(target words)å‡ºç¾åœ¨é¡ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­(context words)ï¼Œå‰‡å®ƒå€‘å¾ˆæœ‰å¯èƒ½æœ‰ç›¸ä¼¼çš„æ„æ€ã€‚
è­¬å¦‚èªªï¼Œ
<br />
-ã€Œé‚£éš»ã€è²“ã€å¥½å¯æ„›ã€
<br />
-ã€Œé‚£éš»ã€ç‹—ã€å¥½å¯æ„›ã€
<br />
é€™æ™‚å€™é™¤äº†ã€è²“ã€å’Œã€ç‹—ã€å¤–ï¼Œé€™å…©å¥è©±çš„ä¸Šä¸‹æ–‡æ˜¯ä¸€æ¨£çš„ï¼Œæ ¹æ“š distributional principleï¼Œé€™å…©å€‹è©æ‡‰è©²æ˜¯ç›¸ä¼¼çš„ã€‚</p>

<h3 id="co-occurrence-matrix">Co-occurrence matrix</h3>

<p><img src="http://localhost:4000/blog/img/posts/Co-occurrence%20matrix.png" alt="" /></p>

<p>ä¸Šåœ–ä¸­ï¼Œcontext words å°±æ˜¯ä¸Šä¸‹æ–‡ï¼Œè€Œ target words å°±æ˜¯æˆ‘å€‘æƒ³è¦åˆ†æçš„å­—è©ã€‚
<br />
ç¾åœ¨ä¾†çœ‹ cheese é€™å€‹å­—ï¼Œå¯ä»¥çœ‹åˆ°å’Œ butter, cake, cow, deer é€™å¹¾å€‹å­—ä¸€èµ·å‡ºç¾çš„æ¬¡æ•¸åˆ†åˆ¥æ˜¯ï¼Œ12, 2, 1å’Œ0æ¬¡ã€‚çœ‹èµ·ä¾†å’Œ butter é‚„æœ‰ cake é€£çµæ€§æ¯”è¼ƒå¼·ã€‚
å†ä¾†çœ‹ bread é€™å€‹å­—ï¼ŒåŒæ¨£çš„åœ¨ butter å’Œ cake ä¸Šçš„é€£çµä¹Ÿæ¯”è¼ƒå¼·ã€‚å¦‚æœæˆ‘å€‘æŠŠé€™å…©å€‹å–®å­—ç”¨å‘é‡è¡¨ç¤ºå°±æœƒæ˜¯ï¼Œ(12, 2, 1, 0) å’Œ (5, 5, 0, 0)ï¼Œå¯ä»¥å»æ¯”è¼ƒå’Œå…¶ä»–å…©å€‹å–®å­—çš„ cosine similarityï¼Œé€™å…©å€‹çš„é—œä¿‚æ˜¯æ¯”è¼ƒå¼·çš„ã€‚</p>

<p>ç¾åœ¨æˆ‘å€‘æŠŠå®ƒå€‘ç•«å‡ºä¾†ï¼ˆå› ç‚ºå®ƒå€‘è¢«æ˜ å°„åˆ°çš„æ˜¯å››ç¶­ç©ºé–“ï¼Œæ‰€ä»¥è€å¸«çš„ slide åªçœ‹ cow å’Œ cake é€™å…©å€‹ context wordsï¼‰</p>

<p><img src="http://localhost:4000/blog/img/posts/From%20co-occurrences%20to%20word%20vectors.png" alt="" /></p>

<p>å¦‚æœé‚„æ˜¯ä¸å¤ªæ˜ç™½ï¼Œ<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">é€™ç¯‡éƒ¨è½æ ¼</a>æ‡‰è©²å¯ä»¥çœ‹æ‡‚ã€‚</p>

<p><strong>è£œå……ï¼š</strong>
å¾ä¸Šé¢çš„ matrix å¯èƒ½æœƒæƒ³åˆ°ä¸€ä»¶äº‹ï¼Œ<em>context words</em> ç­‰æ–¼æ˜¯æ±ºå®šäº† target words çš„å‘é‡å•Šï¼
æ›å¥è©±èªªï¼Œç•¶æˆ‘å€‘åˆ†æä¸åŒçš„æ–‡æœ¬çš„æ™‚å€™ï¼Œæœƒéœ€è¦ä¸åŒçš„ context wordsä¾†ç®—å‡º word embeddingsã€‚æƒ³åƒï¼Œå¦‚æœä»Šå¤©è¦åˆ†ææ–°èå ±å°å’Œ ptt å…§å®¹ï¼Œä¸å¤ªå¯èƒ½ä½¿ç”¨ä¸€æ¨£çš„ context wordsï¼Œç•¢ç«Ÿpttç”¨èªå’Œæ–°èç”¨èªæœƒæœ‰å¾ˆå¤§çš„ä¸åŒã€‚ï¼ˆè›¤ï¼Ÿä½ èªªè¨˜è€…éƒ½æŠ„ ptt å…§å®¹å—ï¼ŸXDDDï¼‰</p>

<hr />

<h3 id="simple-applications-of-word-embeddings">Simple applications of word embeddings</h3>

<p>word embeddings çš„æ‡‰ç”¨</p>

<ul>
  <li>finding similar words. æ‰¾åˆ°ç›¸ä¼¼çš„å­—ï¼Œåƒæ˜¯ä¸Šé¢çš„ä¾‹å­ï¼Œæ‰¾å‡ºå“ªä¸€å€‹å­—å’Œ cheese æ¯”è¼ƒç›¸ä¼¼ã€‚</li>
  <li>answering â€˜odd one outâ€™ questions. æ‰¾å‡ºä¸ä¸€æ¨£çš„è©ï¼Œè­¬å¦‚èªª  lunch, breakfast, dinner, car å“ªä¸€å€‹è©å±¬æ–¼ä¸åŒé¡ï¼Ÿ (æ ¹æ“šä¸Šé¢æåˆ°çš„æ¦‚å¿µï¼Œlunch, breakfast, dinner é€™ä¸‰å€‹çš„ vector æ‡‰è©²æœƒæ¯”è¼ƒæ¥è¿‘ï¼Œæœƒåœ¨æ¯”è¼ƒæ¥è¿‘çš„ä¸Šä¸‹æ–‡ä¸­å‡ºç¾)</li>
</ul>

<h3 id="limitations-of-word-embeddings">Limitations of word embeddings</h3>

<ul>
  <li>
    <p>There are many different facets of â€˜similarityâ€™. Ex. Is a cat more similar to a dog or to a tiger? (åœ¨ä¸åŒæƒ…å¢ƒä¸‹ï¼Œcat å’Œ dog å¯èƒ½æ¯”è¼ƒç›¸ä¼¼ã€‚è­¬å¦‚èªªï¼Œè²“å’Œç‹—éƒ½æ˜¯å¯µç‰©ï¼Œä½†å¦‚æœä»¥ç”Ÿç‰©çš„è§’åº¦ä¾†çœ‹ï¼Œcat å’Œ tiger éƒ½å±¬æ–¼è²“ç§‘å‹•ç‰©ï¼Œé€™æ™‚å€™ cat å’Œ tiger æœƒæ¯”è¼ƒç›¸ä¼¼)</p>
  </li>
  <li>
    <p>Text data does not reflect many â€˜trivialâ€™ properties of words. Ex. more â€˜black sheepâ€™ than â€˜white sheepâ€™ (å¦‚æœåªåˆ†ææ–‡æœ¬ï¼Œå› ç‚ºå¤§éƒ¨åˆ†çš„ç¾Šéƒ½æ˜¯ç™½è‰²çš„ï¼Œæ‰€ä»¥åœ¨æåˆ°ç¾Šçš„æ™‚å€™ä¸¦ä¸æœƒç‰¹åˆ¥æåˆ°é¡è‰²ï¼Œä½†ç•¶æåˆ°æ¯”è¼ƒç¨€å°‘çš„é»‘ç¾Šæ™‚ï¼Œåè€Œæœƒç‰¹åˆ¥èªªåˆ° blackï¼Œé€™æœƒå°è‡´åœ¨åˆ†ææ™‚å¥½åƒé»‘ç¾Šå‡ºç¾çš„é »ç‡æ¯”ç™½ç¾Šå‡ºç¾çš„é »ç‡é«˜)</p>
  </li>
  <li>
    <p>Word vectors reflect social biases in the data used to train them. Ex. including gender and ethnic stereotypes (<a href="https://www.pnas.org/content/115/16/E3635">è«–æ–‡</a>åƒè€ƒ) å¾ˆå¤šè©èªä¸Šçš„ç”¨æ³•å…¶å¯¦å¸¶æœ‰éå¸¸å¤šçš„ç¤¾æœƒåè¦‹å’Œåˆ»æ¿å°è±¡ï¼Œè€Œé€™ä¹Ÿæœƒå°è‡´åˆ†æå‡ºçš„çµæœæœ‰æ‰€åå·®ã€‚</p>
  </li>
</ul>

<hr />

<p><strong>é‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿ</strong></p>

<p>åˆ°ç›®å‰ç‚ºæ­¢ï¼Œçœ‹èµ·ä¾†éƒ½éå¸¸åˆç†ï¼Œé‚£é‚„æœƒæœ‰ä»€éº¼å•é¡Œå‘¢ï¼Ÿ</p>

<p>é€™è£¡æœƒç¢°åˆ°å’Œä¹‹å‰æåˆ°éçš„ï¼ŒçŸ©é™£<em>ç¨€ç–æ€§</em>çš„å•é¡Œã€‚å¦‚æœä»Šå¤© context words æœ‰åè¬å€‹å­—ï¼Œé‚£éº¼ target words å°±æœƒæ˜¯åœ¨åè¬ç¶­åº¦çš„ç©ºé–“çš„ vectorsï¼Œè€Œä¸”å¯èƒ½æœƒæœ‰å¾ˆå¤šçš„å€¼éƒ½æ˜¯ 0 çš„ç‹€æ³ç™¼ç”Ÿã€‚é‚£é€™æ¨£è¦ç”¨ä»€éº¼æ–¹æ³•è§£æ±ºçŸ©é™£çš„ç¨€ç–æ€§ä¸¦ç”¢ç”Ÿ word embeddingsï¼ˆä¹Ÿå°±æ˜¯æ¯å€‹è©çš„å‘é‡ï¼‰ å‘¢ï¼Ÿ</p>

<p>å¾ä¸åŒçš„é¢å‘ä¾†çœ‹å¹¾å€‹å¸¸è¦‹çš„ word embedding æ–¹æ³•ï¼Œ</p>
<ul>
  <li>Learning word embeddings via <em>matrix factorization</em>
    <ol>
      <li>Singular Value Decomposition(SVD)</li>
      <li>Positive Pointwise mutual information(PPMI)</li>
    </ol>
  </li>
  <li>Learning word embeddings via <em>language models</em>
    <ol>
      <li>N-gram</li>
      <li>Neural language models(Ex. word2vec)</li>
    </ol>
  </li>
</ul>

<p>ä»¥ä¸‹å°±è¦ä¾†ä»‹ç´¹é€™å¹¾ç¨®æ–¹æ³•ã€‚</p>

<hr />

<h2 id="matrix-factorization---singular-value-decompositionsvd">Matrix factorization - Singular Value Decomposition(SVD)</h2>

<ul>
  <li>The rows of co-occurrence matrices are long and sparse. Instead, we would like to have word vectors that are short and dense. ç°¡å–®ä¾†èªªï¼Œco-occurrence matrices æœƒæœ‰ç¨€ç–æ€§çš„å•é¡Œã€‚</li>
  <li>One idea is to approximate the co-occurrence matrix by another matrix with fewer columns. Singular Value Decomposition çš„æƒ³æ³•æ˜¯ï¼Œå°‡é€™å€‹åˆé•·åˆè‡­çš„ co-occurrence matrix ç”¨å¦æ¯”è¼ƒå°‘ columns çš„ matrix å–ä»£ã€‚</li>
</ul>

<h3 id="ä»€éº¼æ˜¯-singular-value-decompositionå¥‡ç•°å€¼åˆ†è§£">ä»€éº¼æ˜¯ Singular value decompositionï¼ˆå¥‡ç•°å€¼åˆ†è§£ï¼‰?</h3>

<p>æ¨è–¦<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸</a></p>
<ul>
  <li><em>Singular value decomposition(SVD)</em> can be applied on any matrix. (ä¸éœ€è¦æ˜¯æ–¹é™£ã€‚æ¯”è¼ƒï¼šPCA(ç‰¹å¾µå€¼åˆ†è§£) ä¹Ÿæ˜¯ä¸€å€‹é™ç¶­çš„æ–¹æ³•ï¼Œä½†å®ƒçš„çŸ©é™£å°±å¿…é ˆè¦æ˜¯æ–¹é™£ã€‚)</li>
</ul>

<p>SVD çš„æ¦‚å¿µå°±æ˜¯ï¼Œä»»ä¸€ä¸€å€‹çŸ©é™£ <script type="math/tex">A_{m \times n}</script>ï¼Œå®ƒéƒ½å¯ä»¥æ‹†è§£æˆä¸‰å€‹çŸ©é™£ï¼ˆ<script type="math/tex">U_{m \times n}, \Sigma_{m \times n}, V^T_{n \times n}</script>ï¼‰çš„ç›¸ä¹˜ã€‚</p>

<p>å…¶ä¸­ï¼Œ<script type="math/tex">U_{m \times n}</script> çš„ columns æ˜¯ <em>Orthonormal</em>ï¼Œè€Œ <script type="math/tex">V^T_{n \times n}</script> çš„ rows æ˜¯ <em>Orthonormal</em>ï¼Œ<script type="math/tex">\Sigma_{m \times n}</script> æ˜¯ <em>Diagonal</em>(åªæœ‰å°è§’ç·šæœ‰éè² çš„å€¼ï¼Œä¸”ç”±å¤§åˆ°å°)ã€‚</p>

<p><em>(åœ¨ç·šæ€§ä»£æ•¸ä¸­ï¼Œä¸€å€‹å…§ç©ç©ºé–“çš„æ­£äº¤åŸºï¼ˆorthogonal basisï¼‰æ˜¯å…ƒç´ å…©å…©æ­£äº¤çš„åŸºã€‚ç¨±åŸºä¸­çš„å…ƒç´ ç‚ºåŸºå‘é‡ã€‚ å‡è‹¥ï¼Œä¸€å€‹æ­£äº¤åŸºçš„åŸºå‘é‡çš„æ¨¡é•·éƒ½æ˜¯å–®ä½é•·åº¦1ï¼Œå‰‡ç¨±é€™æ­£äº¤åŸºç‚ºæ¨™æº–æ­£äº¤åŸºæˆ–â€è¦ç¯„æ­£äº¤åŸºâ€ï¼ˆOrthonormal basisï¼‰ã€‚)</em></p>

<p><img src="http://localhost:4000/blog/img/posts/SVD.png" alt="" /></p>

<ul>
  <li><script type="math/tex">\Sigma_{m \times n}</script> æœƒæ˜¯ä¸€å€‹é•·å¾—åƒé€™æ¨£çš„çŸ©é™£ï¼Œä¸” <script type="math/tex">\sigma_1 \ge \sigma_1 \ge \ldots \ge \sigma_k</script></li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Sigma.png" alt="" /></p>

<p>è€Œ <script type="math/tex">\sigma_r, ~~where~~1 \le r \le k</script> æ˜¯å¥‡ç•°å€¼ï¼ˆsingular valueï¼‰ï¼Œè€Œ r è¶Šå°ä¹Ÿä»£è¡¨äº†è©²å€¼è¶Šé‡è¦ï¼Œæ›å¥è©±èªªï¼Œå«æœ‰è¶Šå¤šè¨Šæ¯ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥åªä¿ç•™ <script type="math/tex">\Sigma</script> è¼ƒé‡è¦çš„å‰é¢å¹¾è¡Œå¾—åˆ°ä¸€å€‹ç›¸ä¼¼çš„çŸ©é™£ <script type="math/tex">A</script>ã€‚ç”¨è¼ƒå°çš„å„²å­˜ç©ºé–“å°±å¯ä»¥å¾—åˆ°æ¥è¿‘åŸå§‹çš„çŸ©é™£ <script type="math/tex">A</script>ã€‚</p>

<script type="math/tex; mode=display">A_{m \times n} \approx U_{m \times r} \times \Sigma_{r \times r} \times V^T_{r \times n}</script>

<p>åƒè€ƒ<a href="https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/">ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)</a>çš„åœ–ï¼Œ</p>

<p><img src="http://localhost:4000/blog/img/posts/svd2.jpg" alt="" /></p>

<p>å›åˆ°æˆ‘å€‘çš„ word-embeddingã€‚æˆ‘å€‘å¯ä»¥åˆ©ç”¨ SVD é€²è¡Œå»å™ªåŠé™ç¶­ï¼Œåˆªé™¤ä¸€äº›ä¸é‚£éº¼é‡è¦çš„è¨Šæ¯ï¼Œç”¨ä¾†è§£æ±º Co-occurrence matrix ç¨€ç–æ€§çš„å•é¡Œã€‚</p>

<p>æˆ‘å€‘ä¹Ÿä¸éœ€è¦å†å°‡ç›¸ä¹˜çŸ©é™£ï¼Œç›´æ¥ä½¿ç”¨ ğ‘¼ å°±å¥½ï¼Œæ¯ä¸€åˆ—å°±ä»£è¡¨ä¸€å€‹ target wordã€‚</p>

<ul>
  <li>Each row of the (truncated) matrix ğ‘¼ is a k-dimensional vector that represents the â€˜most importantâ€™ information about a word.</li>
  <li>A practical problem is that computing the singular value decomposition for large matrices is expensive.</li>
</ul>

<p>é€™é‚Šçœ‹ä¸€å€‹<a href="https://www.itread01.com/content/1569742263.html">ä¾‹å­</a>ï¼Œ</p>

<p>ä¸‹åœ–æ˜¯ä¸€å€‹ Co-occurrence matrix <script type="math/tex">~A_{m \times n}</script></p>

<p><img src="http://localhost:4000/blog/img/posts/term-document.png" alt="" /></p>

<p>å°‡ä¸Šé¢çš„çŸ©é™£ <script type="math/tex">A</script> ä½¿ç”¨ SVD åˆ†è§£ã€é™ç¶­ï¼Œåªç•™ä¸‹å‰ä¸‰å€‹ç‰¹å¾µå€¼ã€‚æ¯å€‹ç‰¹å¾µå€¼çš„å¤§å°è¡¨ç¤ºå°æ‡‰ä½ç½®çš„å±¬æ€§å€¼çš„é‡è¦æ€§å¤§å°ï¼Œå·¦å¥‡ç•°çŸ©é™£çš„æ¯ä¸€åˆ—å³ä»£è¡¨æ¯å€‹è©çš„ç‰¹å¾µå‘é‡ï¼Œå³å¥‡ç•°çŸ©é™£çš„æ¯ä¸€è¡Œè¡¨ç¤ºæ¯å€‹æ–‡ä»¶çš„ç‰¹å¾µå‘é‡ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD%20example.png" alt="" /></p>

<p>å–æ¯å€‹å‘é‡å¾Œå…©ç¶­çš„å°æ‡‰å€¼æŠ•å½±åˆ°ä¸€å€‹äºŒç¶­ç©ºé–“ï¼Œå¦‚ä¸‹æ‰€ç¤º</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD%20example%202.png" alt="" /></p>

<p>ä¸Šåœ–ä¸­ï¼Œä¸€å€‹ç´…è‰²çš„é»å°æ‡‰ä¸€å€‹è©ï¼Œä¸€å€‹è—è‰²çš„é»å°æ‡‰ä¸€å€‹æ–‡ä»¶ã€‚ç•¶é€™äº›é»è¢«æŠ•å½±åˆ°ç©ºé–“ä¸­ï¼Œæˆ‘å€‘å¯ä»¥å°é€™äº›è©å’Œæ–‡ä»¶é€²è¡Œåˆ†é¡ï¼Œæ¯”å¦‚èªªstockå’Œmarketå¯ä»¥æ”¾åœ¨ä¸€é¡ï¼Œrealå’Œestateå¯ä»¥æ”¾åœ¨ä¸€é¡ï¼ŒæŒ‰é€™æ¨£çš„åˆ†é¡çµæœï¼Œæˆ‘å€‘å°±å¯ä»¥çŸ¥é“æ–‡ä»¶ä¸­å“ªäº›äº‹ç›¸è¿‘çš„è©ï¼Œæ‰€ä»¥ç•¶ä½¿ç”¨è€…åˆ©ç”¨è©æœå°‹æ–‡ä»¶çš„æ™‚å€™ï¼Œæˆ‘å€‘å°±å¯ä»¥åˆ©ç”¨ç›¸è¿‘çš„è©ï¼ˆåœ¨å‘é‡ç©ºé–“ä¸­ç›¸è¿‘çš„è©ã€è¢«æ­¸ç‚ºåŒä¸€é¡çš„è©ï¼‰é€²è¡Œæª¢ç´¢ï¼Œè€Œä¸æ˜¯åªæ˜¯ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è©æœå°‹ã€‚</p>

<hr />

<h2 id="matrix-factorization---positive-pointwise-mutual-informationppmi">Matrix factorization - Positive Pointwise mutual information(PPMI)</h2>

<h3 id="pointwise-mutual-informationpmi">Pointwise mutual information(PMI)</h3>

<ul>
  <li>
    <p>Raw counts favour pairs that involve very common contexts. 
<br />
E.g. the cat, a cat will receive higher weight than cute cat, small cat.</p>
  </li>
  <li>
    <p>We want a measure that favours contexts in which the target word occurs more often than other words.</p>
  </li>
  <li>
    <p>A suitable measure is pointwise mutual information (PMI):</p>
  </li>
</ul>

<script type="math/tex; mode=display">PMI(x, y) = log \frac{P(x, y)}{P(x) \times P(y)}</script>

<p>ç°¡å–®ä¾†èªªï¼Œæˆ‘å€‘å¯ä»¥ç”¨ PMI å…¬å¼ä¾†çœ‹å…©å€‹å­—ä¹‹é–“çš„é—œä¿‚ã€‚</p>

<p>ç¾åœ¨æˆ‘å€‘æŠŠ <script type="math/tex">x</script> çœ‹æˆæˆ‘å€‘çš„ target wordï¼Œ<script type="math/tex">y</script> çœ‹æˆæˆ‘å€‘çš„ context wordï¼Œ</p>

<ul>
  <li>We want to use PMI to measure the associative strength between a word <script type="math/tex">w</script> and a context <script type="math/tex">c</script> in a data set <script type="math/tex">D</script>:</li>
</ul>

<script type="math/tex; mode=display">PMI(w, c) = log \frac{P(w, c)}{P(w) \times P(c)} = log \frac{\#(w, c)/|D|}{\#(w)/|D| \cdot \#(c)/|D|} = log \frac{\#(w,c) \cdot |D|}{\#(w) \cdot \#(c)}</script>

<p>ä½†æ ¹æ“šä¸Šé¢çš„å…¬å¼ï¼Œæœƒç™¼ç¾ä¸€å€‹å•é¡Œï¼ŒPMI is infinitely small for unseen wordâ€“context pairs, and undefined for unseen target words. (å¦‚æœ <script type="math/tex">w</script> å’Œ <script type="math/tex">c</script> ä¸¦æ²’æœ‰å…±åŒå‡ºç¾éï¼Œå†å– logï¼Œæ•´å€‹å€¼æœƒè®Šæˆ -Inf)</p>

<p>æ‰€ä»¥é€™æ™‚å€™å°±æœ‰äº† <strong>Positive Pointwise mutual information(PPMI)</strong>ã€‚</p>

<ul>
  <li>In positive pointwise mutual information (PPMI), all negative and undefined values are replaced by zero:</li>
</ul>

<script type="math/tex; mode=display">ô°œô°œô°ô°ô°ô°‚ô° PPMI(w, c) = max(PMI(w, c), 0)</script>

<ul>
  <li>PPMI assigns high values to rare events, it is advisable to apply a count threshold or smooth the probabilities.</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Computing%20PPMI%20on%20a%20word%E2%80%93context%20matrix.png" alt="" /></p>

<p>çœ‹ä¸€å€‹<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">ä¾‹å­</a>ï¼Œ</p>

<p>ä¸‹åœ–æ˜¯ä¸€å€‹ Co-occurrence matrixï¼Œåˆ—æ˜¯ target wordsï¼Œè¡Œæ˜¯ context words
<img src="http://localhost:4000/blog/img/posts/PMI%20example.png" alt="" /></p>

<p>å‡è¨­é€™ç¯‡æ–‡ç« ç¸½å…±åªæœ‰ 19 å€‹å­—ï¼Œé€™è£¡æˆ‘å€‘è¨ˆç®— x = informationï¼Œy = data çš„ PMI å€¼ï¼Œ
<img src="http://localhost:4000/blog/img/posts/PMI%20example%202.png" alt="" /></p>

<p>æ ¹æ“šåŒæ¨£çš„æ–¹å¼å¯ä»¥æ±‚å‡ºæ‰€æœ‰ target words å°æ‡‰çš„ context words çš„ PMI å€¼ã€‚</p>

<hr />

<h2 id="language-models">Language models</h2>

<ul>
  <li>A <em>probabilistic language model</em> is a probability distribution over sequences of words in some language.</li>
  <li>Recent years have seen the rise of <em>neural language models</em>, which are based on distributed representations of words.</li>
  <li>By the chain rule, the probability of a sequence of ğ‘ words can be computed using conditional probabilities as</li>
</ul>

<script type="math/tex; mode=display">p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_1 \ldots w_{k-1})</script>

<ul>
  <li>To make probability estimates more robust, we can approximate the full history <script type="math/tex">w_1 \ldots w_N</script> by the last few words (é¦¬å¯å¤«éŠ):</li>
</ul>

<script type="math/tex; mode=display">p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_{k-n+1} \ldots w_{k-1})</script>

<hr />

<h2 id="language-models---n-gram-models">Language models - N-gram models</h2>

<ul>
  <li>
    <p>An n-gram is a contiguous sequence of n words or characters. 
<br />
E.g. unigram (Text), bigram (Text Mining), trigram (Text Mining course)</p>
  </li>
  <li>
    <p>An n-gram model is a language model defined on n-grams â€“ â€¨a probability distribution over sequences of n words.</p>
  </li>
  <li>
    <p>n-gram æ˜¯ä¸€ç¨®èªè¨€æ©Ÿç‡æ¨¡å‹ã€‚ä¸€å¥è©±å‡ºç¾çš„æ©Ÿç‡æ˜¯ä¸€å€‹è¯åˆæ¨¡å‹ã€‚å¦‚æœä¸€å€‹è©çš„å‡ºç¾åªè€ƒæ…®å‰é¢ä¸€å€‹å­—ï¼Œé‚£å°±æ˜¯ bi-gramï¼›å¦‚æœä¸€å€‹è©çš„å‡ºç¾è€ƒæ…®å‰é¢å…©å€‹å­—ï¼Œé‚£å°±æ˜¯ tri-gramã€‚</p>
  </li>
</ul>

<p><strong>Formal definition of an n-gram model</strong></p>

<ul>
  <li><script type="math/tex">n</script>: the modelâ€™s order (1 = unigram, 2 = bigram, â€¦)</li>
  <li><script type="math/tex">V</script>: a set of possible words (character); the vocabulary</li>
  <li><script type="math/tex">P(w\mid u)</script>: a probability that specifies how likely it is to observe the word <script type="math/tex">w</script> after the context 
<br />(n âˆ’ 1)-gram <script type="math/tex">u</script></li>
</ul>

<p><strong>Unigram model</strong></p>

<p><em>n = 1</em> ä¸è€ƒæ…®å‰é¢å‡ºç¾çš„å­—ã€‚
<img src="http://localhost:4000/blog/img/posts/Unigram%20model.png" alt="" /></p>

<p>Thus contexts are empty.</p>

<p><strong>MLE of unigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/MLE%20of%20unigram%20probabilities.png" alt="" /></p>

<hr />

<p><strong>Bigram models</strong></p>

<p><em>n = 2</em> è€ƒæ…®å‰é¢å‡ºç¾çš„ä¸€å€‹å­—ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/Bigram%20models.png" alt="" /></p>

<p>Thus contexts are unigrams.</p>

<p><strong>Estimating bigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Estimating%20bigram%20probabilities.png" alt="" /></p>

<p><strong>Example</strong>
(<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">source</a>)
<img src="http://localhost:4000/blog/img/posts/bigram%20example.png" alt="" /></p>

<hr />

<p><strong>Smoothing</strong></p>

<p>ç•¶åœ¨è¨ˆç®— bigram æ™‚å¯èƒ½æœƒç¢°åˆ°å…©å€‹å­—å®Œå…¨æ²’æœ‰ç›¸é„°çš„ç‹€æ³ï¼Œé€™æœƒå°è‡´ç®—å‡ºä¾†çš„æ©Ÿç‡ç­‰æ–¼ 0ã€‚ï¼ˆå¦‚ä¸‹åœ–ï¼‰</p>

<p><img src="http://localhost:4000/blog/img/posts/bigram%20smoothing.png" alt="" /></p>

<p>é€™ç¨®æ™‚å€™å°±éœ€è¦ç”¨åˆ°ï¼Œsmoothingã€‚</p>

<p><strong>Smoothing methods</strong></p>
<ul>
  <li>Additive smoothing</li>
  <li>Good-Turing estimate</li>
  <li>Jelinek-Mercer smoothing (interpolation)</li>
  <li>Katz smoothing (backoff)</li>
  <li>Witten-Bell smoothing</li>
  <li>Absolute discounting</li>
  <li>Kneser-Ney smoothing</li>
</ul>

<p>ä¸Šé¢çš„ç‹€æ³ç¢°åˆ°çš„æ˜¯ï¼Œâ€CHERâ€ å¾Œé¢æ²’æœ‰å‡ºç¾ â€œREADâ€ çš„ç‹€æ³ï¼Œè€Œå°è‡´æ©Ÿç‡ç­‰æ–¼0ï¼Œä½†å¦‚æœç¾åœ¨æ˜¯ â€œCHERâ€ é€™å€‹å­—å¾æœªå‡ºç¾åœ¨è³‡æ–™é›†ä¸­å‘¢ï¼Ÿé€™ç¨®ç‹€æ³æ™‚ï¼Œsmoothing ä¾¿æ´¾ä¸ä¸Šç”¨å ´äº†ã€‚</p>

<ul>
  <li>In addition to unseen words, a new text may even contain <em>unknown words</em>. For these, smoothing will not help.</li>
</ul>

<p><strong>Unknown words</strong>
<br />
å»ºç«‹ä¸€å€‹ token <UNK>ï¼Œå¦‚æœæ˜¯ unknown words å°±ç”¨ <UNK> ç•¶ä½œæ™®é€šçš„å–®è©è™•ç†ã€‚</UNK></UNK></p>

<ul>
  <li>A simple way to deal with this is to introduce a special word type <em>UNK</em>, and to smooth it like any other word type in the vocabulary.</li>
  <li>When we compute the probability of a document, then we first replace every unknown word with <em>UNK</em>.</li>
</ul>

<hr />

<h2 id="language-models---neural-networks-as-language-models">Language models - Neural networks as language models</h2>

<h3 id="advantages-of-neural-language-models">Advantages of neural language models</h3>
<ul>
  <li>Neural models can achieve better perplexity than probabilistic models, and scale to much larger values of n.</li>
  <li>Words in different positions share parameters, making them share statistical strength. (Everything must pass through the hidden layer.)</li>
  <li>The network can learn that in some contexts, only parts of the â€¨n-gram are informative. (implicit smoothing, helps with unknown words)</li>
</ul>

<h3 id="word2vec">word2vec</h3>

<ul>
  <li>word2vec æ˜¯ word embedding çš„ä¸€ç¨®</li>
  <li>
    <p>word2Vec ä¸»è¦æœ‰ CBOW (continuous bag-of-words) å’Œ skip-gram å…©ç¨®æ¨¡å‹</p>
  </li>
  <li>CBOW æ˜¯çµ¦å®šä¸Šä¸‹æ–‡ï¼Œä¾†é æ¸¬è¼¸å…¥çš„å­—è©ï¼›Skip-gram å‰‡æ˜¯çµ¦å®šè¼¸å…¥å­—è©å¾Œï¼Œä¾†é æ¸¬ä¸Šä¸‹æ–‡</li>
</ul>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1">è©å‘é‡ä»‹ç´¹</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">è‡ªç„¶èªè¨€è™•ç† â€“ Vector Space of Semantics</a>
<br />
<a href="https://zhuanlan.zhihu.com/p/26306795">[NLP] ç§’æ‡‚è¯å‘é‡Word2vecçš„æœ¬è´¨</a>
<br />
<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸ - SVD</a>
<br />
<a href="http://www.shuang0420.com/2017/03/21/NLP%20ç¬”è®°%20-%20å†è°ˆè¯å‘é‡/">NLP ç¬”è®° - å†è°ˆè¯å‘é‡</a>
<br />
<a href="https://www.twblogs.net/a/5c7bdd70bd9eee339918b4da">æ©Ÿå™¨å­¸ç¿’ç­†è¨˜ä¹‹äºŒåäºŒâ€”â€”PCAèˆ‡SVD</a>
<br />
<a href="https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/">ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">è‡ªç„¶èªè¨€è™•ç† â€“ Pointwise Mutual Information</a>
<br />
<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing</a>
<br />
<a href="https://kknews.cc/tech/83yx3qn.html">æ©Ÿå™¨å­¸ç¿’äº”åˆ†é˜ï¼šè‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰çš„N-gramæ¨¡å‹æ˜¯ä»€éº¼ï¼Ÿ</a>
<br />
<a href="https://www.itread01.com/content/1569742263.html">è©å‘é‡(one-hot/SVD/NNLM/Word2Vec/GloVe)</a></p>
</div>
</div>

<div class="likecoin">
  
  <iframe
    src="https://button.like.co/in/embed/shihs_tw/button?referrer=http%3A%2F%2Flocalhost%3A4000%2Fblog%2Ftext%2520mining%2F2019%2F11%2F26%2FText-Mining-Word-embeddings%2F" height="200" width="560" allowfullscreen="" frameborder="0"
  >
  </iframe>
  
</div>

  
<a name="comments"></a>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/26/Text-Mining-Word-embeddings/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



  <div class="related">
    <h2>Latest Posts</h2>
    <ul class="related-posts">
      
      <li>
        <h3>
          <a href="/blog/python/flask/2022/04/06/Flask-%E5%9F%BA%E6%9C%AC%E5%AF%A6%E5%81%9A/"> [Python][Flask]Flask åŸºæœ¬å¯¦åš </a>
          <small>Apr 6, 2022</small>
        </h3>
      </li>
      
      <li>
        <h3>
          <a href="/blog/python/2022/04/05/Python-%E5%9C%A8-Mac-%E4%B8%8B%E8%BC%89%E8%88%87%E5%AE%89%E8%A3%9D-Miniconda/"> [Python]åœ¨ Mac ä¸‹è¼‰èˆ‡å®‰è£ Miniconda </a>
          <small>Apr 5, 2022</small>
        </h3>
      </li>
      
      <li>
        <h3>
          <a href="/blog/python/2022/04/05/Python-%E8%99%9B%E6%93%AC%E7%92%B0%E5%A2%83-(virtual-environment)-conda-create-%E8%88%87-virtualenv-%E6%AF%94%E8%BC%83/"> [Python]Python è™›æ“¬ç’°å¢ƒ (virtual environment) - conda create èˆ‡ virtualenv æ¯”è¼ƒ </a>
          <small>Apr 5, 2022</small>
        </h3>
      </li>
      
    </ul>
  </div>
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
