<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- 數學符號 -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2022. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts" id="begin">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/text%20mining/2019/11/05/Text-Mining-Information-Retrieval/">
        [Text Mining]Information Retrieval
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 5, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 5, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>

    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<p>這門課教授的是英文的文字探勘，中文與英文本身在結構上有非常大的差異，所以有些解析和斷詞的方法中文可能並不適用，但在基本概念上還是有相同的地方。</p>

<p>每個主題都有一個 Lab 附在文章最後。</p>

<hr />

<h2 id="information-retrieval-ir">Information Retrieval (IR)</h2>

<p>Information Retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).</p>

<p>中文稱為「資訊檢索」。從一堆的 unstructured 資料中（通常是文字）找出符合我們需要的條件的資料。</p>

<p>例如，google 搜尋就是一個例子。我們在搜尋 bar 裡輸入我們想要找到的關鍵字，google 搜尋引擎會從它儲存的所有 database（網站們） 中找出符合我們關鍵字條件的網站給我們。</p>

<hr />

<p>那我們要如何找到包含這些關鍵字的文章呢？</p>

<p>最直觀地想應該就是把有我們搜尋的關鍵字的文章抓出來吧！這就是 Boolean retrieval。</p>

<p>在進入 Boolean retrieval 之前先來看一下 The classic search model。</p>

<h3 id="the-classic-search-model">The classic search model</h3>

<ul>
  <li>To communicate her/his information need to an IR system, the user formulates a <em>search query</em>.</li>
  <li>The objective of the IR system is to find documents in the collection that ‘match’ the search query.</li>
  <li>A good IR system finds documents that are also relevant for the user’s information need.</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/The%20classic%20search%20model.png" alt="PMF_two coins.png" /></p>

<hr />

<h3 id="boolean-retrieval">Boolean retrieval</h3>

<p>The Boolean retrieval model is a model for information retrieval in which we can pose any query which is in the form of a Boolean expression of terms, that is, in which terms are combined with the operators AND, OR, and NOT. The model views each document as just a set of words.</p>

<p>很直覺的方法要找出包含關鍵字的可以使用 terms(通常是個單詞) 是否包含在 documents 裡。</p>

<p>例如，現在有 Sherlock Holmes 小說文字。我想要找出哪些篇章符合出現 ‘Moriarty’ 和 ‘Lestrade’ 但不包含 ‘Adair’。那麼下的 query 就會是 <em>Moriarty AND Lestrade AND NOT Adair</em></p>

<p>那個我們可以將上面的概念寫成一個 <strong>Term–document matrix</strong>。</p>

<p><strong>Term–document matrix</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Term%E2%80%93document%20matrix.png" alt="PMF_two coins.png" /></p>

<p>每一行代表一個章節，每一列代表一個條件的單詞（term）。以列來看，0 表示該單詞沒有出現在那一章節中，1 表示該單詞有出現。
<br />
像是第一列，’Adair’ 只有出現在 ‘Empty house’ 這個章節。</p>

<p>這樣的方法看似簡單，但以實務上來說並不是個好辦法。因為，</p>

<ul>
  <li>Term–document matrices are sparse. 可以從上面的 matrix 看到，有很多的 0 ，也就是說其實很多資訊並不需要，但卻還是需要儲存。</li>
  <li>假如現在有 1,000,000 份 documents，有 500,000 不同的 terms ，這樣的條件下會產生 a matrix with 500,000,000,000 entries (62,5 GB)。</li>
</ul>

<p>所以我們必須得使用別的辦法減少不必要的資訊儲存。這時候就有了 Inverted index。</p>

<p><strong>Inverted index</strong></p>

<p>The inverted index is a key–value mapping, the basic idea is shown below.</p>

<ul>
  <li>the keys are search terms</li>
  <li>the values are sorted lists of document identifiers (ids)</li>
  <li>the list for terms identifies those documents that contain the terms</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Inverted%20index.png" alt="PMF_two coins.png" /></p>

<p>從上圖我們可以看到，和 Term–document matrix 不一樣的是 Inverted index 只儲存了有包含該單詞的 document ids。</p>

<hr />

<p>現在我們知道我們要的是去看那些關鍵字是否包含在 documents 中，但我們並不會每一次找關鍵字的時候一篇一篇文章搜索，我們會先建立一個儲存好所有 term 的 matrix，
但我們要怎麼決定是哪些 terms 要被儲存呢？</p>

<h2 id="index-construction">Index construction</h2>

<p>The major steps in index construction:</p>

<ol>
  <li>Collect the documents to be indexed.</li>
  <li>Tokenize the text.</li>
  <li>Do linguistic preprocessing of tokens.</li>
  <li>Index the documents that each term occurs in.</li>
</ol>

<p><img src="http://localhost:4000/blog/img/posts/building%20an%20index%20by%20sorting%20and%20grouping.png" alt="PMF_two coins.png" /></p>

<h3 id="tokenization">Tokenization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raw</span> <span class="o">=</span> <span class="s">"Apple is looking at buying U.K. startup for $1 billion."</span>
<span class="c"># tokenize raw text based on whitespace</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">raw</span><span class="o">.</span><span class="n">split</span><span class="p">():</span> 
	<span class="k">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>


<span class="c"># tokenize using spaCy</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">"en_core_web_sm"</span><span class="p">)</span> 
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">nlp</span><span class="p">(</span><span class="n">raw</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="c"># 結果：</span>
<span class="c"># Apple</span>
<span class="c"># is</span>
<span class="c"># looking</span>
<span class="c"># at</span>
<span class="c"># buying</span>
<span class="c"># U.K.</span>
<span class="c"># startup</span>
<span class="c"># for</span>
<span class="c"># $</span>
<span class="c"># 1</span>
<span class="c"># billion</span>
<span class="c"># . </span>
</code></pre></div></div>

<p>英文每個單字可以很容易地使用空白（.split()）來分割出每個單字，但只使用
split 無法排除標點符號的問題，python 的 spacy 可以將標點符號也區分出來。</p>

<h3 id="stop-words">Stop words</h3>
<p>A stop word is a word that is frequent but does not contribute much value for the application in question. For example: a, the, and…</p>

<p>但根據分析的目的不同，會有不同的 stop words 的定義。以分析 Sherlock Holmes 的小說為例，資料裡面肯定會包含非常大量的 Sherlock Holmes，那這種時候我們或許可以根據分析的目的將 Sherlock Holmes 定義為 stop words。換句話說，並不存在一個所有分析通用的 stop words 資料集。</p>

<p>既然 stop words 對於分析沒有價值貢獻，那麼在做分析前就必須把 stop words 刪除，以避免干擾分析結果。</p>

<h3 id="lexemes-and-lemmas">Lexemes and lemmas</h3>

<p>英文的文法有幾個規則，</p>
<ol>
  <li>動詞會根據人稱與時態有所變化</li>
  <li>動詞有動名詞型態</li>
  <li>名詞的單數與複數有變化</li>
</ol>

<p>但在分析前必須要將這些相同意義但不同型的單字修正為相同以便分析。</p>

<ul>
  <li>
    <p>The term <em>lexeme</em> refers to a set of word forms that all share the same fundamental meaning. 
<br />
For example: word forms run, runs, ran, running – lexeme run</p>
  </li>
  <li>
    <p>The term <em>lemma</em> refers to the particular word form that is chosen, by convention, to represent a given lexeme. 
<br />
For example: what you would put into a lexicon</p>
  </li>
</ul>

<hr />

<h2 id="ranked-retrieval排序檢索">Ranked retrieval（排序檢索）</h2>

<p>進行到這裡我們已經知道如何要先建立一個有所有 term 的 matrix for 所有的 documents，然後使用 boolean retrieval 找出我們要的 documents，但這樣並沒有考慮到單詞的重要性。換句話說，一個很常出現的單詞有可能並沒有比一個出現次數較少但卻在某 document 出現還要重要，我們可能更想要抓出含有那個單詞的文章。所以我們必須要 rank 那些抓出來的 terms。</p>

<p><strong>Problems with Boolean retrieval</strong></p>

<ul>
  <li>Not many users are capable of writing high-quality Boolean queries, and many find the process too time-consuming.</li>
  <li>Feast or famine: Boolean queries tend to return either too many results, or no results at all.</li>
  <li>Intuitively, whether or not a document ‘matches’ a search query is not a Boolean property, but is gradual in nature.</li>
</ul>

<p><strong>Ranked retrieval</strong></p>
<ul>
  <li>
    <p>A ranked retrieval system assigns scores to documents based on how well they match a given search query. (There are many possible ways of scoring.)</p>
  </li>
  <li>
    <p>Based on the score, a ranked retrieval system can return a list of the top documents in the collection with respect to the query.</p>
  </li>
</ul>

<p>前面提到的 boolean retrieval 只考慮了文件是否包含或不包含要的詞彙，為了那個詞彙的「重要性」也考慮進去，我們要給予每個 term 不同的 weight。</p>

<h3 id="term-weighting">Term weighting</h3>

<ul>
  <li>The score of a document d with respect to a query q is the sum of the weights of all terms t that occur in both d and q.</li>
</ul>

<script type="math/tex; mode=display">\textrm{score(d, q)} = \sum_{t \in (d \bigcap q)} \textrm{weight(d, q)}</script>

<ul>
  <li>Any specific way to assign weights to terms is called a term weighting scheme.</li>
</ul>

<p>計算 weight 的方法有很多種，也可以自行定義，這邊介紹一種常用的方法。</p>

<h3 id="tdidf-weight">TD–IDF weight</h3>

<p>TD–IDF is Term frequency–inverse document frequency. This formula includes two parts: Term frequency &amp; Inverse document frequency.</p>

<p><strong>Term frequency</strong></p>

<p>The number of times a term t occurs in a document d is called the term frequency of t in d, and is denoted by tf(t, d).</p>

<p>-<em>A problem with term frequency</em></p>

<p>Relevance is not a linear function of term frequency. 例如，一個出現20次的單詞難道就代表它比一個只出現1次的單詞重要20倍嗎？為了要降低出現頻率造成太大的影響，所以我們將 frequency 取 log，這時候 weight(t, d) 定義成，</p>

<script type="math/tex; mode=display">% <![CDATA[
weight(t, d) = \left\{ \begin{array}{rcl}
1 + log_{10}tf(t, d), & if ~ tf(t, d) > 0 \\
0, & others \\
\end{array}\right. %]]></script>

<p>稱作，<em>log-frequency weighting</em></p>

<p>-<em>Another problem with term frequency</em></p>

<p>時常重複出現的慣用詞彙對一個文件的影響很大。例如，現在想要考慮 Sherlock Holmes 裡包含 Moriarty 和 Holmes 的章節，但當我們下 query 去找時會發現，Holmes 出現最多的章節幾乎沒有 Moriarty，這是因為 Holmes 在這些文本裡出現的比例實在太高了，如果直接以它出現的頻率來分析肯定會造成很大的誤差，進而影響了出現次數少很多的 Moriarty。所以這時候 Inverse document frequency 就非常重要了！</p>

<p><strong>Inverse document frequency</strong></p>

<ul>
  <li>Let N be the total number of documents in the collection.</li>
  <li>The number of documents that contain a term t is called the
document frequency of t, and is denoted by df(t).</li>
</ul>

<p>It denotes by idf(t):</p>

<script type="math/tex; mode=display">\textrm{idf(t)} = log \frac{N}{df(t)}</script>

<p>df(t) 越大代表該詞彙出現在越多文章中，也就是說，如果 df(t) 越常出現，則 idf(t) 就會越小，換句話說，該詞彙的重要性越小。</p>

<p><strong>TD–IDF Weight</strong></p>

<p>現在我們有了 td 和 idf，tf–idf weight 就是將這兩個數字相乘（出現次數乘以重要性），</p>

<p>The <strong>tf–idf weight</strong> of a term t in a document d is defined as</p>

<script type="math/tex; mode=display">\textrm{tf–idf(t, d)} = tf(t, d) \cdot log \frac{N}{df(t)}</script>

<p>where N denotes the number of documents in the collection.</p>

<p><strong>Variations of the tf–idf weighting scheme</strong></p>

<p>因為 df(t) 可能是 0，所以我們改良一下上面的式子。</p>

<p>In scikit-learn, the tf–idf weight is computed as</p>

<script type="math/tex; mode=display">\textrm{tf–idf(t, d)} = tf(t, d) \cdot (log \frac{1 + N}{1 + df(t)} + 1)</script>

<p>where N denotes the number of documents in the collection.</p>

<hr />

<h2 id="the-vector-space-model">The vector space model</h2>

<h3 id="document-representations">Document representations</h3>

<ul>
  <li>
    <p>Documents as sets of terms
<br />
In Boolean retrieval, the only relevant information is whether or not a term is present in a document.</p>
  </li>
  <li>
    <p>Documents as bags of terms
<br />
In ranked retrieval based on term frequency, the only relevant information is how often a term is present in a document.</p>
  </li>
</ul>

<h3 id="the-vector-space-model--idea-1">The vector space model – idea 1</h3>
<p>Represent documents as vectors in a high-dimensional space:</p>

<ul>
  <li>The dimensions (axes) of the space correspond to the terms in
the vocabulary (potentially relevant terms). For example: could be set of all words in the collection, set of most frequent words, …</li>
  <li>The values of the vector components depend on the term weighting scheme: Boolean values, counts, tf–idf values, … (in scikit-learn: CountVectorizer, TfidfVectorizer)</li>
</ul>

<h3 id="the-vector-space-model--idea-2">The vector space model – idea 2</h3>
<p>To rank documents in the vector space model,</p>

<ul>
  <li>we represent the query as a vector in the same space as the documents in the collection</li>
  <li>we compute the score of a candidate document as the similarity between its document vector and the query vector (similarity = proximity in the vector space)</li>
</ul>

<h3 id="cosine-similarity">Cosine similarity</h3>

<p>以下舉一個從一篇<a href="https://raymondyangsite.wordpress.com/2017/05/03/retrieval-model-vector-space-model1/">部落格</a>看到的例子，</p>

<p>例如:
Query = {“Hello”}
Doc1 = {“Foo”, “Foo”}
Doc2 = {“Hello”, “World”}</p>

<p>在經過計算之後，我們知道其向量分別為（可以使用 tf-idf values, counts,… 等等）</p>

<p>Query = (1, 0, 0)
Doc1 = (0, 0, 2)
Doc2 = (1, 1, 0)</p>

<p>我們可以使用歐幾里德距離算出 query 和兩個 documents 的距離分別是，根號5和1。但計算歐幾里德距離忽略掉了單詞出現的頻率的影響。</p>

<p>如果說今天把上面的向量改一下，</p>

<p>Query = (2, 0, 0)
Doc1 = (0, 0, 4)
Doc2 = (2, 2, 0)</p>

<p>這時候 所有的距離都變成了兩倍，但其實他們完全是平行的向量。</p>

<p>那如果我們考慮向量之間的角度呢？也就是計算兩個向量的 dot，這時候會發現，長度還是影響了最後的數值。</p>

<p>因此，使用 <strong>Cosine similarity</strong> 來將數值 normalized。使用 cosine 後，數值只會介於 -1 和 1 之間，並且只需要考慮非0的數值。</p>

<hr />

<h2 id="evaluation-of-information-retrieval-systems">Evaluation of information retrieval systems</h2>

<p>To evaluate an IR system we need:</p>
<ul>
  <li>a document collection</li>
  <li>a collection of queries</li>
  <li>a gold-standard relevance judgement</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Precision%20and%20recall%20for%20Boolean%20retrieval.png" alt="PMF_two coins.png" /></p>

<p><strong>Precision:</strong></p>

<p>Precision (P) is the fraction of retrieved documents that are relevant</p>

<script type="math/tex; mode=display">Precision = \frac{\textrm{#(relevant items retrieved)}}{\textrm{#(retrieved items)}} = P(\textrm{relevant|retrieved})</script>

<p><strong>Recall:</strong></p>

<p>Recall (R) is the fraction of relevant documents that are retrieved</p>

<script type="math/tex; mode=display">Recall = \frac{\textrm{#(relevant items retrieved)}}{\textrm{#(relevant items)}} = P(\textrm{retrieved|relevant})</script>

<p><strong>F1-measure</strong></p>

<p>A good system should balance between precision and recall.   
<br />
The F1-measure is the harmonic mean of the two values:</p>

<script type="math/tex; mode=display">F1 = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}}</script>

<hr />

<p><strong>Lab:</strong> <a href="https://github.com/shihs/732A92-TextMining/blob/master/Lab1/TM-L1.ipynb">Information Retrieval Lab</a></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html">Introduction to Information Retrieval</a>
<br />
<a href="https://taweihuang.hpd.io/2017/03/01/tfidf/">[文件探勘] TF-IDF 演算法：快速計算單字與文章的關聯</a>
<br />
<a href="https://raymondyangsite.wordpress.com/2017/05/03/retrieval-model-vector-space-model1/">Vector Space Model(1)</a>
<br />
<a href="https://www.ycc.idv.tw/confusion-matrix.html">如何辨別機器學習模型的好壞？秒懂Confusion Matrix</a></p>
 -->
    
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//text%20mining/2019/11/05/Text-Mining-Information-Retrieval/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/05/Text-Mining-Information-Retrieval/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//text%20mining/2019/11/05/Text-Mining-Information-Retrieval/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/05/Text-Mining-Information-Retrieval/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/">
        [Machine Learning]Markov Chain Monte Carlo (MCMC)
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on March 12, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">March 12, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>

    </div>
    <!-- <p><strong>MCMC 是什麼？</strong></p>

<p>Markov Chain Monte Carlo （MCMC）是一種抽樣方法，用來解決無法直接抽樣的分佈的隨機抽樣問題。</p>

<p><strong>The Goal of MCMC</strong></p>

<p>We want to sample from some distribution p, or approximate an <script type="math/tex">E[f(x)] ~ where ~ (X \sim p)</script>
<br />
通常 p 是一個很複雜的 distribution，要從這個分佈取 sample 根本不可能，所以這時候就會需要 MCMC。</p>

<hr />

<p><strong>為什麼要使用 MCMC？</strong></p>

<p>在 Baysian Inference 中常使用到這個公式</p>

<script type="math/tex; mode=display">p(\theta\mid y) = \frac{p(y \mid \theta) p(\theta)}{\int p(y \mid \theta) p(\theta)d\theta}</script>

<p>或是，它的參數 <script type="math/tex">\theta</script> 的貝式估計</p>

<script type="math/tex; mode=display">\hat \theta = E[\theta \mid y] = \int \theta p(y \mid \theta) d \theta = \frac{\int \theta p(y \mid \theta) p(\theta)}{\int p(y \mid \theta) p(\theta)d\theta}</script>

<p>其中的 <script type="math/tex">p(\theta)</script> 為先驗機率 (prior probability)，基本上我們無法獲得 <script type="math/tex">p(\theta)</script> 的值，這時候就需要 MCMC。</p>

<hr />

<p>MCMC 由兩部分的觀念 (步驟) 組成, 一個是 「Markov Chain」 ，另一個則是「Monte Carlo integration」，接下來將說明這兩個部分。</p>

<hr />

<p><strong>Monte Carlo Integration</strong></p>

<p>Monte Carlo Integration 可以以抽樣平均的方式計算上面的期望值式子</p>

<script type="math/tex; mode=display">E[\theta \mid y] \approx \frac{1}{n}\sum_{i=1}^{t}{(\theta_i \mid y)}, ~ \theta_i \stackrel{iid}{\sim} p(\theta \mid y)</script>

<p>也就是說，使用樣本平均數來估計期望值。這件事情可以成立是因為，根據大數法則，當樣本數 n 夠大時，樣本的平均數將趨近於母體平均數。</p>

<p>這樣看起來 Monte Carlo Integration 讓我們省去了上面那個看起來複雜的積分式子。但是，現實生活中很多時候並無法從 <script type="math/tex">p(\theta \mid y)</script> 這個 distribution 抽樣，要不就是不知道這個 distribution，要不就是這個 distribution 爆炸複雜啊。</p>

<p>所以說，哪有這麼好的事？</p>

<p>這時候可以採用其他的抽樣方法，譬如，<em>rejection sampling</em>, <em>importance sampling</em> 和本文的重點 <em>MCMC</em>。</p>

<hr />

<p><strong>Markov Chain</strong></p>

<ul>
  <li>
    <p>Markov Chain (馬可夫鍊)：A Markov chain is a sequence <script type="math/tex">X_0, X_1, ...</script> of random variables such that the distribution of the next value depends only on the current on (and parameters). 現在有一隨機變數數列 <script type="math/tex">X_0, X_1, ...</script>，且每一個變數只和前一個變數有關，也就是 <script type="math/tex">X_{t+1}</script> 來自 <script type="math/tex">p(X_{t+1} \mid X_t)</script>，像這樣的數列我們就稱為馬可夫鍊。</p>
  </li>
  <li>
    <p><script type="math/tex">p(X_{t+1} \mid X_t)</script> 被稱為這個馬可夫鍊的轉換核心 (transition kernel)</p>
  </li>
  <li>
    <p>A Markov chain is stationary, with stationary distribution <script type="math/tex">Φ, if ~\forall k ~ X_k  \sim Φ</script></p>
  </li>
  <li>
    <p>One shows (not trivial in general) that under <em>certain</em> conditions a Markov chain will converge to the stationary distribution in the limit. 在一般條件假設底下，馬可夫鍊的變數分配將收斂到目標機率函數 <script type="math/tex">\pi(·)</script> 並且與 <script type="math/tex">X_0</script> 的選擇無關。</p>
  </li>
</ul>

<p><em>Monte Carlo Integration</em>可以看<a href="https://www.youtube.com/watch?v=MKnjsqYVG4Y">這裡</a></p>

<hr />

<p>Reference:
<br />
<a href="https://www.youtube.com/watch?v=3ZmW_7NXVvk">(ML 18.1) Markov chain Monte Carlo (MCMC) introduction-12eZWG0Z5gY.mp4</a>
<br />
<a href="http://web.ntpu.edu.tw/~ccw/statmath/M_mcmc.pdf">The Markov Chain Monte Carlo Simulations</a>
<br /> 
Linköping University - 732A90 Computational Statistics 2019 Lecture 4 slide
<br />
<a href="https://zhuanlan.zhihu.com/p/25610149">[数据分析] Markov Chain Monte Carlo</a>
<br />
<a href="https://www.youtube.com/watch?v=s8w8AsFK77c&amp;list=PLyAft-JyjIYq2SLTHO2ptmx-cChbE5GBm">徐亦达机器学习课程 Markov Chain Monte Carlo</a>
<br />
<a href="http://www.cnblogs.com/pinard/p/6625739.html">MCMC(一)蒙特卡罗方法</a></p>
 -->
    
    <div class="post-excerpt"><p><strong>MCMC 是什麼？</strong></p>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/#disqus_thread';
        this.page.identifier = '/machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/#disqus_thread" data-disqus-identifier="/machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">
        [Machine Learning]Covolutional Neural Networks(CNN)(2)
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on March 2, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">March 2, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>

    </div>
    <!-- <p><a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">前面</a>介紹了 Convolution operation。</p>

<ul>
  <li>Padding</li>
  <li>Strided Convolution</li>
  <li>Convolutions Over Volume
***</li>
</ul>

<h2 id="padding">Padding</h2>

<p><strong>為什麼需要 Padding?</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/padding.png" alt="padding.png" height="400px" width="600px" /></p>

<p>Convolution operation 的缺點：</p>
<ol>
  <li>經過 convolution fitler 後原始圖將變小。如果有很多層 convolutional layer，資訊在每次運算後不斷損失。</li>
  <li>最邊緣的 pixel 只會被使用到一次，但中間的 pixel 會使用到較多次。也就是說，會有邊緣的很多資訊缺失。</li>
</ol>

<p>所以為了解決以上的缺點，我們將 input 的圖的邊緣加上一個 pixel，這樣經過卷積運算後的圖像仍和原圖一樣大。邊緣也是可以加 2 pixel 的。</p>

<p>以上圖為例，原本為 6x6(nxn) 的圖，經過 3x3(fxf) 的 filter 做卷積運算後，變為 (n-f+1)x(n-f+1) 的圖。</p>

<p>但如果今天邊緣加上 p pixel，最後圖檔就會變為 (n+2p-f+1)x(n+2p-f+1) 的圖。</p>

<hr />

<p><strong>The choice of padding - Vaild and Same convolutions</strong></p>

<ol>
  <li>Vaild - No padding</li>
  <li>Same - Pad so that output size is the same as the input size
<br />
根據上面的公式，如果在 input 圖邊緣加上 pixel 後經過卷積運算要和原始圖檔大小ㄧ樣，那要符合 n+2p-f+1 = n，那就會得到 p = (f-1)/2</li>
</ol>

<p>For computer vision, usually f is odd. 3 by 3 filters are very common.</p>

<hr />

<h2 id="strided-convolution">Strided Convolution</h2>

<p>前面我們在做 convolution 時 filter 都是一次跳一格計算，strided 則是一次跳不只一格。</p>

<p>如下圖，這時候最後結果的 pixel 也會不同，計算公式就會是 ((n+2p-f)/2)+1 取 floor。
<img src="http://localhost:4000/blog/img/posts/strided.png" alt="strided.png" height="400px" width="600px" /></p>

<hr />

<p><strong>cross-correlation vs. convolution</strong></p>

<ul>
  <li>cross-correlation：數學家喜歡將 convolutional operation 稱為 cross-correlation。在做運算時會將 filter 做水平與垂直翻轉，如下圖。</li>
  <li>convolution：在 deep learning 通常都稱為 convolution，且不會將 filter 做鏡射的動作。</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/cross-correlation.png" alt="cross-correlation.png" height="400px" width="600px" /></p>

<p>那這樣幹嘛要翻轉？因為在做訊號處理時，翻轉後會有一些好的特性可以拿來使用。但在做 deep learning 時基本上沒有影響，所以做 deep learning 時不會翻轉 filter。</p>

<hr />

<h2 id="convolutions-over-volume">Convolutions Over Volume</h2>

<p><strong>Convolutions on RGB impages</strong></p>

<p>前面舉的例子都是在 2D 上計算 convolution，現在要來看如何在 3D 上實現 convolution。</p>

<hr />

<p>Reference:
<br />
<a href="https://www.coursera.org/learn/convolutional-neural-networks/home/welcome">Coursera - Convolutional Neural Networks(deeplearning.ai)</a></p>
 -->
    
    <div class="post-excerpt"><p><a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">前面</a>介紹了 Convolution operation。</p>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/#disqus_thread';
        this.page.identifier = '/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/#disqus_thread" data-disqus-identifier="/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/">
        [Algorithm]基因演算法Genetic Algorithm
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on March 2, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">March 2, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Algorithm">Algorithm</a>
        
      
    </span>

    </div>
    <!-- <p>基因演算法（Genetic Algorithm）是一種求函數極值的最佳化（函數的最大或最小值）的方法。</p>

<p>它的想法是來自於基因遺傳，透過細胞分裂將好的基因保留，不好的基因淘汰，一代傳一代，最後留下最適合生存的物種，所謂「適者生存」。
實現的方式就像基因的機制那樣，透過<em>隨機</em>選擇後，再進行 crossover 和 mutation，經過多次迭代後，最後結果將會收斂到一個最佳解。但這最佳解不ㄧ定是 global maximum（minimum），通常是 local maximum（minimum）。</p>

<p>（但其實這與基因的真實行為完全無關，就像是 Neural Network 和人類神經反應的真實行為無關一樣。）</p>

<hr />
<h3 id="演算法流程圖">演算法流程圖</h3>
<p><img src="http://localhost:4000/blog/img/posts/genetic%20algorithm.gif" alt="genetic algorithm.gif" height="380px" width="600px" />
<a href="https://dotblogs.com.tw/dragon229/2013/01/03/86692">Source</a></p>

<ol>
  <li>一開始隨機產生n個變數 (n由使用者決定)</li>
  <li>利用適應函數（fitness function）計算所有變數的適應值</li>
  <li>依每個的適應值進行「選擇、複製」</li>
  <li>對留下的變數進行交配（crossover）及突變（mutation）的動作</li>
</ol>

<hr />

<h3 id="範例">範例</h3>

<p>以下範例是來自<a href="http://www.csie.ntnu.edu.tw/~u91029/Optimization.html">演算法筆記的 Optimization</a></p>

<p>1.
[初始化]
一開始先隨便弄出幾個x。本例是四個。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010101010
1011001011
1110101011
0010101000
</code></pre></div></div>

<p>2.
[fitness function]
根據問題特性，定義好壞程度。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f(1010101010) = 678
</code></pre></div></div>

<p>3.
[selection]
隨便找個位置切一刀，每個x都被分成兩段。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010101  010
1011001  011
1110101  011
0010101  000
</code></pre></div></div>

<p>4.
[crossover]
隨便找兩組你覺得夠優良的x，交叉相接變成新答案。
重複一直做，直到x數目跟原先一樣多。本例是四個。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010101 \/ 010  -&gt;  1010101 -- 011
1011001 /\ 011      1011001 -- 010 


1010101011
1011001010
1110101010
1010101000
</code></pre></div></div>

<p>5.
[mutation]
每個x都隨便找一個地方把數字改掉，也可以不改。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010111011
1011001000
1110101010
1010101001
</code></pre></div></div>

<p>6.
重複3. 4. 5.，直到裡面有一個x是你滿意的，令f(x)最大的那個x。</p>
<ol>
  <li>隨機產生N個x。</li>
  <li>計算fitness function。</li>
  <li>重複以下步驟，直到有一個x讓人滿意。
<br />
　甲、selection。
<br />
　乙、crossover。
<br />
　丙、mutation。
<br />
　丁、計算fitness function。</li>
</ol>

<p>一開始的 x 的足夠豐富，多演化幾次就可以得到不錯的結果。一開始的 x 足夠豐富，可以避免進入區域極值。 mutation 用於增加 x 的豐富性，以跳脫區域極值。</p>

<hr />

<p>Reference:</p>

<p><a href="http://www.csie.ntnu.edu.tw/~u91029/Optimization.html">演算法筆記 - Optimization</a>
<br />
<a href="http://littledoa.wixsite.com/pcclab/single-post/2016/02/03/Genetic-Algorithm-基因演算法">Genetic Algorithm 基因演算法</a>
<br />
<a href="https://www.youtube.com/watch?v=UE6YkRWBtZk">人工智慧系列之基因演算法</a></p>
 -->
    
    <div class="post-excerpt"><p>基因演算法（Genetic Algorithm）是一種求函數極值的最佳化（函數的最大或最小值）的方法。</p>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/#disqus_thread';
        this.page.identifier = '/algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/#disqus_thread" data-disqus-identifier="/algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/python/2019/02/27/Python-filter-map-reduce-lambd/">
        [Python]filter、map、reduce、lambda
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on February 27, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">February 27, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Python">Python</a>
        
      
    </span>

    </div>
    <!-- <ul>
  <li>lambda 可以直接定義一些簡單的 funciton。</li>
  <li>filter、map和reduce這三個函數有點像 R 裡頭的 apply 家族系列。</li>
</ul>

<hr />

<p><strong>lambda</strong></p>

<p>我其實覺得就是定義 funciton，只是是定義一些簡單，可以一行完成的 funciton。</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 回傳輸入的變數 + 1</span>
<span class="n">fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
<span class="k">print</span> <span class="p">(</span><span class="n">fun</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="c"># 上面的 func 其實就等於</span>
<span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">+</span><span class="mi">1</span>


<span class="c"># 輸入兩個變數</span>
<span class="n">fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span>
<span class="k">print</span> <span class="p">(</span><span class="n">fun</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>


<span class="c"># 含 if</span>
<span class="n">fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">True</span> <span class="k">if</span> <span class="n">x</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">False</span>
<span class="k">print</span> <span class="p">(</span><span class="n">fun</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>
<hr />

<p><strong>filter(function, sequence)</strong></p>

<p>對 sequence 中的 item 依序執行 function(item)，然後將執行結果為 True 的 item 組成一個 list/string/tuple（與 sequence 類型相同）回傳。
這時候 function 可以使用 lambda 定義或是平常的 def 定義。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 回傳是數字，除 0 外都是 True</span>
<span class="n">fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">-</span><span class="mi">1</span>
<span class="k">print</span> <span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>

<span class="c"># function 為 def</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">!=</span> <span class="mi">0</span> 
<span class="k">print</span> <span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">))</span> <span class="p">)</span>

<span class="c"># function 為 lambda</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">!=</span> <span class="mi">0</span> 
<span class="k">print</span> <span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">))</span> <span class="p">)</span>

</code></pre></div></div>
<hr />

<p><strong>map(function, sequence)</strong></p>

<p>對 sequence 中的 item 依序執行 function(item)，執行結果以 list 回傳。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># funciton 只有一個參數</span>
<span class="n">res</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="n">res</span><span class="p">)</span>
<span class="c"># 每次都回傳 1</span>
<span class="n">res</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="n">res</span><span class="p">)</span>


<span class="c"># function 有兩個參數</span>
<span class="n">res</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div>
<hr />

<p><strong>reduce(function, sequence, starting_value)</strong></p>

<p>對 sequence 中的 item 順序迭代調用 function，最後回傳一個值。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span> <span class="c"># 在 python3 是必須的</span>

<span class="c"># 求 list 的和</span>
<span class="n">res</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div>

<hr />
<p>Reference:
<br />
<a href="https://www.cnblogs.com/longdouhzt/archive/2012/05/19/2508844.html">Python特殊语法：filter、map、reduce、lambda [转]</a></p>
 -->
    
    <div class="post-excerpt"><ul>
  <li>lambda 可以直接定義一些簡單的 funciton。</li>
  <li>filter、map和reduce這三個函數有點像 R 裡頭的 apply 家族系列。</li>
</ul>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//python/2019/02/27/Python-filter-map-reduce-lambd/#disqus_thread';
        this.page.identifier = '/python/2019/02/27/Python-filter-map-reduce-lambd/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//python/2019/02/27/Python-filter-map-reduce-lambd/#disqus_thread" data-disqus-identifier="/python/2019/02/27/Python-filter-map-reduce-lambd/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2019/02/25/Machine-Learning-Reinforcement-Learning/">
        [Machine Learning]Reinforcement Learning
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on February 25, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">February 25, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>

    </div>
    <!-- <h3 id="複習">複習</h3>

<p><strong>機器學習可以被分為三大類：</strong></p>
<ul>
  <li>
    <p>Supervised Learning:
<br />
Learn to generalize and classify new data based on <em>labelled</em> training data. 像是 regression 和 classification 就屬於 Supervised Learning。</p>
  </li>
  <li>
    <p>Unsupervised Learning:
<br />
Discover structure and relationships in complex high-dimensional data. 像是 cluster。</p>
  </li>
  <li>
    <p>Reinforcement Learning:
<br />
Generate policies/strategies that lead to a (possibly delayed) reward. Learn by interacting with the environment! 和人類學習的過程很像，會根據環境而行動，並做調整（像是學騎腳踏車）。這篇主要就是要介紹 Reinforcement Learning。</p>
  </li>
</ul>

<hr />

<p><strong>Reinforcement Learning 可以做什麼？</strong></p>
<ul>
  <li>2016 年 <a href="https://zh.wikipedia.org/wiki/AlphaGo">AlphaGo</a> 擊敗世界棋王使用的就是 Reinforcement Learning</li>
  <li><a href="https://www.youtube.com/watch?v=W_gxLKSsSIE">Robot Learns to Flip Pancakes</a></li>
  <li>pole balancing</li>
</ul>

<p><strong>Differences to other methods</strong></p>
<ul>
  <li>Difference to supervised learning
    <ol>
      <li>Time! Reinforcement Learning 的學習時間會比 Supervised Learning 費時（當然要有缺點，不然大家幹嘛還要用 Supervised Learning），所以如果有 labels 通常還是會選用 Supervised Learning，但當事前沒有 labels，不知道結果的時候就可以使用 Reinforcement Learning。</li>
      <li>Can become better than the system designer, unlike a supervised system that can never become better than the teacher. 因為 Supervised Learning 是根據 system designer 給的 labels 下去計算，但 Reinforcement Learning 是機器自己學習所獲得的結果，就像是，雖然教練教學生打球，但學生的學習成果可能比教練還好。</li>
      <li>Feedback is usually not immediate but is given after many actions - delayed feedback! 像是下棋，要等到結果贏或輸才會知道是得到 positive reward 還是 negative reward。</li>
      <li>Feedback is given as a scalar reward, not as the correct action to make. Supervised Learning 的 feedback 是結果是否正確，但 Reinforcement Learning 得到的 feedback 是 reward（can be positive or negative）。</li>
    </ol>
  </li>
  <li>Difference to control theory
    <ol>
      <li>No physical model of the world, e.g., in pole balancing</li>
    </ol>
  </li>
</ul>

<p><strong>機器怎麼學？</strong></p>

<p>不像 Supervised Learning 有 lable 可以知道結果是正確還是錯誤， Reinforcement Learning 的學習方法是透過 reward 來知道學習結果正確或不正確（就像是狗狗學習技能，如果做對了就給東西吃一樣）。所以透過打分數的結果來知道怎麼樣的行為是正確的而進行調整，往正確的學習道路邁進。（所以每次學習結果所獲得的 reward 就變得有點像是 Supervised Learning 的 labels 的感覺，只是這個 labels 是從學習的過程中獲得的）</p>

<hr />

<h2 id="reinforcement-learning-如何進行">Reinforcement Learning 如何進行？</h2>

<p><img src="http://localhost:4000/blog/img/posts/reinforcement%20learning.png" alt="reinforcement learning.png" height="380px" width="600px" /></p>

<hr />

<p><strong>Reinforcement Learning 問題的基本設定</strong></p>

<p>&lt;A, S, R, P&gt;</p>
<ul>
  <li>Action space : A</li>
  <li>State space : S</li>
  <li>Reward: R : S × A × S → R</li>
  <li>Transition : P :S × A → S</li>
</ul>

<p>A 代表的是 Agent 的所有動作（action）；S 是 state，是 Agent 所能感知的狀態； R 是 reward 代表獎勵或懲罰，是一個實數；P 是 Agent 所交互的世界，也被稱為 model。</p>

<p>根據上圖（<a href="https://www.slideshare.net/yenlung/reinforcement-learning-90737484">Source</a>），電腦是 agent，地球代表 environment（環境），reward（獎勵）是環境所提供的反饋，reward 由模型設計者定義，可以是 positive 或是 negative，reward 的定義對強化學習來說是個很重要的一環。</p>

<p><strong>Policy</strong></p>

<p>Policy 就是 agent 根據每個 state 所做出的不同的 action，也就是根據不同的狀態 agent 會採取不同的「策略」。定義為 <script type="math/tex">\pi</script>，是 RL（Reinforcement Learning） 最核心的問題。可以將 policy 看成是當 agent 感知到 state(S) 採取 action(A) 的 mapping。policy 可以分為隨機的（stochastic policy）和確定性的（deterministic policy）。</p>

<ul>
  <li>stochastic policy: <script type="math/tex">\pi(a\mid s),~ and \sum\pi(a\mid s) = 1</script></li>
  <li>deterministic policy: <script type="math/tex">\pi(s)</script></li>
</ul>

<p><strong>Reward</strong></p>

<p>Reward 是 RL 非常重要的一環，沒有 reward，agent 就不會知道到底學習正確還是錯誤。透過 reward，agent 才知道剛剛採取的 action 好不好。以下圖（<a href="https://zhuanlan.zhihu.com/p/25239682">Source</a>）為例，最一開始的 state 是 <script type="math/tex">s_0</script>，agent 做了 <script type="math/tex">a_0</script> 的 action，這時候產生了一個 reward <script type="math/tex">r_1</script>，然後因為剛剛的 <script type="math/tex">a_0</script> 這時候 state 來到 <script type="math/tex">s_1</script>，而 agent 要採取下一步 <script type="math/tex">a_1</script> 的時候就會根據剛剛的 <script type="math/tex">r_1</script> 來反應。agent 就這樣一步一步根據 reward 的結果來學習。</p>

<p><img src="http://localhost:4000/blog/img/posts/reward.jpg" alt="reward.jpg" height="120px" width="800px" /></p>

<p>但其實並不是所有的學習都會像這個例子一樣立即獲得 reward，像下棋就會是整局結束後才得到 reward。</p>

<p>另外，需要注意的是，Reward <script type="math/tex">\ne</script> Goal。也就是說，agent 的目標並不是「當前」reward 最大，而是「平均累計」回報最大。</p>

<hr />

<p>總結來說，<em>Reinforcement Learning 的目標就是找到一個最佳的 Policy（策略），讓最後平均的 reward 最大！</em></p>

<p>所以現在問題就來了，<em>要如何衡量這個 policy 好不好？哪些 policies 是我們想要 explore 的？</em></p>

<hr />

<h2 id="value-function">Value function</h2>

<p><strong>Value function - How good is a policy?</strong></p>

<p>定義從長期來看 action 平均回報的好壞。例如，象棋中吃掉對方的車看起來即時收益很大，但如果因為吃掉對方的車自己的將卻被吃了，那麼從長期看這個 action 就不是一個好的選擇。</p>

<p><script type="math/tex">V_\pi(s)</script> 表示的是策略 <script type="math/tex">\pi</script>，狀態 s 的長期期望收益。<script type="math/tex">Q_\pi(s, a)</script> 是策略 <script type="math/tex">\pi</script> 在狀態 s 下，採取動作 a 的長期期望收益。</p>

<ul>
  <li>
    <p>長期回報期望：<script type="math/tex">% <![CDATA[
G_t = \sum_{n=0}^{N}\gamma^nr_{t+n}, ~ where ~ 0 < \gamma < 1 %]]></script> （因為 <script type="math/tex">\gamma</script> 介於 0 和 1 之間，所以 makes immediate rewards more important than distant rewards）</p>
  </li>
  <li>
    <p>Value function：<script type="math/tex">V_\pi(s) = E_\pi[G_t\mid S_t = s]</script></p>
  </li>
  <li>
    <p>Q function：<script type="math/tex">Q_\pi(s, a) = E_\pi[G_t\mid S_t = s, A_t = a]</script></p>
  </li>
</ul>

<p><strong>How to learn V(s)?</strong></p>

<ul>
  <li>
    <p>Monte Carlo approach</p>
  </li>
  <li>
    <p>Temporal Difference approach</p>
  </li>
</ul>

<p><strong>Summary</strong></p>
<ul>
  <li>For a given policy, the value (expected reward) V(s) of each state is unknown before we learn it by interacting with the environment.</li>
  <li>V(s) is found iteratively, starting for example with V(s)←0, using the Monte Carlo or Temporal Difference methods.</li>
  <li>The Temporal Difference method generally converges much faster.</li>
</ul>

<hr />

<h2 id="q-learning">Q-Learning</h2>

<p><strong>Which policies should we explore?</strong></p>

<p>上面介紹了 V(s) 是狀態 s 的長期期望收益，那 agent 要如何決定根據它來決定要做什麼 action 採取什麼 policies 呢？這時候我們就會加上 action 這個變數來看，也就是上面提到的 Q(s, a)。</p>

<ul>
  <li><script type="math/tex">V^*(s) = \max_{a} Q(s_{k+1}, a)</script> denote the value function for the optimal policy</li>
  <li><script type="math/tex">Q(s, a)</script> is expected future reward of doing action a in state s and then following the optimal policy <script type="math/tex">Q(s_k, a) = r(s_k, a) + \gamma V^*(s_{k+1})</script></li>
  <li><script type="math/tex">Q(s, a)</script> 未知，必須透過學習得到。</li>
</ul>

<p><script type="math/tex">Q(s_k, a_j) \leftarrow (1 - \eta) Q(s_k, a_j) + \eta (r + \gamma \max_{a} Q(s_{k+1}, a))</script>, <script type="math/tex">\eta</script> is learning rate</p>

<p><a href="https://blog.csdn.net/itplus/article/details/9361915">這篇文章</a>翻譯了這個<a href="http://mnemstudio.org/path-finding-q-learning-tutorial.htm">原文</a>，我覺得非常好理解。但要整篇整理在這裡太麻煩了，所以自己看吧。</p>

<hr />

<p>Reference:
<br />
Linköping University Neural Networks and Learning Systems TBMI26 / 732A55 2019 Lectures.
<br />
<a href="https://zhuanlan.zhihu.com/p/25239682">深度强化学习（Deep Reinforcement Learning）入门：RL base &amp; DQN-DDPG-A3C introduction</a>
<br />
<a href="https://zh.wikipedia.org/wiki/强化学习">wikipedia - 強化學習</a>
<br />
<a href="https://www.slideshare.net/yenlung/reinforcement-learning-90737484">強化學習 Reinforcement Learning</a>
<br />
<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/">莫凡python</a>
<br />
<a href="https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-reinforcement-learning-強化學習-dqn-實作atari-game-7f9185f833b0">[機器學習 ML NOTE] Reinforcement Learning 強化學習(DQN原理)</a>
<br />
<a href="https://www.zhihu.com/question/26408259">如何用简单例子讲解 Q - learning 的具体过程？</a>
<br />
<a href="https://blog.csdn.net/itplus/article/details/9361915">A Painless Q-learning Tutorial (一个 Q-learning 算法的简明教程)</a></p>

 -->
    
    <div class="post-excerpt"><h3 id="複習">複習</h3>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2019/02/25/Machine-Learning-Reinforcement-Learning/#disqus_thread';
        this.page.identifier = '/machine%20learning/2019/02/25/Machine-Learning-Reinforcement-Learning/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2019/02/25/Machine-Learning-Reinforcement-Learning/#disqus_thread" data-disqus-identifier="/machine%20learning/2019/02/25/Machine-Learning-Reinforcement-Learning/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">
        [Machine Learning]Covolutional Neural Networks(CNN)(1)
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on February 25, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">February 25, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>

    </div>
    <!-- <p>這篇為 Coursera 上 <a href="https://www.coursera.org/learn/convolutional-neural-networks/home/welcome">Convolutional Neural Networks</a> 這門課第一週的 Computer Vision、Edge Detection Example 和 More Edge Detection 筆記。</p>

<hr />

<h2 id="前言">前言：</h2>

<p>因為 Deep Learning 的快速發展，所以電腦視覺（computer vision）這幾年也迅速的超展開。像是自動駕駛就受惠於 computer vision 的進步，讓自駕車能更準確的偵測路人和其他車輛，提升安全性；或是人臉辨識系統也變得更厲害，像是手機的人臉辨識解鎖；或是使用 Deep Learning 辨別照片是屬於風景照、人、動物還是車輛等等（像是 iphone 的相簿現在就很變態的會自動歸類照片是什麼）；甚至在藝術作品上都有 Deep Learning 的蹤影。</p>

<hr />

<h2 id="為什麼要使用-covolutional-neural-networkscnn">為什麼要使用 Covolutional Neural Networks(CNN)？</h2>

<h3 id="computer-vision-problem">Computer Vision Problem</h3>

<p><img src="http://localhost:4000/blog/img/posts/Computer%20Vision%20Problem.png" alt="Computer Vision Problem.png" height="400px" width="600px" /></p>

<p>以上圖的例子來看，第一張是 64x64 的照片，第二張畫素則是 1000x1000，這時候如果再加上 RGB 三原色的維度，兩張照片的維度分別會是 64x64x3 和 1000x1000x3。以 Neural Network 來實作的話（右下角），input layer 的 Xn 的 n = 1000x1000x3，如果第一層的 hidden 是 1000 個 nodes，這時候轉換的 W 的維度就會是 1000x3000000，這是不是很 CRAZY？</p>

<p>所以這時候就會碰到以下的問題。</p>

<p><strong>缺點</strong></p>
<ol>
  <li>很難避免 overfitting</li>
  <li>計算上需要大量的電腦效能和 CPU，實在太不切實際了</li>
</ol>

<p>因此，為了解決的這個問題，我們就需要用到 CNN。</p>

<hr />

<h2 id="cnn-的流程圖">CNN 的流程圖</h2>
<p><a href="https://res.mdpi.com/entropy/entropy-19-00242/article_deploy/html/images/entropy-19-00242-g001.png">Sourse</a>
<img src="http://localhost:4000/blog/img/posts/CNN.png" alt="CNN.png" height="300px" width="630px" /></p>

<hr />

<h2 id="how-the-convolution-operation-works">How the convolution operation works?</h2>

<p>現在先來看 convolutional layer 的部分，也是 CNN 非常重要的一個環節。這邊使用 Edge detection 作為範例操作 convolution operation 是如何運作的。</p>

<h3 id="edge-detection-example">Edge detection example</h3>

<p><img src="http://localhost:4000/blog/img/posts/edge%20detect.png" alt="edge detect.png" height="400px" width="600px" /></p>

<p>在辨識下圖時我們可能會想要做垂直的邊緣辨識和水平的邊緣辨識。那要如何操作呢？</p>

<h3 id="vertical-edge-detection">Vertical edge detection</h3>

<p><img src="http://localhost:4000/blog/img/posts/convolutional%20operation.png" alt="convolutional operation.png" height="400px" width="600px" /></p>

<p>最左邊的圖是一張 6x6 pixel 的黑白圖，每個 pixel 的數字表示灰階的深度，數字越小代表顏色越深。中間是一個 3x3 的 filter，在有些地方會被稱為 kernel，但這門課都會稱作 filter。這樣的兩個 matrix 做 convolution 運算後（用 * 表示），會產生一個 4x4 的 matrix。</p>

<p>運算方法則是，將 3x3 的 filter 與圖片的左上方 match（如圖片淺藍底），對應到的格子相乘，最後再將九個數字相加。依序移動 3x3 的 filter，如此最後就會產生一個 4x4 的 matrix。</p>

<p><img src="http://localhost:4000/blog/img/posts/convolution%20operation%20result.png" alt="convolution operation result.png" height="150px" width="210px" /></p>

<p>而中間的 filter 可以看到是一個由左至右從淺到深的圖。因為現在做的是 Vertical edge detection，所以才會選擇這樣的 filter。</p>

<p>再看一個例子，
<img src="http://localhost:4000/blog/img/posts/vertical%20edge%20detection.png" alt="vertical edge detection.png" height="400px" width="600px" /></p>

<p>我們可以看到，最右邊最後產生的是一個中間一條白色的圖，這就偵測到了我們的想找垂直邊緣的圖（最左邊）的正中間有個邊界。（這個例子看起來邊緣很寬，是因為我們這個圖只有 6x6，如果今天用大一點的圖 1000x1000 就會發現這樣的偵測效果是很好的）</p>

<hr />

<h3 id="different-transitions">Different transitions</h3>

<p><img src="http://localhost:4000/blog/img/posts/vertical%20detection%20different%20transitions.png" alt="vertical detection different transitions.png" height="400px" width="600px" /></p>

<p>剛剛的例子是由亮到暗（上），現在的例子是由暗到亮（下），會發現時候最後運算出來的 matrix 也會不同，根據中間的數字我們可以知道，這張圖是由亮到暗還是由暗到亮。下圖的中間偵測得到邊緣數字是 -30，如果今天需要偵測的結果需要知道深淺的變化，那數字就是重要的，但如果今天只是要抓邊緣，也可以將 -30 取絕對值，一樣能抓到轉換的邊界。</p>

<hr />

<h3 id="vertical-and-horizontal-edge-detection">Vertical and horizontal edge detection</h3>

<p>看完了垂直的範例，我們再看一下偵測水平邊緣的 filter。
<img src="http://localhost:4000/blog/img/posts/vertical%20and%20horizontal%20detection.png" alt="vertical and horizontal detection.png" height="400px" width="600px" /></p>

<hr />

<h2 id="different-filters">Different filters</h2>

<p>從上面的結果可以發現，不同的 filter 可以讓我們偵測出不同的狀況。這邊提了一下兩個 filter，<em>Sobel filter</em> 和 <em>Scharr filter</em>。其實我們可以不需要直接使用現成的 filter，而是可以訓練出一個 filter 針對我們的需求，這之後會再提。</p>

<hr />

<p>Reference:
<br />
<a href="https://www.coursera.org/learn/convolutional-neural-networks/home/welcome">Coursera - Convolutional Neural Networks(deeplearning.ai)</a></p>
 -->
    
    <div class="post-excerpt"><p>這篇為 Coursera 上 <a href="https://www.coursera.org/learn/convolutional-neural-networks/home/welcome">Convolutional Neural Networks</a> 這門課第一週的 Computer Vision、Edge Detection Example 和 More Edge Detection 筆記。</p>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/#disqus_thread';
        this.page.identifier = '/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/#disqus_thread" data-disqus-identifier="/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/others/2019/01/22/ASCII-Unicode-%E5%92%8C-UTF-8/">
        字元編碼，ASCII、Unicode、UTF-8
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on January 22, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">January 22, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Others">Others</a>
        
      
    </span>

    </div>
    <!-- <p>關於編碼，相信大家很常聽到 ASCII、Unicode、UTF-8 這幾個名詞，這篇要根據我的理解來介紹這三個名詞。若有錯歡迎指正:)
<br />
2022/03/15 上完 Harvard CS50 的 Lecture 0 後做了一些新增與修改。</p>

<hr />

<h2 id="字元編碼">字元編碼</h2>

<ul>
  <li>在電腦的世界是使用二進位（binary）的方式來進行運算，也就是 0 與 1 兩個 digits（each binary digit called bit）。</li>
  <li>而電腦是使用「關」與「開」來表示二進位中的 0 與 1。</li>
  <li>在現代電腦中是由上百萬的電晶體（transistors）來控制這樣的「關」與「開」（0 與 1）。</li>
</ul>

<hr />

<h2 id="ascii">ASCII</h2>
<p><a href="https://zh.wikipedia.org/wiki/ASCII">ASCII</a> 是電腦早期發展時由美國制定的一套編碼規則，且沿用至今。它的目的是用來統一規範，規定數字與字母大小寫、符號等等的對應關係，而每一個數字由 8 個 bits 表示。最後一次更新是在1986年，至今為止共定義了 128 個字元。</p>

<p>像是 <code class="highlighter-rouge">HI!</code> 就是由數字 72、73 和 33 所組成的。由二進位表示就是，<code class="highlighter-rouge">01001000</code>、<code class="highlighter-rouge">01001001</code>、<code class="highlighter-rouge">00100001</code>。</p>

<p><strong>Extended ASCII</strong>
<br />
但因為一個 byte 共有 8 個 bits，也就是說還有很多個位元並未使用到，所以後來又產生了 Extended ASCII。很多歐洲國家有一些字母並未在 ASCII 中，所以除了 ASCII 為固定的字元外，每個國家根據自己所需給與字元，但也造成每個國家並未統一編碼。（<a href="https://zh.wikipedia.org/zh-tw/EASCII">Wikipedia</a>）</p>

<hr />

<h2 id="unicode">Unicode</h2>
<p>隨著網際網路的發展，世界各國交換訊息，未統一的編碼便造成了很大的問題。這時候有了 Unicode 的誕生。</p>

<p>Unicode 協會將所有的文字與符號都分配一個數字，這個數字的寫起來像是，U+0645，而這個數字就稱為 Code point。U+ 的意思是 Unicode，
數字則是用十六進位表示。</p>

<p>但 Unicode 編碼使用至少兩個位元儲存字元，若是今天只有英文，仍舊是每個字元使用兩個位元儲存，這表示浪費了很多的資源。所以這時候有有人想出了更好的辦法。</p>

<hr />

<h2 id="utf-8">UTF-8</h2>
<p>有人發明了 UTF-8 這種儲存方式。</p>

<p>UTF-8 編碼把一個 Unicode 字元根據不同的數字大小編碼成 1-6 個字節，常用的英文本母被編碼成 1 個字節，漢字通常是 3 個字節。如果今天要傳輸的內容都是英文，這時使用 UTF-8 編碼就能省下空間。</p>

<p><strong>比較</strong>
<br />
Unicode 是一種編碼方式，和 ASCII 是同一個概念，而 UTF-8 是一種存儲方式，在存儲和傳輸上節約空間、提高性能的一種編碼形式</p>

<hr />

<p>Reference:
<br />
<a href="http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html">字符编码笔记：ASCII，Unicode 和 UTF-8</a>
<br />
<a href="http://blog.chunnorris.cc/2015/04/unicode.html">淺談電腦編碼與 Unicode (一) 基礎概念篇</a>
<br />
<a href="https://hk.saowen.com/a/6615242f89d424bc28e35ea8efc115707b2ec7dcf43bb52491bca9805f99a118">常見三種字符編碼的區別：ASCII、Unicode、UTF-8</a>
<br />
<a href="https://www.pttweb.cc/bbs/ISU_CSIE94B/M.1152573430.A.88F">[轉錄] [doc] 每個軟體開發者都絕對一定要會的Unicode及字元集必備知</a>
<br />
<a href="https://cs50.harvard.edu/x/2022/notes/0/">CS50 2022 - Lecture 0</a></p>
 -->
    
    <div class="post-excerpt"><p>關於編碼，相信大家很常聽到 ASCII、Unicode、UTF-8 這幾個名詞，這篇要根據我的理解來介紹這三個名詞。若有錯歡迎指正:)
<br />
2022/03/15 上完 Harvard CS50 的 Lecture 0 後做了一些新增與修改。</p>

</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//others/2019/01/22/ASCII-Unicode-%E5%92%8C-UTF-8/#disqus_thread';
        this.page.identifier = '/others/2019/01/22/ASCII-Unicode-%E5%92%8C-UTF-8/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//others/2019/01/22/ASCII-Unicode-%E5%92%8C-UTF-8/#disqus_thread" data-disqus-identifier="/others/2019/01/22/ASCII-Unicode-%E5%92%8C-UTF-8/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
</div>


<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page5">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page3">Newer</a>
    
  
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
