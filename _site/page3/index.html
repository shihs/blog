<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- 數學符號 -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2019. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts" id="begin">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2018/12/09/Machine-Learning-Regression-splines/">
        [Machine Learning]Regression splines
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 9, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 9, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <p><img src="http://localhost:4000/blog/img/posts/splines%20scatter%20plot.png" alt="splines scatter plot.png" height="380px" width="600px" /></p>

<p>上圖是一個 x, y 的分佈圖，紅線是這些點分部的方程式。但現實中，我們並無法真的知道紅線的方程式，我們可能使用一次方程式、二次方程式甚至更高次方的方程式去嘗試（如下圖
）。
<img src="http://localhost:4000/blog/img/posts/splines%20line.png" alt="splines line.png" height="380px" width="600px" /></p>

<p>我們可以將擬合的多項式方程式（polynomial function）寫成，</p>

<p>\(y = \beta_0 + \beta_1 x^1 + \beta_2 x^2 + … + \beta_n x^n + \epsilon\)</p>

<p>但只是一味的提高多項式的次方只是增加模型的複雜度會導致 overfitting 的問題，在 testing data 上的結果也不會太好。</p>

<p>這時候我們可以使用 Piecewise 將 data 劃分成多個區間，根據每個區間的 data 給予一個模型去擬合。</p>

<hr />

<p><strong>Basis function</strong></p>

<p>\(y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + … + \beta_n b_n(x_i) + \epsilon_i\)</p>

<p>透過 \(b_1(X)\)、\(b_2(X)\)、……、\(b_n(X)\) 可以將非線性的自變量轉換成線性。</p>

<hr />

<p><strong>Piecewise Function</strong></p>

<p>將 data 劃分多個區段後，每個區段再各自找到可以擬合的 model，model 可以是一次方程式、二次方程式或是三次方程式等等，不過三次方最常使用。</p>

<p>如下圖每個區段都是用一次方程式去擬合。\(\xi\) 為區段的分隔點，稱為 knot，每個分段函數稱為 piecewise function。</p>

<p><img src="http://localhost:4000/blog/img/posts/piecewise%20linear.png" alt="piecewise linear.png" height="600px" width="500px" />
From: 《Elements of Statistical Learning》</p>

<p>但這些 piecewise function 是有條件的。
<br />
雖然 piecewise function 是每個區段各自擬合出來的 function，但所有區段 function 必須整個為連續，也就是在 \(\xi\) 的交界處的值必須相同。</p>

<hr />

<p><strong>Cubic Spline</strong></p>

<p>這裡則是使用三次方程式。</p>

<p><img src="http://localhost:4000/blog/img/posts/piecewise%20cubic%20polynomials.png" alt="piecewise cubic polynomials.png" height="600px" width="500px" />
From: 《Elements of Statistical Learning》</p>

<p>cubic spline 除了邊界的值相同外，還必須要一階和二階倒數相同。</p>

<p>看上圖左上的圖加上邊界連續後成為右上，雖然看起來是連續的函數，但並不是完美的曲線，如果再加上一階導數相同就變成左下，再加上二階導數就可以畫出右下的圖。</p>

<hr />

<p>這個 R code 是畫出最上面圖的程式碼，使用 <a href="https://www.youtube.com/watch?v=bESJ81dyYro">Introduction to Splines</a> 裡頭的範例。</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">100</span><span class="p">)</span><span class="w">

</span><span class="c1"># function</span><span class="w">
</span><span class="n">f</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">f_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.2</span><span class="o">*</span><span class="n">x</span><span class="o">^</span><span class="m">11</span><span class="o">*</span><span class="p">(</span><span class="m">10</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="o">^</span><span class="m">6</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">10</span><span class="o">*</span><span class="p">(</span><span class="m">10</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">^</span><span class="m">3</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">^</span><span class="m">10</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="n">from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">500</span><span class="p">)</span><span class="w">
</span><span class="n">f_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="n">eps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># epsilon</span><span class="w">
</span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">f_x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eps</span><span class="w">

</span><span class="n">d</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
  </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w">
  </span><span class="n">f_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f_x</span><span class="p">,</span><span class="w">
  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># plot</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f_x</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.8</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="w">

</span><span class="n">ggplot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lm"</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_smooth</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lm"</span><span class="p">,</span><span class="w"> </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">I</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"green"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="c1"># stat_smooth(method = "lm", formula = y ~ x + I(x^2) + I(x^3), se = FALSE, colour = "gold1") +</span><span class="w">
  </span><span class="c1"># stat_smooth(method = 'loess', se = FALSE, colour = "red") +</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<hr />

<p>Reference:
<br />
<a href="https://zhuanlan.zhihu.com/p/34825299">一文读懂回归样条（regression splines），附Python代码</a>
<br />
<a href="https://www.youtube.com/watch?v=bESJ81dyYro">Introduction to Splines</a>
<br />
<a href="https://www.youtube.com/watch?v=V1JRs6AP1AI">Spline Regression | Non Linear Model | Polynomial Regression</a>
<br />
<a href="https://zh.wikipedia.org/wiki/样条函数">wikipedia - 樣條函數</a>
<br />
<a href="https://datascienceplus.com/cubic-and-smoothing-splines-in-r/">Cubic and Smoothing Splines in R</a>
<br />
<a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf">《An Introduction to Statistical Learning with Applications in R》</a></p>
 -->
    <div class="post-excerpt"><p><img src="http://localhost:4000/blog/img/posts/splines%20scatter%20plot.png" alt="splines scatter plot.png" height="380px" width="600px" /></p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2018/12/09/Machine-Learning-Regression-splines/#disqus_thread';
        this.page.identifier = '/machine%20learning/2018/12/09/Machine-Learning-Regression-splines/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2018/12/09/Machine-Learning-Regression-splines/#disqus_thread" data-disqus-identifier="/machine%20learning/2018/12/09/Machine-Learning-Regression-splines/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2018/12/08/Machine-Learning-Principal-Component-Analysis(PCA)/">
        [Machine Learning]Principal Component Analysis(PCA)
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 8, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 8, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <p><img src="http://localhost:4000/blog/img/posts/PCA.png" alt="PCA.png" height="260px" width="600px" /></p>

<p>From: «Pattern Recognition and Machine Learning» P.561</p>

<hr />

<p><strong>What is PCA?</strong></p>

<p>Principal Component Analysis(PCA)，中文翻作「主成分分析」。</p>

<p>PCA 是一種將多維度降維的方法。
<br />
一個變數其實可能是多個潛在變因（laten variables）組成，但我們無法實際測量出那些 laten variables，而 PCA 就是要拆解出影響較大的變因。使用較少的變數解釋多個 variables。</p>

<p>舉個例子，股市的點數上上下下，我們所能觀測到的是點數這個變數，但其實影響點數變動的潛在變因可能包含了很多市場因素，且每個因素可能又是互相影響。</p>

<p>如果用數學符號表示，\(x_i\) 是我們有的變數，\(z_i\) 是裡頭含有的潛在變因，\(x_i\) 是 \(z_i\) 的線性組合（linear combination）。
<br />
\(x_1 = a_{11} z_1 + a_{12} z_2 + a_{13} z_3 + \epsilon_1\)
<br />
\(x_2 = a_{21} z_1 + a_{22} z_2 + a_{23} z_3 + \epsilon_2\)
<br />
\(x_3 = a_{31} z_1 + a_{32} z_2 + a_{33} z_3 + \epsilon_3\)
<br />
……</p>

<p>可以將上式改寫成，
<br />
\(z_1 = x_1 u_{i1} + x_2 u_{i2} + x_3 u_{i3}\)
<br />
……</p>

<hr />

<p><strong>如何降維？</strong></p>

<p>根據 «Pattern Recognition and Machine Learning» 這本書第 561 頁給了 PCA 兩種定義</p>
<ol>
  <li>PCA can be defined as the orthogonal projection of the data onto a lower dimensional linear space, known as the principal subspace, such that the variance of the projected data is maximized (Hotelling, 1933).</li>
  <li>PCA can be defined as the linear projection that minimizes the average projection cost, defined as the mean squared distance tbtween the data points and their projections (Pearson, 1901).</li>
</ol>

<p>可以用上圖來理解，或是 <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ#t=4m35s">StatQuest: Principal Component Analysis (PCA), Step-by-Step</a> 這段。</p>

<p>根據上面的定義，可以看到，降維的方法是要做 <a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-algebra-projections-onto-subspaces">orthogonal projection</a>，且找到投影向量讓投影後的資料變異量最大。</p>

<p>這邊我使用 <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ">StatQuest: Principal Component Analysis (PCA), Step-by-Step</a> 影片的內容來介紹。</p>

<ul>
  <li>maximized 什麼？
<img src="http://localhost:4000/blog/img/posts/PCA_maximizing.png" alt="PCA_maximizing.png" height="320px" width="600px" /></li>
</ul>

<p>假如現在座標上有個綠色的點，以座標\((0, 0)\)原點，其長度為 a（影片沒有表示這是 a 向量，方便起見，後面以 a, b, c 向量表示），c 向量為 a 向量的投影方向與長度。根據畢氏定理，可以畫出一個正三角形，現在三角形的三邊長分別為 a, b, c（因為我懶得再圖上修正了，現在又變回長度）。而 PCA 要找的投影向量就是，最小化 \(b^2\) 的值，或是最大化 \(c^2\) 的值。</p>

<ul>
  <li>投影</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/PCA_projection.png" alt="PCA_projection.png" height="320px" width="600px" /></p>

<p>根據上圖，這個二維座標上有好幾個點，我們現在就是要找到一條能讓投影後 variance 最大的投影向量（紅色虛線），如下圖。要找到 SS（eigenvalue） 最小的投影向量。</p>

<p><img src="http://localhost:4000/blog/img/posts/SS.png" alt="SS.png" height="320px" width="600px" /></p>

<p>在投影前，我們會先將資料平移 \(x_i - \mu_i\)，也就是不改變點之間的相對位置，這樣不但不會影響找投影向量的結果，在計算上也比較容易。</p>

<hr />

<p><strong>Variation 變異量</strong></p>

<p>變異量 = \(SS/(n-1), n \)是點的數量</p>

<p>有了變異量以後，我們通常會想要知道每個投影向量的變異量占比。</p>

<p>假如現在有 PC1 其變異量為 15，PC2 的變異量為 3，則 PC1 與 PC2 的變異量總和為 18。</p>

<p>所以 PC1 = 15*100%/18 = 83%，PC2 = 3*100%/18 = 17%。</p>

<p>在做 PCA 的時候，我們會根據轉換的 \(PC_i\) 的比重，來決定要考慮要使用幾個 \(PC_i\)。
如果可以解釋百分之九十基本上就可以拿來使用。</p>

<p>假如今天有一筆多維度的資料，但轉換後 PC1 與 PC2 可以表示百分之九十的變異量，那麼這時候只要使用 PC1 與 PC2 就好，且還可以在平面座標上看點的分佈。</p>

<hr />

<p>基本上 PCA 就是在做座標變換，將原變數投影成新變數。接著以最少的新變數來代表原始資料最大的成分（variation 涵蓋量最大）。</p>

<p>其原則如下</p>
<ul>
  <li>新變數是原變數的線性組合</li>
  <li>保留原變數間的最大變異量（variance）</li>
</ul>

<hr />

<p>Reference:
<br />
<a href="https://www.youtube.com/watch?v=FgakZw6K1QQ">StatQuest: Principal Component Analysis (PCA), Step-by-Step</a>
<br />
<a href="https://medium.com/@chih.sheng.huang821/機器-統計學習-主成分分析-principle-component-analysis-pca-58229cd26e71">機器/統計學習:主成分分析(Principal Component Analysis, PCA)</a>
<br />
<a href="https://zh.wikipedia.org/wiki/主成分分析">wikipedia - 主成分分析</a>
<br />
The Elements of Statistical Learning
<br />
Pattern Recognition and Machine Learning</p>
 -->
    <div class="post-excerpt"><p><img src="http://localhost:4000/blog/img/posts/PCA.png" alt="PCA.png" height="260px" width="600px" /></p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2018/12/08/Machine-Learning-Principal-Component-Analysis(PCA)/#disqus_thread';
        this.page.identifier = '/machine%20learning/2018/12/08/Machine-Learning-Principal-Component-Analysis(PCA)/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2018/12/08/Machine-Learning-Principal-Component-Analysis(PCA)/#disqus_thread" data-disqus-identifier="/machine%20learning/2018/12/08/Machine-Learning-Principal-Component-Analysis(PCA)/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2018/12/03/Machine-Learning-Boosting-and-Adaboost/">
        [Machine Learning]Boosting and Adaboost
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 3, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 3, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <h2 id="boosting">Boosting</h2>

<p><img src="http://localhost:4000/blog/img/posts/boosting.png" alt="boosting.png" height="230px" width="600px" /></p>

<p>From: <a href="https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/">Introduction to Boosted Trees</a></p>

<p>Boosting 也是一種 Ensemble learning，它會結合多個弱分類器（weak classifiers）成為一個較準確的分類器，也可以應用在 regression 上。</p>

<hr />

<p><strong>和 Bagging 有什麼不同？</strong></p>

<p>Bagging 各個 classifier 產生的過程是獨立的，但 Boosting 後產生的 classifier 與前面的 classifier 有關。也就是說，Bagging 的 classifier 可以並行產生，但 Boosting 必須要有順序的產生。所以時間上來說，Bagging 可以節省比較多的時間。</p>

<hr />

<p><strong>與前面的分類器有關？</strong></p>

<p>在 Boosting 中，每一次產生 classifier 後，後面的 classifier 會根據前面 classifier 的結果調整每個點的權重。</p>

<p>在前一個 classifier 分類錯誤後，在後一個 classifier 的權重會比較重，而表現較好的則會權重減少。這就是和 Bagging 最大的不同，Bagging 中所有的點都是隨機選取，且權重都是一樣的。</p>

<p>簡單來說，Boosting learns features from data.</p>

<hr />

<p><strong>步驟</strong></p>

<ol>
  <li>用最原始的 training data 跑一個 classifiers</li>
  <li>利用這個 classifiers 提高分類錯誤的點的權重，降低分類正確的點的權重。</li>
  <li>重複第二步驟 N 次，最後使用權重的平均值。</li>
</ol>

<hr />

<h2 id="adaboost">AdaBoost</h2>

<p><img src="http://localhost:4000/blog/img/posts/AdaBoost.png" alt="AdaBoost.png" height="350px" width="700px" /></p>

<p>From: <a href="https://medium.com/@chih.sheng.huang821/機器學習-ensemble-learning之bagging-boosting和adaboost-af031229ebc3">機器學習: Ensemble learning之Bagging、Boosting和AdaBoost</a></p>

<p>AdaBoost 應用了 boosting 的方法。</p>

<p><strong>概念</strong></p>

<p>假設現在有 n 個樣本，要執行 boosting T 次。</p>

<p>第一次將所有樣本的權重都設為 \(\alpha_i\) = 1/n, i = 1, …, n</p>

<p>對所有 t = 1, …, T</p>
<ol>
  <li>根據樣本的權重 \(\alpha_i\) 建立出 classifier \(f_t(x)\)</li>
  <li>使用 \(f_t(x)\) 後計算 \(\epsilon_t\) 誤差</li>
  <li>利用 \(\epsilon_t\) 算出係數 \(w_t\)</li>
  <li>再重新計算 \(\alpha_i\)</li>
</ol>

<p>最後的 model 就會是：</p>

<p>\(\hat{y} = sign \sum_{t=1}^{T} w_i f_t(x)\)</p>

<p>AdaBoost 的優勢就是讓 model 能夠從錯中學，使用提升與降低權重的方式讓分類錯誤的點可以在下一次的 model 中被改進。</p>

<hr />

<p>Reference:
<br />
<a href="http://leijun00.github.io/2014/10/decision-tree-2/">决策树（二）</a>
<br />
<a href="https://medium.com/@chih.sheng.huang821/機器學習-ensemble-learning之bagging-boosting和adaboost-af031229ebc3">機器學習: Ensemble learning之Bagging、Boosting和AdaBoost</a>
<br />
<a href="https://blog.csdn.net/niuniuyuh/article/details/54346930">AdaBoost和随机森林的区别</a>
<br />
<a href="https://www.coursera.org/learn/ml-classification/home/week/5">Machine Learning -  University of Washington</a>
<br />
<a href="https://blog.csdn.net/mach_learn/article/details/39501849">机器学习算法优缺点及其应用领域</a></p>
 -->
    <div class="post-excerpt"><h2 id="boosting">Boosting</h2>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2018/12/03/Machine-Learning-Boosting-and-Adaboost/#disqus_thread';
        this.page.identifier = '/machine%20learning/2018/12/03/Machine-Learning-Boosting-and-Adaboost/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2018/12/03/Machine-Learning-Boosting-and-Adaboost/#disqus_thread" data-disqus-identifier="/machine%20learning/2018/12/03/Machine-Learning-Boosting-and-Adaboost/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2018/12/03/Machine-LearningDecision-Tree/">
        [Machine Learning]Random Forest
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 3, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 3, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <p>基本上 Random Frost 是要改善 Decision Tree 容易 overfitting 的問題，它結合了 Decision Tree 和 bagging 的方法。</p>

<p>Random Forest 是建立很多顆決策樹，再利用多數決選出最好的選項，和 <a href="https://shihs.github.io/blog/machine%20learning/2018/12/03/Machine-Learning-Bagging/">bagging</a> 這篇提到的方法有點類似，但有些小差異。</p>

<p>使用的方法是 bagging（結合多個 model），所以這也是 Ensemble learning。</p>

<hr />

<p><strong>Random Forest步驟</strong></p>

<p>假如有一 training data，有 N 個樣本，p 個features。
<br />
今天要利用這個 training data 建立一個 Random Forest model，裡頭共有 B 棵決策樹，</p>

<ol>
  <li>使用 boostrap 從 training data 中抽出 N 個樣本產生一組 data</li>
  <li>在這組 data 中隨機從 p 個 featrues 中選取 m (m &lt; p) 個 features，再從這 m 個 features 找出最好的一個分割結果，如此產生一個 node</li>
  <li>重複步驟 2，直到完成這個 model 為止</li>
  <li>重複步驟 1~3 B 次，共會產生 B 棵擁有不同 feature 的決策樹</li>
</ol>

<p>最後要進行 predict 時，分類問題使用多數決，回歸問題使用平均數決定。</p>

<hr />

<ul>
  <li>因為每一棵樹的隨機選取的樣本與 feature 都不同，所以每棵樹的結果都不會相同。</li>
  <li>Random Frost 建立的 decision tree 不需要 pruning。在 decision tree 剪枝是爲了避免 overfitting，但在 Random Frost 使用 bagging 的方式就已經避免 overfitting 了。</li>
  <li>因為每一棵決策樹都是隨機篩選 feature 的結果，所以可以想像每棵樹就像是精通某個領域的專家。當有個新的數據近來，經由各個領域的專家投票表決，做出最後的選擇。</li>
  <li>
    <p>Random Forest 中有兩個參數需要人為控制，一個是樹的數量（B），一般建議取很大。另一個是 feature 的大小（m）。</p>
  </li>
  <li>優點：
    <ol>
      <li>不用做特徵（feature）選擇。</li>
      <li>訓練完後可以知道哪些 feature 比較重要。</li>
    </ol>
  </li>
</ul>

<hr />

<p>Reference:
<br />
<a href="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ">StatQuest: Random Forests Part 1 - Building, Using and Evaluating</a>
<br />
<a href="https://blog.csdn.net/edogawachia/article/details/79357844">随机森林（Random Forest）算法原理</a>
<br />
<a href="https://blog.csdn.net/niuniuyuh/article/details/54346930">AdaBoost和随机森林的区别</a></p>
 -->
    <div class="post-excerpt"><p>基本上 Random Frost 是要改善 Decision Tree 容易 overfitting 的問題，它結合了 Decision Tree 和 bagging 的方法。</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2018/12/03/Machine-LearningDecision-Tree/#disqus_thread';
        this.page.identifier = '/machine%20learning/2018/12/03/Machine-LearningDecision-Tree/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2018/12/03/Machine-LearningDecision-Tree/#disqus_thread" data-disqus-identifier="/machine%20learning/2018/12/03/Machine-LearningDecision-Tree/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2018/12/03/Machine-Learning-Bagging/">
        [Machine Learning]Bagging
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 3, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 3, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <h2 id="bagging">Bagging</h2>

<p><img src="http://localhost:4000/blog/img/posts/bagging.png" alt="bagging.png" height="450px" width="500px" /></p>

<p>From: Hastie, T., Tibshirani, R. and Friedman, J. The Elements of Statistical Learning. Springer, 2009. p.285</p>

<hr />

<p><strong>Bagging 是什麼？</strong></p>

<p>Bootstrap aggregating (bagging) is a machine learning ensemble meta-algorithm to improve classification and regression models in terms of stability and classification accuracy. It also reduces variance and helps to avoid ‘overfitting’. Although it is usually applied to decision tree models, it can be used with any type of model. (<a href="https://docs.rapidminer.com/latest/studio/operators/modeling/predictive/trees/parallel_decision_tree.html">Decision Tree - Bagging</a>)</p>

<p>簡單來說，Bagging是一種 Ensemble learning方法，它集結一些 model 來做最後的決策。通常會是集結表現沒那麼好的 model，讓這些 model 一起做決定，一種三個臭皮匠勝過一個諸葛亮的感覺。</p>

<p>Decision tree 是一個常見的 weak classifier，所以如果在 decision tree 上使用 bagging，可以讓最後表現結果比較好，也避免 overfitting 的情況。</p>

<hr />

<p><strong>Boostrap</strong></p>

<p>在開始說明 bagging 步驟前先介紹 bootstrap。</p>

<p>bootstrap 是一種抽樣方法。假設今天有一組資料，裡頭共有 N 個樣本，我們想要有 m 個大小為 N 的樣本作為訓練資料。方法是，每次從這 N 個樣本隨機抽取，且每次都是取後放回(也就是有些樣本可能被抽到一次以上，有些樣本可能沒被抽到)，同樣的方法重複 m 次，這樣我們就會有 m 組樣本數為 N 的 y 資料集。</p>

<p>這樣的方法在樣本數量少時很有用。如果樣本小，但我們用 train-validation-test 這樣的方式訓練資料，訓練的樣本資料非常小，會造成 bias 較大的問題。而使用 bootstrap 不會減少樣本的數量，也能保留 test data。</p>

<hr />

<p><strong>如何操作？</strong></p>

<ol>
  <li>使用 boostrap 方法從 training data 中採集 B 組樣本數與 training data 相同的資料集。</li>
  <li>這 B 組資料集都建立一個 model， \(f_b(x)\)，共產生 B 個 model。</li>
  <li>最後預測的結果就是將這 B 個 model 做統合。在分類問題上，可以平均各個 model 的 posterior class probabilities，或是使用 majority voting。（選機率比較大的結果或是多數決）；在回歸上，則取平均值。</li>
</ol>

<p>從最上圖可以看到，不論是使用機率或是投票決定，bagging 選出的結果的 test error 都要比個別的 model 表現較好。</p>

<hr />

<p>除了 Bagging 外，Ensemble learning 還有另一種常見的方法 Boosting，這篇先到這裡之後繼續介紹。</p>

<hr />

<p>Reference:
<br />
<a href="https://medium.com/@chih.sheng.huang821/機器學習-ensemble-learning之bagging-boosting和adaboost-af031229ebc3">機器學習: Ensemble learning之Bagging、Boosting和AdaBoost</a>
<br />
<a href="https://blog.csdn.net/edogawachia/article/details/79357844">随机森林（Random Forest）算法原理</a>
<br />
The Elements of Statistical Learning
<br />
Pattern Recognition and Machine Learning</p>

 -->
    <div class="post-excerpt"><h2 id="bagging">Bagging</h2>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2018/12/03/Machine-Learning-Bagging/#disqus_thread';
        this.page.identifier = '/machine%20learning/2018/12/03/Machine-Learning-Bagging/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2018/12/03/Machine-Learning-Bagging/#disqus_thread" data-disqus-identifier="/machine%20learning/2018/12/03/Machine-Learning-Bagging/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2018/12/03/Machine-Learning-Decision-Tree-%E5%92%8C-bagging/">
        [Machine Learning]Decision Tree
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 3, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 3, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <h2 id="decision-tree">Decision Tree</h2>
<p><img src="http://localhost:4000/blog/img/posts/decision_tree.png" alt="decision_tree.png" height="190px" width="500px" /></p>

<hr />

<p><strong>Decision Tree是什麼？</strong></p>

<p>簡單來說，decision tree 是一個分類模型。</p>

<ul>
  <li>Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one.</li>
</ul>

<p>這是 Pattern Recognition and Machine Learning 裡頭的定義。</p>

<p>根據上圖，想像一下，在右邊的二維座標平面上有一堆散布的點，而 decision tree，就是將這個平面一一分割，每一個框框都是一個分類結果。但我們可以用右邊那樣一層一層的樹枝結構狀來表示右邊難懂的圖。（這裡的例子是二維平面，當變數 \(x_i\) 增加時，這樣的概念可以推廣到多維空間）</p>

<p>每一個分支都像是一個 if-else 問題，如果是就選某一邊，不是就選另外一邊。</p>

<p>上圖左，我們可以看到有兩個變數，\(x_1\), \(x_2\)，第一關是 \(x_1 &gt; \theta_1 \)，如果小於就往左邊分，如果大於就往右邊分，以此類推往下繼續細分，最後給予分類結果。最後共分成 A, B, C, D, E 五個類別，也就是右邊的五個框框。</p>

<p>Decision Tree 又可以分為</p>
<ol>
  <li>
    <p>Regression trees - 最後分類結果為連續變數</p>
  </li>
  <li>
    <p>Classification trees - 最後分類結果為類別變數</p>
  </li>
</ol>

<hr />

<p><strong>名詞</strong></p>

<ul>
  <li>Root node (The Root) - 第一個起始的點</li>
  <li>(Internal) Nodes - 中間的節點。上方會有箭頭指向 node，且 node 也會往下指向其他點。</li>
  <li>Leaves (terminal nodes) - 最後的節點，也就是最後的分類結果。</li>
</ul>

<hr />

<p><strong>如何分類？</strong></p>

<p>假如今天我有一份 data，decision tree 是如何決定要先使用哪一個變數與什麼值作為分割呢？</p>

<p><em>分割的原則是，這樣的分割要能得到最大的資訊增益 (Information gain, IG)</em> (<a href="https://medium.com/@yehjames/資料分析-機器學習-第3-5講-決策樹-decision-tree-以及隨機森林-random-forest-介紹-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林</a>)</p>

<p>資訊量根據最後的分類結果可以使用，</p>
<ol>
  <li>Regression trees:
    <ul>
      <li>MSE (mean-squared error)</li>
    </ul>
  </li>
  <li>Classification trees:
    <ul>
      <li>Entropy (Deviance)</li>
      <li>Gini impurity</li>
      <li>Missclassification error</li>
    </ul>
  </li>
</ol>

<p>在 Classification tree 裡，Entropy 和 Gini impurity 是常用的兩種方式，
詳細的公式請參考 <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Wikipedia</a></p>

<p>假如今天碰到兩種 model 算出來的資訊量都ㄧ樣，請選擇比較簡單的那個 model。（分支、leaves node 較少）</p>

<hr />

<p><strong>R 範例</strong>
這裡使用 tree 這個 library 做範例，另外還有像是 rpart 也是做 decision 常用的 package。</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="p">(</span><span class="s2">"iris"</span><span class="p">)</span><span class="w">

</span><span class="c1"># 使用 Entropy (Deviance)</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">Species</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sepal.Length</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Petal.Length</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="w"> </span><span class="n">split</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"deviance"</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="o">=</span><span class="m">0</span><span class="p">)</span><span class="w">

</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">


</span><span class="c1"># 使用 Gini impurity</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">Species</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sepal.Length</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Petal.Length</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="w"> </span><span class="n">split</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"gini"</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="o">=</span><span class="m">0</span><span class="p">)</span><span class="w">

</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<hr />

<p><strong>優缺點</strong></p>

<ul>
  <li>優點：
    <ol>
      <li>Simple to understand and interpret. 容易理解與解釋</li>
      <li>Able to handle both numerical and categorical data. 可以用在類別與 numerical 資料</li>
    </ol>
  </li>
  <li>缺點
    <ol>
      <li>Low bias and high variance with respect to the training data.
（當 decision tree model 變得太過複雜時，太多 nodes，就會導致 overfitting (<a href="https://en.wikipedia.org/wiki/Bias–variance_tradeoff">bias-variance trade off</a> 的狀況）</li>
    </ol>
  </li>
</ul>

<p>所以為了讓 model 不要 overfitting，我們可以使用 pruning 的方式，砍掉底下的樹枝。</p>

<p>在 R 可以使用，<code class="highlighter-rouge">prune.tree()</code>，並且可以使用 <code class="highlighter-rouge">best</code> 這個參數來決定最後要保留多少 leaves。</p>

<p>使用剛剛最後的 <code class="highlighter-rouge">fit</code>示範</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prune_fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prune.tree</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">prune_fit</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">prune_fit</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="o">=</span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">prune_fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>這時候 leaves 由原本的 9，修剪到了剩下 5，但 Misclassification error rate 卻是相同的。修剪過後的 model 較簡單，且 variance 也會比較小，在預測上表現也會較佳。</p>

<hr />

<p>Reference:
<br />
<a href="https://medium.com/@yehjames/資料分析-機器學習-第3-5講-決策樹-decision-tree-以及隨機森林-random-forest-介紹-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林</a>
<br />
<a href="https://www.youtube.com/watch?v=7VeUPuFGJHk">StatQuest: Decision Trees</a>
<br />
The Elements of Statistical Learning
<br />
Pattern Recognition and Machine Learning</p>
 -->
    <div class="post-excerpt"><h2 id="decision-tree">Decision Tree</h2>
<p><img src="http://localhost:4000/blog/img/posts/decision_tree.png" alt="decision_tree.png" height="190px" width="500px" /></p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2018/12/03/Machine-Learning-Decision-Tree-%E5%92%8C-bagging/#disqus_thread';
        this.page.identifier = '/machine%20learning/2018/12/03/Machine-Learning-Decision-Tree-%E5%92%8C-bagging/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2018/12/03/Machine-Learning-Decision-Tree-%E5%92%8C-bagging/#disqus_thread" data-disqus-identifier="/machine%20learning/2018/12/03/Machine-Learning-Decision-Tree-%E5%92%8C-bagging/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/r/2018/11/10/R-%E5%A6%82%E4%BD%95%E5%9C%A8-ggplot2-%E5%9C%96%E5%BD%A2%E4%B8%8A%E5%B8%8C%E8%87%98%E5%AD%97%E6%AF%8D/">
        [R]如何在 ggplot2 圖形上希臘字母？
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 10, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 10, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#R">R</a>
        
      
    </span>


    </div>
    <!-- <p><strong>1. 圖形文字</strong>
<br />
Text Labels: Use parse = T inside geom_text or annotate.</p>

<p><strong>2. 座標軸標籤</strong>
<br />
Axis Labels: Use expression(alpha) to get greek alpha.</p>

<p><strong>3. Legend</strong>
<br />
Legend Labels: Use bquote(alpha == .(value)) in legend label.</p>

<p><strong>4. Fecet圖形</strong>
<br />
Facet 圖形比較特殊，要將 <code class="highlighter-rouge">labeller = label_parsed</code>
<br />
Facet Labels: Use labeller = label_parsed inside facet.</p>

<p><a href="https://stackoverflow.com/questions/5293715/how-to-use-greek-symbols-in-ggplot2">來源</a></p>
 -->
    <div class="post-excerpt"><p><strong>1. 圖形文字</strong>
<br />
Text Labels: Use parse = T inside geom_text or annotate.</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//r/2018/11/10/R-%E5%A6%82%E4%BD%95%E5%9C%A8-ggplot2-%E5%9C%96%E5%BD%A2%E4%B8%8A%E5%B8%8C%E8%87%98%E5%AD%97%E6%AF%8D/#disqus_thread';
        this.page.identifier = '/r/2018/11/10/R-%E5%A6%82%E4%BD%95%E5%9C%A8-ggplot2-%E5%9C%96%E5%BD%A2%E4%B8%8A%E5%B8%8C%E8%87%98%E5%AD%97%E6%AF%8D/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//r/2018/11/10/R-%E5%A6%82%E4%BD%95%E5%9C%A8-ggplot2-%E5%9C%96%E5%BD%A2%E4%B8%8A%E5%B8%8C%E8%87%98%E5%AD%97%E6%AF%8D/#disqus_thread" data-disqus-identifier="/r/2018/11/10/R-%E5%A6%82%E4%BD%95%E5%9C%A8-ggplot2-%E5%9C%96%E5%BD%A2%E4%B8%8A%E5%B8%8C%E8%87%98%E5%AD%97%E6%AF%8D/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/r/2018/10/20/R-Environment/">
        [R]Environments (1)
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on October 20, 2018</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">October 20, 2018</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#R">R</a>
        
      
    </span>


    </div>
    <!-- <p>這篇要介紹 R 的環境(Environments)。</p>

<p>我個人覺得環境是一個寫程式很重要但一開始會很困惑的東西，之前一直處於似懂非懂的狀態。
<br />
其實如果只是用 R 來跑一些數據分析，其實不理解環境並不會造成什麼太大的問題，但如果能夠懂當然絕對是有利無弊。
<br />
最近有了寫 R package 的經驗後，讓我開始對「環境」比較了解，所以這篇想要將我的理解記錄下來。</p>

<hr />

<h3 id="1-什麼是環境environments">1. 什麼是環境（Environments）？</h3>

<p>當我們每次打開 R studio 這時候便是打開了某個環境，接著我們產生了一些變數，這些變數便是在這個環境底下。</p>

<p>很抽象嗎？
<br />
打開 R studio 時，在右上的框框（或是在某個位置）有個「Environment」，然後可以看到如下圖的「Gloabal Environment」，並且在下面的框框可以看到我們產生的變數（圖中的 x）。
<img src="http://localhost:4000/blog/img/posts/enrionment%20variable.png" alt="enrionment variable.png" height="395px" width="380px" /></p>

<p>這就表示，我們現在在「Gloabal Environment」這個環境底下，且 x 這個變數在「Gloabal Environment」環境中。</p>

<p>當我們把「Gloabal Environment」這個圖往下拉，可以看到像下圖這樣（每個人的可能都不太一樣）
<img src="http://localhost:4000/blog/img/posts/enrionment.png" alt="enrionment.png" height="395px" width="380px" />
依序往下就會是「Gloabal Environment」的 「parent 環境」。</p>

<p>（「parent 環境」是殺毀？！）</p>

<p><strong>環境就像是房間</strong></p>

<p>我覺得可以把「環境」想像是一個房間。
<br />
當妳/你在操作 R ，產生任何變數，做任何動作，都是在這個房間裡面操作（通常一打開 R studio 都會是在 Global 環境）。所以這些變數全部都會被放在這個房間裡。或是想像在一個房間裡縫娃娃這些娃娃做完都會被放在這個房間裡。</p>

<p>那「parent 環境」就會是，妳/你走出這個房間來到客廳，也就是說，這個房間是被包在家裡的，走出家裡，整個家是被包在建築物裡的，然後社區，然後某條路，某個區，某個縣市……就是一個俄羅斯娃娃的感覺。往上一層就會是「parent 環境」、「grand parent 環境」…..以此類推。</p>

<hr />

<h3 id="2-r-是如何在環境裡找變數的">2. R 是如何在環境裡找變數的？</h3>

<p>根據上面的解釋我們知道，每次操作 R 的時候我們都會是在某個環境底下。
<br />
那這和 R 要找變數有什麼關係呢？</p>

<p>在這之前我們先來看我現在有幾間房間呢？</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">search</span><span class="p">()</span><span class="w">
</span><span class="c1"># [1] ".GlobalEnv"        "tools:rstudio"     "package:stats"     "package:graphics"  "package:grDevices"</span><span class="w">
</span><span class="c1"># [6] "package:utils"     "package:datasets"  "package:methods"   "Autoloads"         "package:base"</span><span class="w">
</span></code></pre></div></div>
<p>這表示，最底下的環境是<code class="highlighter-rouge">.GlobalEnv</code>(Gloabal Environment)，它的 parent 變數是<code class="highlighter-rouge">tools:rstudio</code>，再往上是<code class="highlighter-rouge">package:stats</code>……等等，最後會是<code class="highlighter-rouge">package:base</code>，但其實最上層會是 Empty environment。</p>

<p><strong>在房間裡找東西？</strong></p>

<p>如果今天妳/你人在一個房間裡找東西妳/你會怎麼找？
<br />
一定會先在房間裡搜尋一遍，如果怎麼都找不到，就會去客廳找，再找不到就到這棟建築物找……一直往外找對吧？</p>

<p>所以同樣的，如果今天我們想要找個變數 x，結果 R 發現找不到，那它就會往 parent 環境（search()的順序）找，再找不到就會再往上找，如果一直找到最上層還是沒有，就會跳出像這樣的錯誤。</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="w">
</span><span class="c1"># Error: object 'x' not found</span><span class="w">
</span></code></pre></div></div>
<p>找的方式就像是這個圖，<a href="http://adv-r.had.co.nz/Environments.html">source</a>
<img src="http://localhost:4000/blog/img/posts/search%20path.png" alt="search path.png" height="160px" width="500px" /></p>

<p><strong>parent 是誰？</strong></p>

<p>我們可以用<code class="highlighter-rouge">parent.env()</code>來查看上一層環境，</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">now.env</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">environment</span><span class="p">()</span><span class="w">
</span><span class="n">parent.env</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">parent.env</span><span class="p">(</span><span class="n">now.env</span><span class="p">)</span><span class="w">
</span><span class="n">grandparent.env</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">parent.env</span><span class="p">(</span><span class="n">parent.env</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<hr />

<h3 id="3-可以增加環境嗎">3. 可以增加環境嗎？</h3>
<p>如果仔細看剛剛<code class="highlighter-rouge">search()</code>的結果會發現，那些都是 package 呢！
<br />
其實在每次使用 <code class="highlighter-rouge">library()</code>或是<code class="highlighter-rouge">require()</code> attache 一個 pacakge 後環境就會被改變，這時 Gloabal Environment 的 parent 環境就會變成這個 package。（請自行實驗）</p>

<hr />

<p>（題外話）
<br />
<strong>library() v.s require()</strong>
這兩者到底有什麼差別呢？</p>

<p>基本上兩個的差別只有在於找不到這個 package 時，<code class="highlighter-rouge">library()</code> 會產生 Error 暫停程式，<code class="highlighter-rouge">require()</code>會 FALSE 並吐出 Warning ，然後繼續執行程式。
<br />
所以可以把<code class="highlighter-rouge">require()</code>包在 if 裡，如果沒有這個 package 就安裝，程式也能繼續執行。</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="c1"># Error in library(x) : there is no package called ‘x’</span><span class="w">

</span><span class="n">require</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="c1"># Loading required package: x</span><span class="w">
</span><span class="c1"># Warning message:</span><span class="w">
</span><span class="c1"># In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :</span><span class="w">
</span><span class="c1">#  there is no package called ‘x’</span><span class="w">
</span></code></pre></div></div>
<p>（題外話結束）</p>

<hr />

<p>也就是說，如果我們今天使用<code class="highlighter-rouge">library()</code>或是<code class="highlighter-rouge">require()</code>就會讓環境改變。
<br />
所以如果今天是要 create package 的話，千萬不要將 <code class="highlighter-rouge">library()</code>和<code class="highlighter-rouge">require()</code>包在任何 function，這樣非常危險，可能會造成 <a href="http://r-pkgs.had.co.nz/namespace.html">Namespace</a> 的混亂。（如果看不懂可以先忽略這段）</p>

<p>那如果今天需要用到這個包，但是又不想讓它加到 search path（parent 環境）裡怎麼辦？</p>

<p>這時候請使用<code class="highlighter-rouge">::</code>。例如，今天想要呼叫 pkg1 裡頭的 fun()，</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pkg1</span><span class="o">::</span><span class="n">fun</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>
<p><code class="highlighter-rouge">::</code>還有一個很好用的時刻，就是當今天有兩個不同的 package 擁有兩個同樣名稱的 function 時，先載入的 package 順序會被放在後面，所以如果直接呼叫 funciton ，一定會用到後載入的 package 的 function。</p>

<p>如果現在是 pkg1 和 pkg2 都有 fun()，那我們可以這樣用，</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 呼叫 pkg1 的 fun</span><span class="w">
</span><span class="n">pkg1</span><span class="o">::</span><span class="n">fun</span><span class="p">()</span><span class="w">

</span><span class="c1"># 呼叫 pkg2 的 fun</span><span class="w">
</span><span class="n">pkg2</span><span class="o">::</span><span class="n">fun</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>
<p>這樣就絕對不會出錯了。
不過記得，要使用<code class="highlighter-rouge">::</code>必須要有 install 這個 package 才能使用。</p>

<p><strong>Attaching v.s Loading</strong></p>

<p>library() 和 require() 這兩個 function 的動作都是 <strong>Attaching</strong>，也就是會將這 package 加到 parent 環境 (search path)。
<br />
但 <code class="highlighter-rouge">::</code>則是 <strong>Loading</strong>，也就是說，這個 pacakge 在 loading 後可以在記憶體（memory）中被找到，但不會被加到 parent 環境 (search path) 中。
<a href="http://r-pkgs.had.co.nz/namespace.html#search-path">source</a></p>

<hr />

<p>下次介紹
<br /></p>
<ol>
  <li><code class="highlighter-rouge">new.env()</code>和 function 中的環境</li>
  <li>&lt;- 和 «- 的差別</li>
</ol>

<hr />

<p>Reference:
<br />
<a href="http://adv-r.had.co.nz/Environments.html">Advance R - Environments</a>
<br />
<a href="http://r-pkgs.had.co.nz/namespace.html">R package - Namespace</a>
<br />
<a href="https://www.gl-li.com/2017/09/14/build-a-r-package-for-yourself/">Build a R package for yourself</a>
<br />
<a href="http://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html">Making Your First R Package</a></p>
 -->
    <div class="post-excerpt"><p>這篇要介紹 R 的環境(Environments)。</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//r/2018/10/20/R-Environment/#disqus_thread';
        this.page.identifier = '/r/2018/10/20/R-Environment/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//r/2018/10/20/R-Environment/#disqus_thread" data-disqus-identifier="/r/2018/10/20/R-Environment/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page4">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page2">Newer</a>
    
  
</div>
    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
