<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- 數學符號 -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2022. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts" id="begin">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/neural%20network/2020/02/01/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/">
        [Neural Network]Convolution Neural Network 卷積神經網路
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on February 1, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">February 1, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        &nbsp;
      
        <a target="_blank" href="/blog/category/#Neural Network">Neural Network</a>
        
      
    </span>

    </div>
    <!-- <p>之前寫了兩篇的 CNN 課程筆記，但後來因為太忙沒有繼續看完課程就中斷了 T_T。
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">[Machine Learning]Covolutional Neural Networks(CNN)(1)</a>
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">[Machine Learning]Covolutional Neural Networks(CNN)(2)</a></p>

<p>卷積神經網絡(Convolutional Neural Network)簡稱 CNN，主要應用在影像辨識，這篇想要簡單的說明 CNN 的概念。</p>

<hr />

<p>先看下面兩張 CNN 概念圖</p>

<p><a href="https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/3">圖片來源</a>
<img src="http://localhost:4000/blog/img/posts/CNN%20%E6%A6%82%E5%BF%B5%E5%9C%96%201.png" alt="" /></p>

<p><a href="https://medium.com/jameslearningnote/資料分析-機器學習-第5-1講-卷積神經網絡介紹-convolutional-neural-network-4f8249d65d4f">圖片來源</a>
<img src="http://localhost:4000/blog/img/posts/CNN%20%E6%A6%82%E5%BF%B5%E5%9C%96%202.png" alt="" /></p>

<p>從上圖可以看到整個流程各經過兩次的 Convolution、Pooling 和 Fully Connected，所以其實只要搞懂 <strong>Convolution</strong>、<strong>Pooling</strong> 和 <strong>Fully Connected</strong> 分別在做什麼就可以掌握 CNN 了。</p>

<p>整個流程是，前面的 Convolution 和 Pooling (又稱為 subsampling) 在做的是 feature extraction (特徵擷取)，而 Fully connection 在做的是 classification (分類辨識)。</p>

<hr />

<h2 id="1-convolution-layer-卷積層">1. Convolution Layer 卷積層</h2>

<p>Convolution Layer 由 filter 和激活函數 (ex. Relu) 所構成。</p>

<p>下圖是常見的美圖效果，其實就是使用 filter 達成的
<a href="https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-convolution-neural-network-卷積神經網路-bfa8566744e9">圖片來源</a>
<img src="http://localhost:4000/blog/img/posts/%E7%BE%8E%E5%9C%96.png" alt="" /></p>

<p>卷積 (Convolution) 做的是特徵擷取，而不同的效果在做的就是對不同的特徵做擷取。例如，銳化效果在做的就是強化邊緣的特徵，也就是加強擷取出邊緣的特徵。</p>

<p>那所以卷積到底是如何擷取不同的特徵的呢？
<a href="https://icecreamlabs.com/2018/08/19/3x3-convolution-filters%E2%80%8A-%E2%80%8Aa-popular-choice/">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/Convolution.gif" alt="" /></p>

<p>特徵擷取的方式是使用 Filter (或稱作 Kernel) 來萃取圖片中的特徵，上圖是一個 3x3 window 的 filter，而算法就是像圖那樣移動 (移動的步數是 stride)，將數字相乘後再相加 (詳情可以看之前的<a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">筆記1</a>、<a href="https://shihs.github.io/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">筆記2</a>，包括 padding 和 stride 的概念)。美圖的不同效果就是使用不同的 Filter (Kernel) 做出來的。</p>

<p>下圖是不同的 filter 做出來的效果
<a href="https://icecreamlabs.com/2018/08/19/3x3-convolution-filters%E2%80%8A-%E2%80%8Aa-popular-choice/">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/kernels.png" alt="" /></p>

<p>現在明白了 filter，但再回去看最一開始的流程圖，為什麼 converlution 有三個呢？這是因為使用了三個 filter 去擷取不同的特徵。</p>

<p>看完 filter 再看 Relu。可以看到最上圖圖中有個 Relu，也就是將所有計算出來結果的負值都變為 0。
<a href="https://medium.com/jameslearningnote/資料分析-機器學習-第5-1講-卷積神經網絡介紹-convolutional-neural-network-4f8249d65d4f">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/Relu.png" alt="" /></p>

<p>如果是彩色的圖，通常會有三個 channel (RGB)，也就是一開始輸入的 image 有三個，可以看下圖
<a href="https://zhuanlan.zhihu.com/p/42559190">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/three%20images%20convolution.jpg" alt="" /></p>

<h2 id="2-pooling-layer-池化層">2. Pooling Layer 池化層</h2>

<p>pooling 又稱為 subsampling，是為了提取特定區域主要特徵，但又想要減少參數的數量，以防止模型過擬合，常用的是 max pooling，也就是取該區域最大的值，也有 average pooling，也就是取該區域的平均值。
<a href="https://zhuanlan.zhihu.com/p/42559190">圖片來源</a></p>

<p>下圖是取 max pooling，使用 2x2 的 window，且 stride 為 2</p>

<p><img src="http://localhost:4000/blog/img/posts/pooling.jpg" alt="" /></p>

<h2 id="3-fully-connected-layer-全連接層">3. Fully Connected Layer 全連接層</h2>

<p>Fully Connected Layer 主要是進行 Flattening 平坦化，將最後的矩陣轉換成一維，然後再使用傳統的 neural network + softmax()。</p>

<p>參考我之前的<a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">[Neural Network]Neural Network 1</a></p>

<hr />

<p>以上就是 CNN 的流程。</p>

<p>最後提一下，從上面的過程我們可以知道，CNN 整個過程中需要求解的參數是</p>

<ol>
  <li>Convolution layer filters</li>
  <li>Full Connected Layer filters</li>
</ol>

<p><a href="http://doremi2016.logdown.com/posts/2017/01/25/convolutional-neural-networks-cnn">來源</a></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://medium.com/jameslearningnote/資料分析-機器學習-第5-1講-卷積神經網絡介紹-convolutional-neural-network-4f8249d65d4f">[資料分析&amp;機器學習] 第5.1講: 卷積神經網絡介紹(Convolutional Neural Network)
</a>
<br />
<a href="https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-convolution-neural-network-卷積神經網路-bfa8566744e9">[機器學習 ML NOTE]Convolution Neural Network 卷積神經網路</a>
<br />
<a href="https://zhuanlan.zhihu.com/p/42559190">【DL笔记6】从此明白了卷积神经网络（CNN）</a>
<br />
<a href="http://doremi2016.logdown.com/posts/2017/01/25/convolutional-neural-networks-cnn">Convolutional Neural Networks’ (CNN) Backward Propagation</a></p>
 -->
    
    <div class="post-excerpt"><p>之前寫了兩篇的 CNN 課程筆記，但後來因為太忙沒有繼續看完課程就中斷了 T_T。
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">[Machine Learning]Covolutional Neural Networks(CNN)(1)</a>
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">[Machine Learning]Covolutional Neural Networks(CNN)(2)</a></p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/neural%20network/2020/02/01/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/#disqus_thread';
        this.page.identifier = '/machine%20learning/neural%20network/2020/02/01/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/neural%20network/2020/02/01/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/#disqus_thread" data-disqus-identifier="/machine%20learning/neural%20network/2020/02/01/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/neural%20network/2020/01/23/Neural-Network-Neural-Network-2/">
        [Neural Network]Neural Network 2 - Gradient descent
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on January 23, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">January 23, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Neural Network">Neural Network</a>
        
      
    </span>

    </div>
    <!-- <p><a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">上一篇</a>說明了神經網路的基本原理，這篇我們要講解神經網路是如何學習的 - Gradient descent。</p>

<p>這篇是<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">Neural networks</a>的課程筆記。</p>

<hr />

<p>在 Machine Learning 裡我們使用 training data 訓練得到一個 model，但這「訓練」到底是怎麼訓練的呢？訓練的過程，通常是要找到最小的 cost，也就是最後的結果與真實值差距越小越好。</p>

<p>在 Neural Network 我們要訓練的變數就是<a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">前面一篇</a>提到的 weight 和 bias。一開始我們隨機的給予這些數字，然後在一次次的學習中去改進 weight 和 bias 的值，找到最小的成本函數。</p>

<p>在課程的例子裡，Neural Network function 的 input 是 784 個 pixel 的數字，output 是 10 個數字，0 到 9 的機率，parameters 是 weight 和 bias。但在cost function 中，weight 和 bias 變成了 input，而 output 就是 cost 的值，parameters 則是很多的 training pairs。</p>

<p>我們可以把 cost function 這樣表示，</p>

\[C(w_1, w_2,..., b_1, b_2, ....) = cost\]

<p>cost 的算法是 Neural Network 算出來的值，扣掉正確值然後平方相加。學習過程中，我們將所有 training data 的 cost 相加平均（average cost of all training data），希望這個值越小越好。</p>

<p><img src="http://localhost:4000/blog/img/posts/cost.png" alt="" /></p>

<p>那我們要如何找到最小的 cost 值？</p>

<p>如果 cost function 是一個像下圖的函數圖形，我們知道微分等於 0 可以找到最小值，但這並非一件容易的事，尤其是當我們有很多變數時。</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima.png" alt="" /></p>

<p>所以我們可以用的方法是，先隨機到函數圖形上的一點，然後根據該點的切線斜率來決定該往哪個方向移動可以找到最低點。這樣的方法，可以讓我們到 local minimum，我們無法確認找到的最小是否是 global minimum，因為這取決與一開始的 initial value。</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima%202.png" alt="" /></p>

<hr />

<p>在進入 Gradient descent 之前，我想要先介紹一下基礎觀念。以下將會講解，</p>

<ol>
  <li>導函數 Derivative function 與導數 Derivative</li>
  <li>方向導數 Directional derivative</li>
  <li>偏導數 Partial derivative</li>
  <li>梯度 Gradient</li>
</ol>

<h3 id="導數-derivative">導數 Derivative</h3>

<p>切線的斜率稱為「導數」（Derivative），也就是微分。</p>

<p>定義成，</p>

\[m = \text{lim}_{\Delta x \to 0}\frac{f(x+\Delta x) - f(x) }{\Delta x} = f'(x) = \frac{\text{d} f(x)}{\text{d}x}\]

<h3 id="偏導數-partial-derivative">偏導數 Partial derivative</h3>

<p>斜率求的是單變數的微分，而偏導數是在多變數中的某個變數方向的斜率。</p>

<p>以兩個自變數為例， \(z = f(x, y)\)</p>

<p>\(f_x(a, b)\) 為對 \(x\) 的偏導數，定義為</p>

\[f_x(a, b) = \text{lim}_{h \to 0} \frac{f(a + h, b) - f(a, b)}{h}\]

<p>\(f_y(a, b)\) 為對 \(y\) 的偏導數，定義為</p>

\[f_y(a, b) = \text{lim}_{k \to 0} \frac{f(a, b + k) - f(a, b)}{k}\]

<p>也就是對 x 或 y 方向做偏微分。</p>

<h3 id="方向導數-directional-derivative">方向導數 Directional derivative</h3>

<p>斜率求的是單變數的微分，在多變數中要求斜率必須要看是哪個方向的斜率，因為各個方向的斜率都不相同，這時要求的就是方向導數。</p>

<p>假設函數 \(z = f(x, y)\)，在定義域 \(xy\) 平面上有一點 \((x, y)\) 及單位向量 \(\overset{\rightharpoonup}{u} = (u_1, u_2)\)。則曲面 \(z = f(x, y)\) 在 \((a, b)\) 處，沿 \(\overset{\rightharpoonup}{u}\) 的方向的斜率定義為，</p>

\[\text{D}_u f(x_0, y_0)= \text{lim}_{h \to 0} \frac{f(x_0 + h u_1, y_0 + h u_2) - f(x_0, y_0)}{h} = \nabla f \boldsymbol{u}\]

<p>方向導數的意義在於，有助於計算出空間中某一場量 \(f(x, y, z)\) 在某一特定方向 \(\boldsymbol{u}\) 之變化率。</p>

<h3 id="梯度-gradient">梯度 Gradient</h3>

<p>梯度的定義就是對各個分量做偏微分。</p>

<p>例如，現在是兩變數函數 \(f(x, y)\)，其梯度 \(\nabla f(x, y)\) 定義為</p>

\[\nabla f(x, y) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)\]

<p>若是變數有三個，則 \(\nabla f(x, y, y)\) 定義為</p>

\[\nabla f(x, y, z) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)\]

<p>Gernal 來看，有 n 個變數的函數 \(f(x_1, x_2, \cdots, x_n)\)，我們可以寫成</p>

\[\nabla f(x_1, x_2, \cdots, x_n) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}\right)\]

<p>梯度 Gradient 的幾何意義，</p>
<ul>
  <li>\(\nabla f\) 表示曲面 \(f(x, y, z) = C\) 之垂直向量，如下圖所示。</li>
  <li>\(\nabla f(P)\) 表在 \(P\) 點增量最大的方向，也就是 the direction of steepest increase</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/gradient.png" alt="" /></p>

<hr />

<h3 id="梯度下降法-gradient-descent">梯度下降法 Gradient descent</h3>

<p>Gradient descent 是一種最佳化方法。</p>

<p>從上一部分，我們知道 P 點 gradeint 表示該點增量最大的方向。現在我們要找 cost function 的最小值，也就是該 function 斜率為 0 的點，當我們今天在 P 點知道哪個方向是會增加最快速的點，那就只要往反方向就會是減少最快速的方向了。也就是 \(-\nabla f(P)\) 是找尋 minimum cost 的方向。</p>

<p>總的來說，梯度下降法 Gradient descent 就是不斷的在某個點 P 算它的 Gradient，然後往 Gradient 的反方向一定，直到找到 local minimum 為止（無法知道是否找到 glocal minimum，會根據 initial values 而定）。</p>

<p>在 neural netword 中，讓這個梯度計算更有效率的方法叫做 <strong>Backpropagation</strong>，而這正是 neural netword 的重點！下一篇繼續介紹 :)</p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/827/mod_resource/content/33/Summary_227.pdf">方向導數(Directional Derivative)之定義與意義</a>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/826/mod_resource/content/31/Summary_226.pdf">純量函數之梯度(Gradient)</a></p>

 -->
    
    <div class="post-excerpt"><p><a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">上一篇</a>說明了神經網路的基本原理，這篇我們要講解神經網路是如何學習的 - Gradient descent。</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//neural%20network/2020/01/23/Neural-Network-Neural-Network-2/#disqus_thread';
        this.page.identifier = '/neural%20network/2020/01/23/Neural-Network-Neural-Network-2/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//neural%20network/2020/01/23/Neural-Network-Neural-Network-2/#disqus_thread" data-disqus-identifier="/neural%20network/2020/01/23/Neural-Network-Neural-Network-2/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">
        [Neural Network]Neural Network 1 - The idea
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on January 22, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">January 22, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Neural Network">Neural Network</a>
        
      
    </span>

    </div>
    <!-- <p>在了解 CNN (卷積神經網路, Convolutional neural network) 與 RNN (循環神經網絡, Recurrent neural network) 前，先來認識最簡單的 NN (神經網路, Neural network) Multilayer perceptron。</p>

<p>這篇是<a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">Neural networks</a>的課程筆記。</p>

<hr />

<p>Neural network 啟發自大腦神經，所謂的 Neural 就是 neurons (神經元)，而在 Neural network 裡，neurons 是個介於 0 和 1 之間的數字。</p>

<p>我們將以讓程式辨認手寫數字 0 到 9 為例解說 Neural network。下圖是一個 28*28 像素的手寫數字，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%201.png" alt="" /></p>

<p>每一個像素都是一個 0 到 1 的灰階值，而 Neural network 以這個圖的每個像素，對應到每個神經元作為輸入值，也就是說第一層輸入層共有 28*28 = 784 個神經元。灰階值 0 為黑色，1 為白色。這些在神經元中的數字稱為「激勵值」（activation）。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%202.png" alt="" /></p>

<p>這裡範例使用，第一層為輸入層，中間兩層 hidden layers，最後一層為輸出層。而最後輸出層就是 0 到 9 的數字，每個神經元都有個「激勵值」（activation），而這數字就代表著最後給定可能是這個數字的判定結果。</p>

<p>而中間的 hidden layers 正是神經網路的重點。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%203.png" alt="" /></p>

<p>我們可以想像，將整個數字分解成不同的部分，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%204.png" alt="" /></p>

<p>第一層 hidden layer 是比較小段的部分，而第二層 hidden layer 是由這些小區塊組成比較大塊的區塊。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%205.png" alt="" /></p>

<p>有了這些概念之後，那我們要怎麼從輸入層到第一層的 hidden layer 呢？</p>

<p>現在我們希望，第二層的其中一個神經元可以辨識這個圖有沒有一個邊，如下圖，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%206.png" alt="" /></p>

<p>我們會給這個神經元與輸入層的神經元之間每個連線一個權重（weight），然後將輸入層的神經元乘上每個權重再加總，權重可以是正或負值。</p>

<p>但因為，我們希望神經元的激勵值（activation）是在 0 和 1 之間，所以使用 sigmoid function \(\sigma(x) = \frac{1}{1+e^x}\) 。經過 <a href="https://zh.wikipedia.org/wiki/S函数">sigmoid function</a> 計算，越小的值會越接近 0，越大的值會越接近 1。</p>

<p>但也許並不是每次加總大於零我們就希望給正的灰階值，因此，權重最後再加上個 bias，來調整我們想要給灰階值的值。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%207.png" alt="" /></p>

<p>所有第一層 hidden layers 的神經元都根據這樣的概念，有自己關注的區塊，有自己的權重，再加上自己的 bias。第二層的 hidden layers 神經元也以同樣的概念計算，所以我們總共會有 13,002 個權重加上 bias。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%208.png" alt="" /></p>

<p>而所謂的 deep learning 的 learning 就是在學習這些權重和 bias，讓電腦去找到正確的值。</p>

<p>上上圖的式子只是其中一個神經元，而我們可以用矩陣表達所有第一層 hidden layers 的神經元。如下圖，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%209.png" alt="" /></p>

<p>總的來說，我們可以將整個 neural network 看成一個 function。</p>

\[f(a_0, a_1,..., a_{783}) =  \begin{bmatrix}
   y_0 \\
   \vdots \\
   y_9
  \end{bmatrix}\]

<p>最後，其實現在已經不使用 sigmoid function 了，現在主流的 function 用的是 <strong>ReLU</strong>。</p>

\[ReLU(a) = max(0, a)\]

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%2010.png" alt="" /></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.youtube.com/watch?v=aircAruvnKk">究竟神經網路是什麼？ 第一章 深度學習</a></p>
 -->
    
    <div class="post-excerpt"><p>在了解 CNN (卷積神經網路, Convolutional neural network) 與 RNN (循環神經網絡, Recurrent neural network) 前，先來認識最簡單的 NN (神經網路, Neural network) Multilayer perceptron。</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//neural%20network/2020/01/22/Neural-Network-Neural-Network/#disqus_thread';
        this.page.identifier = '/neural%20network/2020/01/22/Neural-Network-Neural-Network/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//neural%20network/2020/01/22/Neural-Network-Neural-Network/#disqus_thread" data-disqus-identifier="/neural%20network/2020/01/22/Neural-Network-Neural-Network/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/database/2020/01/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96/">
        [Database]ER Model 實體關聯模型
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on January 1, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">January 1, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Database">Database</a>
        
      
    </span>

    </div>
    <!-- <h2 id="實體關聯模型entity-relationship-model">實體關聯模型（Entity-Relationship Model）</h2>

<p>是Conceptual model（概念塑模）的一環，用來描述<strong>實體</strong>與<strong>實體</strong>之間<strong>關係</strong>的工具。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/ER%20Model.png" alt="ER Model.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">ER Model</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>實體（Entity）：是指用以描述真實世界的物件，在關聯式資料庫中為一個「資料表」
    <ul>
      <li>實體至少擁有一個不是鍵（主鍵）的屬性</li>
      <li>一個實體可以含有多個「屬性」(Attribute)用以描述該實體，在關聯式資料庫中，以資料表的「欄位」來表示</li>
      <li>例如：學生、員工、產品等等都是屬於實體</li>
    </ul>
  </li>
  <li>關係（Relationship）：指用來表示「一個實體」與「另一個實體」關聯的方式
    <ul>
      <li>例如：一對一關係、一對多關係、多對多關係</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="實體entity">實體（Entity）</h2>

<p><strong>定義</strong></p>
<ul>
  <li>a ”thing” in the real world with an independent existence</li>
  <li>用來描述實際存在的事物（如:學生），也可以是邏輯抽象的概念（如:課程）</li>
  <li>必須可以被識別，亦即能夠清楚分辨出兩個不同的實體(an independent existence)</li>
  <li>以「名詞」的來命名，不可以是「形容詞」或「動詞」</li>
</ul>

<p><strong>分類</strong></p>
<ul>
  <li>實體(entity)</li>
  <li>弱實體(weak entity)：必須依靠其他實體才能存在，若其依靠的實體消失，則該實體的存在也沒有意義了
    <ul>
      <li>例如：學生家長就是依附在學生實體的弱實體，若學生不存在，則家長也沒有存在的意義</li>
    </ul>
  </li>
</ul>

<p><strong>圖示</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/entity%20diagram.png" alt="entity diagram.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://images.app.goo.gl/auQ9MAQCKttsdFGEA">Entity diagram</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="屬性attribute">屬性(Attribute)</h2>

<p><strong>定義</strong></p>
<ul>
  <li>用來描述實體的性質（Property）（就是關聯式表格的行）
    <ul>
      <li>例如：學生的學號、名字、班級等等</li>
    </ul>
  </li>
</ul>

<p><strong>分類</strong></p>
<ul>
  <li>多值屬性 vs 單值屬性
    <ul>
      <li>多值屬性（Multivalued attributes）：屬性值不只一個時，我們稱該屬性為<em>多值屬性</em>
        <ul>
          <li>例如：學生有多個專長，珠算、程式…</li>
          <li>關係圖中以「雙橢圓形」來表示</li>
        </ul>
      </li>
      <li>單值屬性（Single-valued attribute）：非多值屬性稱為<em>單值屬性</em></li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Multivalued%20attributes.png" alt="Multivalued attributes.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://spaces.isu.edu.tw/upload/19225/0/news/postfile_308.pdf">多值屬性 Multivalued attributes diagram</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>複合屬性 vs 簡單屬性
    <ul>
      <li>複合屬性（Composite attributes）：屬性由數個屬性所組成時，我們稱該屬性為<em>複合屬性</em>
        <ul>
          <li>例如：姓名由姓、名組成</li>
          <li>關係圖如下</li>
        </ul>
      </li>
      <li>簡單屬性（Simple attribute）：非複合屬性則稱為<em>簡單屬性</em></li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Composite%20attributes.png" alt="Composite attributes.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://spaces.isu.edu.tw/upload/19225/0/news/postfile_308.pdf">複合屬性 Composite attributes diagram</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>衍生屬性（Derived Attribute）
    <ul>
      <li>它的值可以由其它屬性之值經由某種方式的計算或推論而獲得</li>
      <li>例如：年齡和星座可由生日推算出來</li>
    </ul>
  </li>
</ul>

<h2 id="鍵屬性key-attribute">鍵屬性(Key attribute)</h2>

<p><strong>定義</strong></p>
<ul>
  <li>是指該屬性的值在某個環境下具有唯一性（primary key）</li>
</ul>

<p><strong>圖示</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/attribute.png" alt="attribute.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://images.app.goo.gl/auQ9MAQCKttsdFGEA">Attribute diagram</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="關係relationship">關係(Relationship)</h2>

<p><strong>定義</strong></p>
<ul>
  <li>是指用來表達兩個實體之間所隱含的關聯性</li>
  <li>使用足以說明關聯性質的「動詞」或「動詞片語」命名
    <ul>
      <li>例如：『學生』與『系所』兩個實體型態間存在著一種關係─「就讀於」</li>
    </ul>
  </li>
</ul>

<p><strong>分類</strong></p>
<ul>
  <li>關聯強度 Strength
    <ul>
      <li>強關聯（strong relationship）：a relationship where entity is existence-independent of other entities, and PK of Child doesn’t contain PK component of Parent Entity</li>
      <li>弱關聯（weak (identifying) relationship）：a relationship where Child entity is existence-dependent on parent, and PK of Child Entity contains PK component of Parent Entity，也就是說，此關聯是連結<em>實體</em>與<em>弱實體</em></li>
    </ul>
  </li>
  <li>Cardinality
    <ul>
      <li>1:1 一對一</li>
      <li>1:N 一對多</li>
      <li>M:1 多對一</li>
      <li>M:N 多對多</li>
    </ul>
  </li>
  <li>Participation constraints
    <ul>
      <li>全部參與（Total participation）：every entity in the set is involved in the relationship</li>
      <li>部分參與（Partial participation）：not all entities in the set are involved in the relationship</li>
    </ul>
  </li>
</ul>

<p><strong>圖示</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Relationship.png" alt="Relationship.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://images.app.goo.gl/auQ9MAQCKttsdFGEA">Relationship diagram</a></td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>弱實體例子</strong></p>

<p>為了區分和一般實體的關係, 我們會以雙菱形來表示實體與弱實體之間的關係, 而弱實體和關係之間以雙直線連接</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/books.png" alt="books.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.vertabelo.com/blog/chen-erd-notation/">Example</a></td>
    </tr>
  </tbody>
</table>

<p>以上圖為例，CHAPTER 必須依靠 BOOK 才能存在，所以 CHAPTER 是一個弱實體，兩者間用雙菱形（弱關聯）來連結。CHAPTER 用雙直線（Total participation）連接關係，因為 CHAPTER 是依靠 BOOK 而存在，表示每一個 CHAPTER 必定屬於某本 BOOK。而 CHAPTER 的 primary key 由 BOOK 的 primary key（BOOK ID） 與 CHAPTER ID 組成。</p>

<p>如果將上圖關係轉換成表格，CHAPTER 表格會有 {TITLE, CHAPTER ID, BOOK ID} 三個欄位，而 (CHAPTER ID, BOOK ID) 合併為 primary key。</p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="http://cc.cust.edu.tw/~ccchen/doc/db_03.pdf">ER Model 實體關係圖</a>
<br />
<a href="http://www.csie.sju.edu.tw/cm/course/db/ch04.pdf">第4章 資料庫設計與實體關聯模型</a>
<br />
<a href="http://spaces.isu.edu.tw/upload/19225/0/news/postfile_308.pdf">利用實體-關係模型 (E-R Model) 規劃資料庫</a>
<br />
<a href="https://www.vertabelo.com/blog/chen-erd-notation/">Chen notation</a>
<br />
<a href="http://snowlin.cmu.edu.tw/mis/MIS_Lec01.pdf">資料庫基礎概念</a></p>
 -->
    
    <div class="post-excerpt"><h2 id="實體關聯模型entity-relationship-model">實體關聯模型（Entity-Relationship Model）</h2>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//database/2020/01/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96/#disqus_thread';
        this.page.identifier = '/database/2020/01/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//database/2020/01/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96/#disqus_thread" data-disqus-identifier="/database/2020/01/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/database/2019/12/31/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/">
        [Database]Database 基礎概念
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 31, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 31, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Database">Database</a>
        
      
    </span>

    </div>
    <!-- <h2 id="資料data與資訊information">資料(Data)與資訊(Information)</h2>

<ul>
  <li>資料(Data)：資訊(Information)的原始型態，是未經整理和分析的原始數值、文字或符號。</li>
  <li>資訊(Information)：經過整理和分析後的資料，是有實質意義的資料。</li>
  <li>資料處理(Data Processing)：資料(Data) 經過資料處理(Data Processing) 後將資料轉換為資訊(Information)。</li>
</ul>

<h2 id="資料階層">資料階層</h2>

<p>資料階層的最小儲存單位是位元，8個位元組成一個位元組，也就是ASCII碼的字元。數個位元組結合成欄位，多個欄位組成記錄，最後將一組記錄儲存成檔案，資料庫就是 一組相關檔案的集合。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/%E8%B3%87%E6%96%99%E9%9A%8E%E5%B1%A4.png" alt="資料階層.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">資料階層</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>第一階層：位元(Bits)</li>
  <li>第二階層：位元組(Bytes)</li>
  <li>第三階層：欄位(Fields)</li>
  <li>第四階層：記錄(Records)</li>
  <li>第五階層：檔案(Files)</li>
  <li>第六階層：資料庫(Database)</li>
</ul>

<hr />

<h2 id="資料庫系統database-system">資料庫系統(Database System)</h2>

<p>資料庫系統(Database System)是由「資料庫」(Database)和「資料庫管理系統」(Database Management System，DBMS)所組成。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Database%20System.png" alt="Database System.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">資料庫系統</a></td>
    </tr>
  </tbody>
</table>

<h3 id="資料庫database">資料庫(Database)</h3>
<ol>
  <li>Represents some aspects of the real world (miniworld)</li>
  <li>資料庫是一個對結構化資訊或資料的組織性收集，通常以電子方式儲存在電腦系統。</li>
  <li>資料庫通常由資料庫管理系統(DBMS)控制。</li>
  <li>目前運行中最常見的資料庫型態通常是在一系列的表格中進行行列間建模，使得處理和資料查詢更為有效。</li>
  <li>大部分的資料庫使用結構化查詢語言(SQL)來書寫或查詢資料。</li>
  <li>資料庫容許多名用戶在同一時間快速、安全地以高度複雜邏輯和語言取得或查詢資料。</li>
  <li>Meta-data: Database definition or descriptive information (Stored by the DBMS in a database catalog or data dictionary)</li>
</ol>

<h3 id="資料庫管理系統dbms">資料庫管理系統(DBMS)</h3>
<ol>
  <li>資料庫通常需要一個稱做資料庫管理系統(DBMS)的全面資料庫軟體程式。</li>
  <li>DBMS是作為資料庫和其用戶或程式之間的介面，讓用戶能檢索、更新和管理，使得資訊組織化和最佳化。</li>
  <li>DBMS亦可促進對資料庫的監督和控制，增強各方面的管理運作，包括績效監控、最佳化、備份以及系統恢復。</li>
</ol>

<hr />

<h2 id="database-system-design-process">Database System Design Process</h2>

<p>好的資料庫系統設計對於資料庫的維護、更新及修改相當重要，另外，如何有效的儲存、提取資料也是資料庫重要的一環，接下來要介紹資料庫的設計。</p>

<p><strong>資料庫系統設計（Database System Process）</strong>有兩個主要的方向，</p>

<ol>
  <li><strong>Database design</strong>（資料庫設計）
    <ul>
      <li>focuses on defining the database</li>
    </ul>
  </li>
  <li><strong>Application design</strong>（應用設計）
    <ul>
      <li>focuses on the programs and interfaces that access the database</li>
    </ul>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Database%20System%20Design%20Process.png" alt="Database System Design Process.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology01-2019-6up.pdf">Database System Design Process</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="database-design資料庫設計-data-modeling資料塑模">Database Design（資料庫設計）/ Data Modeling（資料塑模）</h2>

<p>(在 <a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology01-2019-6up.pdf">Database Technology</a> 講義裡寫的是 Database Design，但找到兩個台灣的講義（<a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">第1章 資料庫的基礎</a>、<a href="https://www.mis.nsysu.edu.tw/db-book/PDF/Ch2.pdf">第二章 實體關係模式:基本概念</a>）都是用資料塑模（Data Modeling），所以這兩個東西我把它們放在一起)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Data%20Modeling.png" alt="Data Modeling.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">Data Modeling</a></td>
    </tr>
  </tbody>
</table>

<h3 id="資料庫設計階段phases-for-designing-a-database">資料庫設計階段（Phases for designing a database）:</h3>
<ol>
  <li>Requirements specification and analysis</li>
  <li><strong>Conceptual design</strong>（Conceptual model，概念塑模)
    <ul>
      <li>e.g., using the Entity-Relationship model (ER Model)</li>
    </ul>
  </li>
  <li><strong>Logical design</strong>（Logical model，邏輯塑模)
    <ul>
      <li>e.g., using the relational model (關聯模型)</li>
    </ul>
  </li>
  <li><strong>Physical design</strong>（Physical model，實體塑模)
    <ul>
      <li>e.g., 設定索引</li>
    </ul>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://www.guru99.com/images/DatabaseDesignProcess(1).png" alt="Database development life cycle" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.guru99.com/database-design.html">Database development life cycle</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="conceptual-model概念塑模">Conceptual model（概念塑模）</h2>

<p>概念塑模的目的是將現實中某部分的資料關係用<strong>結構化</strong>的方式呈現，建立整個資料庫邏輯結構的模型，過程不涉及任何資料庫管理系統、資料庫種類、軟體和實際儲存結構。最常使用<em>實體關係圖</em>（Entity Relationship Diagram）來繪製<em>實體關聯模型</em>（Entity-Relationship Model）。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/ER%20Model.png" alt="ER Model.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">ER Model</a></td>
    </tr>
  </tbody>
</table>

<h3 id="entity-relationship-model-er-model">Entity-Relationship model (ER Model)</h3>

<ol>
  <li>ER Model 則是用來繪製<strong>結構化</strong>資料的概念圖。</li>
  <li>ER Model 組成元件包括實體（Entity）和關係（Relationship）。
    <ul>
      <li>實體（Entity）- 是在真實世界識別出的東西，例如：老師、學生、車子、品牌。</li>
      <li>關聯性（Relationships）- 在二個或多個實體間擁有的關係，主要分為三種：一對一、一對多、多對多。</li>
    </ul>
  </li>
</ol>

<p><a href="https://shihs.github.io/blog/database/2020/01/01/Database-ER-Model實體關係圖/">下一篇</a>有詳細解說</p>

<hr />

<h2 id="logical-model邏輯塑模">Logical model（邏輯塑模）</h2>

<p>使用的工具是關聯模型（Relational Model），最後會產生資料表的定義關聯綱目（schema）。</p>

<p>邏輯模型主要是由三種元素所組成，如下所示:</p>
<ul>
  <li>資料結構（Data Structures）：資料的組成方式，就是欄和列組成表格的關聯表(Relations)</li>
  <li>資料操作或運算（Data Manipulation 或 Operations）：資料的相關操作，關聯式代數(Relational Algebra)和關聯式計算(Relational Calculus)</li>
  <li>完整性限制條件（Integrity Constraints）：維護資料完整性的條件，其目的是確保儲存的資料是合法的資料</li>
</ul>

<p>資料庫系統演進各年代的資料庫系統中，其使用的資料庫模型就是邏輯資料模型，主要有四種邏輯資料模型，如下所示:</p>
<ul>
  <li>階層式模型（Hierarchical Model）</li>
  <li>網路式模型（Network Model）</li>
  <li><strong>關聯式模型（Relational Model）</strong><a href="(https://zh.wikipedia.org/wiki/关系模型)">wiki</a></li>
  <li>物件導向式模型（Object-Oriented Model）</li>
</ul>

<h3 id="relational-model-relational-data-model">Relational Model (Relational Data Model)</h3>

<ul>
  <li>中文稱「關聯（式）模型」。</li>
  <li>Relational database 關聯式資料庫: represent data as a collection of relations。「關聯式資料庫」使用多個關聯表（relations）來呈現資料。</li>
</ul>

<p><strong>Relation（關聯表）</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/relation.png" alt="relation.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology02-2019-6up.pdf">relation (關聯表)</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>Relation 是關聯表（如上圖），是包含 data 的 table（表格）</p>
  </li>
  <li>Each row (tuple) represents a record of related data values
    <ul>
      <li>一列（row）稱為一個值組（tuple），紀錄各個 data 的值</li>
    </ul>
  </li>
  <li>Each column (attribute) holds a corresponding value for each row
    <ul>
      <li>Columns associated with a data type (domain)</li>
      <li>Each column header: attribute name</li>
    </ul>
  </li>
  <li><strong>Domain</strong>
    <ul>
      <li>Domain is a set of <em>atomic values</em>. 
<br />
e.g., \(\{ 0, 1, 2, ... \}\), \(\{\text{Jo Smith}, \text{Dana Jones}, \text{Ashley Wong}, \text{Y. K. Lee}, ...\}\)</li>
      <li><em>atomic values</em>: Each value indivisible（不可分割）</li>
      <li>Domains specified by <em>data type</em>(integer, string, date, real, etc.)</li>
    </ul>
  </li>
  <li><strong>Relation Schema</strong>
    <ul>
      <li>Schema describes the relation（關聯表）</li>
      <li>關聯表綱要（relation schema）是由關聯表名稱 R（relation name）和一連串屬性 \((A_1,A_2,….,A_n)\)清單（list）還有定義域（domain）所組成</li>
      <li><strong>integrity constraints</strong>（完整性限制）</li>
      <li>Denoted by \(R(A_1, A_2, ..., A_n)\)</li>
    </ul>
  </li>
  <li><strong>Attribute</strong> \(A_i\)
    <ul>
      <li>Name of a role in the relation schema R</li>
      <li>使用 \(dom(A_i)\) 的符號來說明屬性 \(A_i\) 的定義域的範圍是什麼</li>
      <li>屬性名稱不可重複，但 domain 可以重複</li>
    </ul>
  </li>
  <li><strong>NULL Values</strong>
    <ul>
      <li>Each domain may be augmented with a special value called NULL</li>
    </ul>
  </li>
  <li><em>Question:</em> A relation schema consists of?
    <ul>
      <li>Ans: relation name, attribute names and domains, and integrity constraints</li>
    </ul>
  </li>
</ul>

<h3 id="integrity-constraints完整性限制">Integrity Constraints（完整性限制）</h3>

<ul>
  <li>建立檢查資料庫儲存資料的依據和保障資料的正確性。不但可以防止授權使用者將不合法資料存入資料庫，還能夠避免關聯表間的資料不一致。</li>
  <li>關聯式資料庫模型的完整性限制條件有很多種，適用所有關聯式資料庫的完整性限制條件有四種，如下所示:
    <ul>
      <li>鍵限制條件(Key Constraints)</li>
      <li>定義域限制條件(Domain Constraints)</li>
      <li>實體完整性(Entity Integrity)</li>
      <li>參考完整性(Referential Integrity)</li>
    </ul>
  </li>
</ul>

<p><strong>鍵限制條件（Key Constraints）</strong></p>

<p>鍵限制條件（Key Constraints）是指關聯表一定擁有一個唯一和最小的主鍵（Primary Key）</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/keys.png" alt="keys.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">Key</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>超鍵（Superkeys）
    <ul>
      <li>是關聯表綱要的單一屬性或屬性值集合，超鍵需要滿足唯一性（Uniqueness）</li>
      <li>Superkeys 可以由一個或多個行（屬性）組成，只要唯一就可以</li>
      <li>下方範例關聯表來看，符合的有(sid)、(SSN)、(sid, SSN)、(sid, ename)、(SSN, cname)、(sid, tel)、(SSN, cname, postcode)……</li>
    </ul>
  </li>
  <li>候選鍵（Candidate Keys）
    <ul>
      <li>候選鍵（Candidate Keys）是一個超鍵，在每一個關聯表至少擁有一個候選鍵，不只滿足超鍵的唯一性，還需要滿足最小性（Minimality）</li>
      <li>Candidate Keys 是超鍵的子集合，但不同的是，候選鍵還必須要有最小性，所以單一屬性的超鍵一定是候選鍵</li>
      <li>舉個例子，如果下面的範例多了一個學生 sid = 5, ename = Jane，這時候 (sid, ename) 就不是候選鍵，因為去掉 ename 也能區分每一行，所以 ename 是多餘的</li>
    </ul>
  </li>
  <li>主鍵（Primary Key）
    <ul>
      <li>符合的有(sid)、(SSN)</li>
      <li>從候選鍵中選出一個作為主鍵，挑選主鍵的原則如下所示:
        <ul>
          <li>不可為空值(Not Null)</li>
          <li>永遠不會改變(Never Change)</li>
          <li>非識別值(Nonidentifying Value)</li>
          <li>簡短且簡單的值(Brevity and Simplicity)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>替代鍵（Alternate Keys）
    <ul>
      <li>在候選鍵中不是主鍵的其他候選鍵稱為替代鍵(Alternate Keys)，因為這些是可以用來替代主鍵的候選鍵</li>
      <li>如果 (sid) 是主鍵，則 (SSN) 是替代鍵</li>
    </ul>
  </li>
  <li>外來鍵（Foreign Keys）
    <ul>
      <li>是關聯表的單一或多個屬性的集合，它的屬性值是參考到其他關聯表的主鍵，用來建立兩個關聯表間的連接</li>
      <li>換句話說，Foreign Keys 是其他關聯表的主鍵，它（們）可以是該關聯表的主鍵，但不一定是</li>
      <li>外來鍵和參考的主鍵屬於相同定義域，不過屬性名稱可以不同</li>
      <li>外來鍵可以是空值NULL</li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Students.png" alt="Students.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">範例關聯表</a></td>
    </tr>
  </tbody>
</table>

<p><strong>定義域限制條件(Domain Constraints)</strong></p>
<ul>
  <li>每個值都必須是基元值且必須在定義域內</li>
  <li>例如，屬性 age 的定義域是 int，屬性值可以為5 ，但不可以是4.5</li>
</ul>

<p><strong>實體完整性(Entity Integrity)</strong></p>
<ul>
  <li>實體完整性是關聯表內部的完整性條件，主要是用來規範關聯表主鍵的使用規則</li>
  <li>主鍵（primary  key）不可以是 Null</li>
  <li>例如，（ename, cname）是主鍵，ename 屬性不可為空值；cname 屬性也不可是空值。</li>
</ul>

<p><strong>參考完整性(Referential Integrity)</strong></p>
<ul>
  <li>關聯表的所有外來鍵值，都必須能參考到另一關聯表的主鍵值</li>
</ul>

<hr />

<h2 id="physical-model實體塑模">Physical Model（實體塑模）</h2>

<ul>
  <li>針對指定資料庫管理系統建立實際資料庫結構的資料模型，例如:SQL Server</li>
  <li>對於關聯式資料庫模型的實體模型來說，就是在資料庫管理系統軟體建立關聯表(Relation) 的表格、關聯性(Relationship)和索引等定義資料</li>
  <li>簡單來說，就是將 Conceptual model 和 Logical model 實際建立出來</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/%E5%AF%A6%E9%AB%94%E6%A8%A1%E5%9E%8B.png" alt="實體模型.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">實體模型</a></td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">第1章 資料庫的基礎</a>
<br />
<a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">第3章 關聯式資料庫模型</a>
<br />
<a href="https://www.oracle.com/tw/database/what-is-database.html">Oracle - 什麼是資料庫？</a>
<br />
<a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology01-2019-6up.pdf">Database Technology - Topic 1: Introduction</a>
<br />
<a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology02-2019-6up.pdf">Database Technology - Topic 2: Relational Databases</a>
<br />
<a href="http://championdatablog.com/自動草稿/">深入了解關聯式資料模型(Relational Data Model)</a>
<br />
<a href="https://www.mis.nsysu.edu.tw/db-book/PDF/Ch2.pdf">第二章 實體關係模式:基本概念</a>
<br />
<a href="https://www.mysql.tw/2015/04/data-modeling.html">Data Modeling (資料塑模) : 概念塑模、邏輯塑模、實體塑模</a>
<br />
<a href="https://www.itread01.com/content/1545320464.html">快速理解資料庫超鍵，候選鍵，主鍵</a></p>

 -->
    
    <div class="post-excerpt"><h2 id="資料data與資訊information">資料(Data)與資訊(Information)</h2>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//database/2019/12/31/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/#disqus_thread';
        this.page.identifier = '/database/2019/12/31/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//database/2019/12/31/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/#disqus_thread" data-disqus-identifier="/database/2019/12/31/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/">
        [NLP]NLTK & spaCy 筆記
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 17, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 17, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#NLP">NLP</a>
        
      
    </span>

    </div>
    <!-- <p>常見的 NLP package 有 NLTK、spaCy、Scikit-learn 等等，參考下面的功能比較圖。</p>

<p><img src="http://localhost:4000/blog/img/posts/NLPlibraresPython.png" alt="" /></p>

<p>這裡則要分別介紹 NLTK 和 spaCy 的一些用法。從上面的比較圖我們知道，這兩個 package 基本上功能相似，在 preprocessing 時一定會使用到，但兩者有各自的優缺點。</p>

<h3 id="nltk">NLTK</h3>

<p>主要用於處理斷詞、詞頻、 pos 等等，更多用法請參照下面連結。</p>

<p><a href="https://github.com/shihs/732A92-TextMining/blob/master/note/NLTK.ipynb">NLTK</a></p>

<h3 id="spacy">spaCy</h3>

<p>spaCy 是 NLTK 的主要競爭者，它們的功能類似。</p>

<p><a href="https://github.com/shihs/732A92-TextMining/blob/master/note/spaCy.ipynb">spaCy</a></p>

 -->
    
    <div class="post-excerpt"><p>常見的 NLP package 有 NLTK、spaCy、Scikit-learn 等等，參考下面的功能比較圖。</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/#disqus_thread';
        this.page.identifier = '/nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/#disqus_thread" data-disqus-identifier="/nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/text%20mining/2019/11/26/Text-Mining-Word-embeddings/">
        [Text Mining]Word embeddings
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 26, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 26, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>

    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="word-embeddings">Word embeddings</h2>

<p>對於人來說，要理解文字並不是件困難的事，但對電腦來說，每個字不過是一串 string，所以當我們要做 text mining 時就必須要將這些 string 轉化成電腦可以理解的方式。
而 word embedding(word vector or word representation) 的概念就是將文字轉換成 vector ，好讓電腦可以讀懂文字間的關係。</p>

<p>譬如說，人類可以理解 pretty 和 beautiful 是相近詞，但如果只是給電腦這兩個單字，對於電腦來說，這只是兩個不同長度的 string 罷了。word embedding 會將這兩個字轉換成不同的 vector 映射到一個高維空間，當這兩個 vector 越接近（可以使用 consine similarity）就表示這兩個詞越相近。這就是 word embedding 主要的概念。</p>

<ul>
  <li>A word embedding is a mapping of words to points in a vector space such that nearby words (points) are similar in terms of their distributional properties.</li>
</ul>

<h3 id="the-distributional-principle">The distributional principle</h3>

<p>word embedding 方法可以使用最重要的就是因為有 distributional hypothesis 這個假設。</p>

<ul>
  <li>The distributional principle states that words that occur in similar contexts tend to have similar meanings.</li>
</ul>

<p>這裡的概念是說，詞(target words)出現在類似的上下文中(context words)，則它們很有可能有相似的意思。
譬如說，
<br />
-「那隻『貓』好可愛」
<br />
-「那隻『狗』好可愛」
<br />
這時候除了『貓』和『狗』外，這兩句話的上下文是一樣的，根據 distributional principle，這兩個詞應該是相似的。</p>

<h3 id="co-occurrence-matrix">Co-occurrence matrix</h3>

<p><img src="http://localhost:4000/blog/img/posts/Co-occurrence%20matrix.png" alt="" /></p>

<p>上圖中，context words 就是上下文，而 target words 就是我們想要分析的字詞。
<br />
現在來看 cheese 這個字，可以看到和 butter, cake, cow, deer 這幾個字一起出現的次數分別是，12, 2, 1和0次。看起來和 butter 還有 cake 連結性比較強。
再來看 bread 這個字，同樣的在 butter 和 cake 上的連結也比較強。如果我們把這兩個單字用向量表示就會是，(12, 2, 1, 0) 和 (5, 5, 0, 0)，可以去比較和其他兩個單字的 cosine similarity，這兩個的關係是比較強的。</p>

<p>現在我們把它們畫出來（因為它們被映射到的是四維空間，所以老師的 slide 只看 cow 和 cake 這兩個 context words）</p>

<p><img src="http://localhost:4000/blog/img/posts/From%20co-occurrences%20to%20word%20vectors.png" alt="" /></p>

<p>如果還是不太明白，<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">這篇部落格</a>應該可以看懂。</p>

<p><strong>補充：</strong>
從上面的 matrix 可能會想到一件事，<em>context words</em> 等於是決定了 target words 的向量啊！
換句話說，當我們分析不同的文本的時候，會需要不同的 context words來算出 word embeddings。想像，如果今天要分析新聞報導和 ptt 內容，不太可能使用一樣的 context words，畢竟ptt用語和新聞用語會有很大的不同。（蛤？你說記者都抄 ptt 內容嗎？XDDD）</p>

<hr />

<h3 id="simple-applications-of-word-embeddings">Simple applications of word embeddings</h3>

<p>word embeddings 的應用</p>

<ul>
  <li>finding similar words. 找到相似的字，像是上面的例子，找出哪一個字和 cheese 比較相似。</li>
  <li>answering ‘odd one out’ questions. 找出不一樣的詞，譬如說  lunch, breakfast, dinner, car 哪一個詞屬於不同類？ (根據上面提到的概念，lunch, breakfast, dinner 這三個的 vector 應該會比較接近，會在比較接近的上下文中出現)</li>
</ul>

<h3 id="limitations-of-word-embeddings">Limitations of word embeddings</h3>

<ul>
  <li>
    <p>There are many different facets of ‘similarity’. Ex. Is a cat more similar to a dog or to a tiger? (在不同情境下，cat 和 dog 可能比較相似。譬如說，貓和狗都是寵物，但如果以生物的角度來看，cat 和 tiger 都屬於貓科動物，這時候 cat 和 tiger 會比較相似)</p>
  </li>
  <li>
    <p>Text data does not reflect many ‘trivial’ properties of words. Ex. more ‘black sheep’ than ‘white sheep’ (如果只分析文本，因為大部分的羊都是白色的，所以在提到羊的時候並不會特別提到顏色，但當提到比較稀少的黑羊時，反而會特別說到 black，這會導致在分析時好像黑羊出現的頻率比白羊出現的頻率高)</p>
  </li>
  <li>
    <p>Word vectors reflect social biases in the data used to train them. Ex. including gender and ethnic stereotypes (<a href="https://www.pnas.org/content/115/16/E3635">論文</a>參考) 很多詞語上的用法其實帶有非常多的社會偏見和刻板印象，而這也會導致分析出的結果有所偏差。</p>
  </li>
</ul>

<hr />

<p><strong>還有什麼問題？</strong></p>

<p>到目前為止，看起來都非常合理，那還會有什麼問題呢？</p>

<p>這裡會碰到和之前提到過的，矩陣<em>稀疏性</em>的問題。如果今天 context words 有十萬個字，那麼 target words 就會是在十萬維度的空間的 vectors，而且可能會有很多的值都是 0 的狀況發生。那這樣要用什麼方法解決矩陣的稀疏性並產生 word embeddings（也就是每個詞的向量） 呢？</p>

<p>從不同的面向來看幾個常見的 word embedding 方法，</p>
<ul>
  <li>Learning word embeddings via <em>matrix factorization</em>
    <ol>
      <li>Singular Value Decomposition(SVD)</li>
      <li>Positive Pointwise mutual information(PPMI)</li>
    </ol>
  </li>
  <li>Learning word embeddings via <em>language models</em>
    <ol>
      <li>N-gram</li>
      <li>Neural language models(Ex. word2vec)</li>
    </ol>
  </li>
</ul>

<p>以下就要來介紹這幾種方法。</p>

<hr />

<h2 id="matrix-factorization---singular-value-decompositionsvd">Matrix factorization - Singular Value Decomposition(SVD)</h2>

<ul>
  <li>The rows of co-occurrence matrices are long and sparse. Instead, we would like to have word vectors that are short and dense. 簡單來說，co-occurrence matrices 會有稀疏性的問題。</li>
  <li>One idea is to approximate the co-occurrence matrix by another matrix with fewer columns. Singular Value Decomposition 的想法是，將這個又長又臭的 co-occurrence matrix 用另比較少 columns 的 matrix 取代。</li>
</ul>

<h3 id="什麼是-singular-value-decomposition奇異值分解">什麼是 Singular value decomposition（奇異值分解）?</h3>

<p>推薦<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">李宏毅老師的線性代數</a></p>
<ul>
  <li><em>Singular value decomposition(SVD)</em> can be applied on any matrix. (不需要是方陣。比較：PCA(特徵值分解) 也是一個降維的方法，但它的矩陣就必須要是方陣。)</li>
</ul>

<p>SVD 的概念就是，任一一個矩陣 \(A_{m \times n}\)，它都可以拆解成三個矩陣（\(U_{m \times n}, \Sigma_{m \times n}, V^T_{n \times n}\)）的相乘。</p>

<p>其中，\(U_{m \times n}\) 的 columns 是 <em>Orthonormal</em>，而 \(V^T_{n \times n}\) 的 rows 是 <em>Orthonormal</em>，\(\Sigma_{m \times n}\) 是 <em>Diagonal</em>(只有對角線有非負的值，且由大到小)。</p>

<p><em>(在線性代數中，一個內積空間的正交基（orthogonal basis）是元素兩兩正交的基。稱基中的元素為基向量。 假若，一個正交基的基向量的模長都是單位長度1，則稱這正交基為標準正交基或”規範正交基”（Orthonormal basis）。)</em></p>

<p><img src="http://localhost:4000/blog/img/posts/SVD.png" alt="" /></p>

<ul>
  <li>\(\Sigma_{m \times n}\) 會是一個長得像這樣的矩陣，且 \(\sigma_1 \ge \sigma_1 \ge \ldots \ge \sigma_k\)</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Sigma.png" alt="" /></p>

<p>而 \(\sigma_r, ~~where~~1 \le r \le k\) 是奇異值（singular value），而 r 越小也代表了該值越重要，換句話說，含有越多訊息，因此我們可以只保留 \(\Sigma\) 較重要的前面幾行得到一個相似的矩陣 \(A\)。用較小的儲存空間就可以得到接近原始的矩陣 \(A\)。</p>

\[A_{m \times n} \approx U_{m \times r} \times \Sigma_{r \times r} \times V^T_{r \times n}\]

<p>參考<a href="https://ccjou.wordpress.com/2009/09/01/奇異值分解-svd/">線代啟示錄-奇異值分解 (SVD)</a>的圖，</p>

<p><img src="http://localhost:4000/blog/img/posts/svd2.jpg" alt="" /></p>

<p>回到我們的 word-embedding。我們可以利用 SVD 進行去噪及降維，刪除一些不那麼重要的訊息，用來解決 Co-occurrence matrix 稀疏性的問題。</p>

<p>我們也不需要再將相乘矩陣，直接使用 𝑼 就好，每一列就代表一個 target word。</p>

<ul>
  <li>Each row of the (truncated) matrix 𝑼 is a k-dimensional vector that represents the ‘most important’ information about a word.</li>
  <li>A practical problem is that computing the singular value decomposition for large matrices is expensive.</li>
</ul>

<p>這邊看一個<a href="https://www.itread01.com/content/1569742263.html">例子</a>，</p>

<p>下圖是一個 Co-occurrence matrix \(~A_{m \times n}\)</p>

<p><img src="http://localhost:4000/blog/img/posts/term-document.png" alt="" /></p>

<p>將上面的矩陣 \(A\) 使用 SVD 分解、降維，只留下前三個特徵值。每個特徵值的大小表示對應位置的屬性值的重要性大小，左奇異矩陣的每一列即代表每個詞的特徵向量，右奇異矩陣的每一行表示每個文件的特徵向量。</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD%20example.png" alt="" /></p>

<p>取每個向量後兩維的對應值投影到一個二維空間，如下所示</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD%20example%202.png" alt="" /></p>

<p>上圖中，一個紅色的點對應一個詞，一個藍色的點對應一個文件。當這些點被投影到空間中，我們可以對這些詞和文件進行分類，比如說stock和market可以放在一類，real和estate可以放在一類，按這樣的分類結果，我們就可以知道文件中哪些事相近的詞，所以當使用者利用詞搜尋文件的時候，我們就可以利用相近的詞（在向量空間中相近的詞、被歸為同一類的詞）進行檢索，而不是只是使用完全相同的詞搜尋。</p>

<hr />

<h2 id="matrix-factorization---positive-pointwise-mutual-informationppmi">Matrix factorization - Positive Pointwise mutual information(PPMI)</h2>

<h3 id="pointwise-mutual-informationpmi">Pointwise mutual information(PMI)</h3>

<ul>
  <li>
    <p>Raw counts favour pairs that involve very common contexts. 
<br />
E.g. the cat, a cat will receive higher weight than cute cat, small cat.</p>
  </li>
  <li>
    <p>We want a measure that favours contexts in which the target word occurs more often than other words.</p>
  </li>
  <li>
    <p>A suitable measure is pointwise mutual information (PMI):</p>
  </li>
</ul>

\[PMI(x, y) = log \frac{P(x, y)}{P(x) \times P(y)}\]

<p>簡單來說，我們可以用 PMI 公式來看兩個字之間的關係。</p>

<p>現在我們把 \(x\) 看成我們的 target word，\(y\) 看成我們的 context word，</p>

<ul>
  <li>We want to use PMI to measure the associative strength between a word \(w\) and a context \(c\) in a data set \(D\):</li>
</ul>

\[PMI(w, c) = log \frac{P(w, c)}{P(w) \times P(c)} = log \frac{\#(w, c)/|D|}{\#(w)/|D| \cdot \#(c)/|D|} = log \frac{\#(w,c) \cdot |D|}{\#(w) \cdot \#(c)}\]

<p>但根據上面的公式，會發現一個問題，PMI is infinitely small for unseen word–context pairs, and undefined for unseen target words. (如果 \(w\) 和 \(c\) 並沒有共同出現過，再取 log，整個值會變成 -Inf)</p>

<p>所以這時候就有了 <strong>Positive Pointwise mutual information(PPMI)</strong>。</p>

<ul>
  <li>In positive pointwise mutual information (PPMI), all negative and undefined values are replaced by zero:</li>
</ul>

\[􏰜􏰜􏰝􏰞􏰍􏰂􏰠PPMI(w, c) = max(PMI(w, c), 0)\]

<ul>
  <li>PPMI assigns high values to rare events, it is advisable to apply a count threshold or smooth the probabilities.</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Computing%20PPMI%20on%20a%20word%E2%80%93context%20matrix.png" alt="" /></p>

<p>看一個<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">例子</a>，</p>

<p>下圖是一個 Co-occurrence matrix，列是 target words，行是 context words
<img src="http://localhost:4000/blog/img/posts/PMI%20example.png" alt="" /></p>

<p>假設這篇文章總共只有 19 個字，這裡我們計算 x = information，y = data 的 PMI 值，
<img src="http://localhost:4000/blog/img/posts/PMI%20example%202.png" alt="" /></p>

<p>根據同樣的方式可以求出所有 target words 對應的 context words 的 PMI 值。</p>

<hr />

<h2 id="language-models">Language models</h2>

<ul>
  <li>A <em>probabilistic language model</em> is a probability distribution over sequences of words in some language.</li>
  <li>Recent years have seen the rise of <em>neural language models</em>, which are based on distributed representations of words.</li>
  <li>By the chain rule, the probability of a sequence of 𝑁 words can be computed using conditional probabilities as</li>
</ul>

\[p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_1 \ldots w_{k-1})\]

<ul>
  <li>To make probability estimates more robust, we can approximate the full history \(w_1 \ldots w_N\) by the last few words (馬可夫鍊):</li>
</ul>

\[p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_{k-n+1} \ldots w_{k-1})\]

<hr />

<h2 id="language-models---n-gram-models">Language models - N-gram models</h2>

<ul>
  <li>
    <p>An n-gram is a contiguous sequence of n words or characters. 
<br />
E.g. unigram (Text), bigram (Text Mining), trigram (Text Mining course)</p>
  </li>
  <li>
    <p>An n-gram model is a language model defined on n-grams –  a probability distribution over sequences of n words.</p>
  </li>
  <li>
    <p>n-gram 是一種語言機率模型。一句話出現的機率是一個聯合模型。如果一個詞的出現只考慮前面一個字，那就是 bi-gram；如果一個詞的出現考慮前面兩個字，那就是 tri-gram。</p>
  </li>
</ul>

<p><strong>Formal definition of an n-gram model</strong></p>

<ul>
  <li>\(n\): the model’s order (1 = unigram, 2 = bigram, …)</li>
  <li>\(V\): a set of possible words (character); the vocabulary</li>
  <li>\(P(w\mid u)\): a probability that specifies how likely it is to observe the word \(w\) after the context 
<br />(n − 1)-gram \(u\)</li>
</ul>

<p><strong>Unigram model</strong></p>

<p><em>n = 1</em> 不考慮前面出現的字。
<img src="http://localhost:4000/blog/img/posts/Unigram%20model.png" alt="" /></p>

<p>Thus contexts are empty.</p>

<p><strong>MLE of unigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/MLE%20of%20unigram%20probabilities.png" alt="" /></p>

<hr />

<p><strong>Bigram models</strong></p>

<p><em>n = 2</em> 考慮前面出現的一個字。</p>

<p><img src="http://localhost:4000/blog/img/posts/Bigram%20models.png" alt="" /></p>

<p>Thus contexts are unigrams.</p>

<p><strong>Estimating bigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Estimating%20bigram%20probabilities.png" alt="" /></p>

<p><strong>Example</strong>
(<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">source</a>)
<img src="http://localhost:4000/blog/img/posts/bigram%20example.png" alt="" /></p>

<hr />

<p><strong>Smoothing</strong></p>

<p>當在計算 bigram 時可能會碰到兩個字完全沒有相鄰的狀況，這會導致算出來的機率等於 0。（如下圖）</p>

<p><img src="http://localhost:4000/blog/img/posts/bigram%20smoothing.png" alt="" /></p>

<p>這種時候就需要用到，smoothing。</p>

<p><strong>Smoothing methods</strong></p>
<ul>
  <li>Additive smoothing</li>
  <li>Good-Turing estimate</li>
  <li>Jelinek-Mercer smoothing (interpolation)</li>
  <li>Katz smoothing (backoff)</li>
  <li>Witten-Bell smoothing</li>
  <li>Absolute discounting</li>
  <li>Kneser-Ney smoothing</li>
</ul>

<p>上面的狀況碰到的是，”CHER” 後面沒有出現 “READ” 的狀況，而導致機率等於0，但如果現在是 “CHER” 這個字從未出現在資料集中呢？這種狀況時，smoothing 便派不上用場了。</p>

<ul>
  <li>In addition to unseen words, a new text may even contain <em>unknown words</em>. For these, smoothing will not help.</li>
</ul>

<p><strong>Unknown words</strong>
<br />
建立一個 token <UNK>，如果是 unknown words 就用 <UNK> 當作普通的單詞處理。</UNK></UNK></p>

<ul>
  <li>A simple way to deal with this is to introduce a special word type <em>UNK</em>, and to smooth it like any other word type in the vocabulary.</li>
  <li>When we compute the probability of a document, then we first replace every unknown word with <em>UNK</em>.</li>
</ul>

<hr />

<h2 id="language-models---neural-networks-as-language-models">Language models - Neural networks as language models</h2>

<h3 id="advantages-of-neural-language-models">Advantages of neural language models</h3>
<ul>
  <li>Neural models can achieve better perplexity than probabilistic models, and scale to much larger values of n.</li>
  <li>Words in different positions share parameters, making them share statistical strength. (Everything must pass through the hidden layer.)</li>
  <li>The network can learn that in some contexts, only parts of the  n-gram are informative. (implicit smoothing, helps with unknown words)</li>
</ul>

<h3 id="word2vec">word2vec</h3>

<ul>
  <li>word2vec 是 word embedding 的一種</li>
  <li>
    <p>word2Vec 主要有 CBOW (continuous bag-of-words) 和 skip-gram 兩種模型</p>
  </li>
  <li>CBOW 是給定上下文，來預測輸入的字詞；Skip-gram 則是給定輸入字詞後，來預測上下文</li>
</ul>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1">詞向量介紹</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">自然語言處理 – Vector Space of Semantics</a>
<br />
<a href="https://zhuanlan.zhihu.com/p/26306795">[NLP] 秒懂词向量Word2vec的本质</a>
<br />
<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">李宏毅老師的線性代數 - SVD</a>
<br />
<a href="http://www.shuang0420.com/2017/03/21/NLP%20笔记%20-%20再谈词向量/">NLP 笔记 - 再谈词向量</a>
<br />
<a href="https://www.twblogs.net/a/5c7bdd70bd9eee339918b4da">機器學習筆記之二十二——PCA與SVD</a>
<br />
<a href="https://ccjou.wordpress.com/2009/09/01/奇異值分解-svd/">線代啟示錄-奇異值分解 (SVD)</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">自然語言處理 – Pointwise Mutual Information</a>
<br />
<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing</a>
<br />
<a href="https://kknews.cc/tech/83yx3qn.html">機器學習五分鐘：自然語言處理（NLP）的N-gram模型是什麼？</a>
<br />
<a href="https://www.itread01.com/content/1569742263.html">詞向量(one-hot/SVD/NNLM/Word2Vec/GloVe)</a></p>
 -->
    
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//text%20mining/2019/11/26/Text-Mining-Word-embeddings/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/26/Text-Mining-Word-embeddings/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//text%20mining/2019/11/26/Text-Mining-Word-embeddings/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/26/Text-Mining-Word-embeddings/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/">
        [Text Mining]Text clustering and topic modelling
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 21, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 21, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>

    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<p>在進入 Text clustering 前我想要先介紹 <strong>Clustering</strong>。</p>

<h2 id="clustring">Clustring</h2>

<ul>
  <li>
    <p>Clustering (分群）是一種 unsupervised learning（非監督學習）</p>
  </li>
  <li>Typical applications
    <ol>
      <li>As a stand-alone tool to get insight into data distribution</li>
      <li>As a preprocessing step for other algorithms</li>
    </ol>
  </li>
  <li>Cluster: a collection of data objects。但什麼樣的 data 會被歸在同一個 cluster 呢？ 這就是 clustering 最重要的概念，<strong>Similar</strong> 和 <strong>Dissimilar</strong>。</li>
  <li><strong>Similar</strong> to one another within the same cluster</li>
  <li><strong>Dissimilar</strong> to the objects in other clusters</li>
</ul>

<p>=&gt; 而提到 Similar 和 Dissimilar 就要必須要提到 distance (or similarity) measures。有了 distance（similarity）才有辦法定義 Similar 和 Dissimilar。</p>

<h3 id="distancesimilarity">Distance（Similarity）</h3>

<ul>
  <li>Distances are normally used to measure the similarity or dissimilarity between two data objects</li>
</ul>

<p>Distances 可以有很多種定義方式，但不管怎麼定義，都一定要符合以下的 properties。</p>
<ul>
  <li>\(d(i, j) \ge 0\) (non-negativity)</li>
  <li>\(d(i, i) = 0\) (identity of indiscernibles)</li>
  <li>\(d(i, j) = d(j, i)\) (symmetry)</li>
  <li>\(d(i, j) \le d(i, k) + d(k, j)\) (triangle inequality)</li>
</ul>

<p><strong>Minkowski distance</strong></p>

<p>\(d(i, j) = \sqrt[q]{(|x_{i_1} - x_{j_1}|^q + |x_{i_2} - x_{j2}|^q + \ldots + |x_{i_p} - x_{j_p}|^q)}\)
, q is a positive integer</p>

<ul>
  <li>If q = 1, d is Manhattan distance</li>
  <li>If q = 2, d is Euclidean distance</li>
</ul>

<p><strong>Binary Variables</strong></p>

<ul>
  <li>symmetric binary variables: both states are equally important; 0/1</li>
  <li>asymmetric binary variables: one state is more important than the other (e.g. outcome of disease test); 1 is the important state, 0 the other</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Contingency%20tables%20for%20Binary%20Variables.png" alt="PMF_two coins.png" /></p>

<ul>
  <li>Accuracy for symmetric binary variables</li>
</ul>

\[sim(i, j) = \frac{a+d}{a+b+c+d}\]

<ul>
  <li>Jaccard similarity for asymmetric binary variables</li>
</ul>

\[sim(i, j) = \frac{a}{a+b+c}\]

<hr />

<h2 id="text-clustering">Text clustering</h2>

<p><img src="http://localhost:4000/blog/img/posts/Conceptual%20framework%20for%20text%20mining.png" alt="PMF_two coins.png" /></p>

<p>上圖是 Conceptual framework for text mining，而這篇要介紹的是 Clustering 和 Topic Analysis。</p>

<ul>
  <li>
    <p><strong>Text clustering</strong> is the task of grouping similar texts together. What is considered ‘similar’ depends on the application.</p>
  </li>
  <li>
    <p>Clustering is a central tool in exploratory data analysis, where it can help us to get insights into the distribution of a data set.</p>
  </li>
  <li>
    <p>Clustering is also useful as a pre-processing technique in knowledge-focused applications. Example: Brown clustering</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/The%20standard%20text%20clustering%20pipeline.png" alt="PMF_two coins.png" /></p>

<hr />

<h2 id="similarity-measures">Similarity measures</h2>

<p>剛剛前面提到的 Similarity，如何用在 text 上呢？</p>

<p><strong>Accuracy for symmetric binary vectors</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Accuracy%20for%20symmetric%20binary%20vectors.png" alt="PMF_two coins.png" /></p>

<p><strong>Jaccard similarity for asymmetric binary vectors</strong></p>

<hr />

<h2 id="hard-clustering-and-soft-clustering">Hard clustering and soft clustering</h2>

<p><strong>Hard clustering</strong></p>
<ul>
  <li>Each document either belongs to a cluster or not. Ex. Hierarchical clustering(brown clustring), Partitioning clustering(k-means), Density-Based clustering(DBSCAN)</li>
</ul>

<p><strong>Soft clustering</strong></p>
<ul>
  <li>Each document belongs to each cluster to a certain degree. Ex. LDA (topic model)</li>
</ul>

<hr />

<h2 id="an-overview-of-hard-clustering-methods">An overview of hard clustering methods</h2>

<h3 id="hierarchical-clustering">Hierarchical clustering</h3>

<p>This method does not require the number of clusters k as an input, but needs a termination condition</p>

<p><img src="http://localhost:4000/blog/img/posts/Hierarchical%20Clustering.png" alt="" /></p>

<ul>
  <li>
    <p><strong>Agglomerative</strong>: Each document starts in its own cluster. Hierarchy is created by merging pairs of clusters.
將點倆倆合併，最後所有的點會全部在同一個 cluster 裡。那至於要用什麼決定要將兩個點合併呢？這時候又有 Linkage criteria 來決定，eg. Single-link, Complete-link, Average-link 等等。不同的 linkage criteria 會導致不同的分群結果，如下圖。</p>
  </li>
  <li>
    <p><strong>Divisive clustering</strong>: All documents start in one cluster. Hierarchy is created by splitting clusters recursively.</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Linkage%20criteria.png" alt="" /></p>

<ul>
  <li><strong>Brown clustring</strong></li>
</ul>

<h3 id="partitioning-clustering">Partitioning clustering</h3>
<ul>
  <li>
    <p><strong>K-means</strong></p>
  </li>
  <li>
    <p>Issues with the k-means algorithm</p>
    <ol>
      <li>The k-means algorithm always converges, but there is no guarantee that it finds a global optimum. (Solution: random restarts)</li>
      <li>The number of clusters needs to be specified in advance, or chosen based on heuristics and cross-validation. (Example: elbow method)</li>
      <li>The k-means algorithm is not good at handling outliers – every document will eventually belong to some cluster.</li>
      <li>K-means is restricted to clusters with convex shapes =&gt; Density-Based clustering</li>
    </ol>
  </li>
</ul>

<h3 id="density-based-clustering">Density-Based clustering</h3>

<ul>
  <li>The basic idea behind density-based algorithms is that different regions of the vector space can be more or less densely populated.</li>
  <li>Under this view, clusters can take any shape; they are not constrained to convex clusters as in k-means.</li>
</ul>

<p><strong>Directly density-reachable</strong></p>

<ul>
  <li>DBSCAN</li>
</ul>

<hr />

<h2 id="evaluation-of-hard-clustering">Evaluation of hard clustering</h2>

<h3 id="intrinsic-and-extrinsic-evaluation">Intrinsic and extrinsic evaluation</h3>
<ul>
  <li>In <strong>intrinsic</strong> evaluation, a clustering is evaluated based on internal measures such as coherence and separation. <em>Are documents in the same cluster similar? Are clusters well-separated?</em></li>
  <li>In <strong>extrinsic</strong> evaluation, a clustering is evaluated based on data that was not used for the clustering, such as known class labels. <em>cluster purity, Rand index</em></li>
</ul>

<h3 id="rand-index">Rand index</h3>

<p>假設一個集合中有N篇文章
一個集合中有N(N-1)/2個集合對
TP：同一類的文章被分到同一個簇
TN：不同類的文章被分到不同簇
FP：不同類的文章被分到同一個簇
FN：同一類的文章被分到不同簇
Rand Index度量的正確的百分比
RI = （TP+TN）/（TP+FP+FN+TN）</p>

<hr />

<h2 id="topic-models">Topic models</h2>
<ul>
  <li>A topic model is a statistical model for representing the abstract topics that are expressed in a collection of documents.</li>
  <li>Topic models are examples of soft clustering techniques – each document belongs to each cluster (topic) to a certain degree.</li>
</ul>

<h3 id="latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</h3>

<p>LDA有兩個原則，</p>
<ol>
  <li>每個 domcuments 是由多個 Topic 組成（each document belongs to each cluster (topic) to a certain degree）</li>
  <li>每個主題會有不同的 terms 來描述，且同樣對詞可以同時出現在不同的主題。</li>
</ol>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models.png" alt="" /></p>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models2.png" alt="" /></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://www.ida.liu.se/~732A75/info/courseinfo.en.shtml">732A75 Advanced Data Mining</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/238079-natural-language-processing-brown-clustering">自然語言處理 – Brown Clustering</a>
<br />
<a href="https://medium.com/@tengyuanchang/直觀理解-lda-latent-dirichlet-allocation-與文件主題模型-ab4f26c27184">直觀理解 LDA (Latent Dirichlet Allocation) 與文件主題模型</a>
<br />
<a href="https://www.twblogs.net/a/5b8c83e12b717718833363ae">聚類評價指標 Rand Index,RI,Recall,Precision,F1</a></p>
 -->
    
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/">0 Comments</a>
      </span>
    </div>
    
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page4">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page2">Newer</a>
    
  
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
