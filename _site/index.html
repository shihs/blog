<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blogblog/atom.xml">
  <!-- æ•¸å­¸ç¬¦è™Ÿ -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item active" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive/">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category/">Category</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search/">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2019. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts" id="begin">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/">
        [NLP]NLTK ç­†è¨˜
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 17, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 17, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#NLP">NLP</a>
        
      
    </span>


    </div>
    <!-- 
 -->
    <div class="post-excerpt">
</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/#disqus_thread';
        this.page.identifier = '/nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/#disqus_thread" data-disqus-identifier="/nlp/2019/12/17/NLP-NLTK-%E7%AD%86%E8%A8%98/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/text%20mining/2019/11/26/Text-Mining-Word-embeddings/">
        [Text Mining]Word embeddings
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 26, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 26, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>


    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="word-embeddings">Word embeddings</h2>

<p>å°æ–¼äººä¾†èªªï¼Œè¦ç†è§£æ–‡å­—ä¸¦ä¸æ˜¯ä»¶å›°é›£çš„äº‹ï¼Œä½†å°é›»è…¦ä¾†èªªï¼Œæ¯å€‹å­—ä¸éæ˜¯ä¸€ä¸² stringï¼Œæ‰€ä»¥ç•¶æˆ‘å€‘è¦åš text mining æ™‚å°±å¿…é ˆè¦å°‡é€™äº› string è½‰åŒ–æˆé›»è…¦å¯ä»¥ç†è§£çš„æ–¹å¼ã€‚
è€Œ word embedding(word vector) çš„æ¦‚å¿µå°±æ˜¯å°‡æ–‡å­—è½‰æ›æˆ vector ï¼Œå¥½è®“é›»è…¦å¯ä»¥è®€æ‡‚æ–‡å­—é–“çš„é—œä¿‚ã€‚</p>

<p>è­¬å¦‚èªªï¼Œäººé¡å¯ä»¥ç†è§£ pretty å’Œ beautiful æ˜¯ç›¸è¿‘è©ï¼Œä½†å¦‚æœåªæ˜¯çµ¦é›»è…¦é€™å…©å€‹å–®å­—ï¼Œå°æ–¼é›»è…¦ä¾†èªªï¼Œé€™åªæ˜¯å…©å€‹ä¸åŒé•·åº¦çš„ string ç½·äº†ã€‚word embedding æœƒå°‡é€™å…©å€‹å­—è½‰æ›æˆä¸åŒçš„ vector æ˜ å°„åˆ°ä¸€å€‹é«˜ç¶­ç©ºé–“ï¼Œç•¶é€™å…©å€‹ vector è¶Šæ¥è¿‘ï¼ˆå¯ä»¥ä½¿ç”¨ consine similarityï¼‰å°±è¡¨ç¤ºé€™å…©å€‹è©è¶Šç›¸è¿‘ã€‚é€™å°±æ˜¯ word embedding ä¸»è¦çš„æ¦‚å¿µã€‚</p>

<ul>
  <li>A word embedding is a mapping of words to points in a vector space such that nearby words (points) are similar in terms of their distributional properties.</li>
</ul>

<h3 id="the-distributional-principle">The distributional principle</h3>

<p>word embedding æ–¹æ³•å¯ä»¥ä½¿ç”¨æœ€é‡è¦çš„å°±æ˜¯å› ç‚ºæœ‰ distributional hypothesis é€™å€‹å‡è¨­ã€‚</p>

<ul>
  <li>The distributional principle states that words that occur in similar contexts tend to have similar meanings.</li>
</ul>

<p>é€™è£¡çš„æ¦‚å¿µæ˜¯èªªï¼Œè©(target words)å‡ºç¾åœ¨é¡ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­(context words)ï¼Œå‰‡å®ƒå€‘å¾ˆæœ‰å¯èƒ½æœ‰ç›¸ä¼¼çš„æ„æ€ã€‚
è­¬å¦‚èªªï¼Œ
<br />
-ã€Œé‚£éš»ã€è²“ã€å¥½å¯æ„›ã€
<br />
-ã€Œé‚£éš»ã€ç‹—ã€å¥½å¯æ„›ã€
<br />
é€™æ™‚å€™é™¤äº†ã€è²“ã€å’Œã€ç‹—ã€å¤–ï¼Œé€™å…©å¥è©±çš„ä¸Šä¸‹æ–‡æ˜¯ä¸€æ¨£çš„ï¼Œæ ¹æ“š distributional principleï¼Œé€™å…©å€‹è©æ‡‰è©²æ˜¯ç›¸ä¼¼çš„ã€‚</p>

<h3 id="co-occurrence-matrix">Co-occurrence matrix</h3>

<p><img src="http://localhost:4000/blog/img/posts/Co-occurrence%20matrix.png" alt="" /></p>

<p>ä¸Šåœ–ä¸­ï¼Œcontext words å°±æ˜¯ä¸Šä¸‹æ–‡ï¼Œè€Œ target words å°±æ˜¯æˆ‘å€‘æƒ³è¦åˆ†æçš„å­—è©ã€‚
<br />
ç¾åœ¨ä¾†çœ‹ cheese é€™å€‹å­—ï¼Œå¯ä»¥çœ‹åˆ°å’Œ butter, cake, cow, deer é€™å¹¾å€‹å­—ä¸€èµ·å‡ºç¾çš„æ¬¡æ•¸åˆ†åˆ¥æ˜¯ï¼Œ12, 2, 1å’Œ0æ¬¡ã€‚çœ‹èµ·ä¾†å’Œ butter é‚„æœ‰ cake é€£çµæ€§æ¯”è¼ƒå¼·ã€‚
å†ä¾†çœ‹ bread é€™å€‹å­—ï¼ŒåŒæ¨£çš„åœ¨ butter å’Œ cake ä¸Šçš„é€£çµä¹Ÿæ¯”è¼ƒå¼·ã€‚å¦‚æœæˆ‘å€‘æŠŠé€™å…©å€‹å–®å­—ç”¨å‘é‡è¡¨ç¤ºå°±æœƒæ˜¯ï¼Œ(12, 2, 1, 0) å’Œ (5, 5, 0, 0)ï¼Œå¯ä»¥å»æ¯”è¼ƒå’Œå…¶ä»–å…©å€‹å–®å­—çš„ cosine similarityï¼Œé€™å…©å€‹çš„é—œä¿‚æ˜¯æ¯”è¼ƒå¼·çš„ã€‚</p>

<p>ç¾åœ¨æˆ‘å€‘æŠŠå®ƒå€‘ç•«å‡ºä¾†ï¼ˆå› ç‚ºå®ƒå€‘è¢«æ˜ å°„åˆ°çš„æ˜¯å››ç¶­ç©ºé–“ï¼Œæ‰€ä»¥è€å¸«çš„ slide åªçœ‹ cow å’Œ cake é€™å…©å€‹ context wordsï¼‰</p>

<p><img src="http://localhost:4000/blog/img/posts/From%20co-occurrences%20to%20word%20vectors.png" alt="" /></p>

<p>å¦‚æœé‚„æ˜¯ä¸å¤ªæ˜ç™½ï¼Œ<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">é€™ç¯‡éƒ¨è½æ ¼</a>æ‡‰è©²å¯ä»¥çœ‹æ‡‚ã€‚</p>

<p><strong>è£œå……ï¼š</strong>
å¾ä¸Šé¢çš„ matrix å¯èƒ½æœƒæƒ³åˆ°ä¸€ä»¶äº‹ï¼Œ<em>context words</em> ç­‰æ–¼æ˜¯æ±ºå®šäº† target words çš„å‘é‡å•Šï¼
æ›å¥è©±èªªï¼Œç•¶æˆ‘å€‘åˆ†æä¸åŒçš„æ–‡æœ¬çš„æ™‚å€™ï¼Œæœƒéœ€è¦ä¸åŒçš„ context wordsä¾†ç®—å‡º word embeddingsã€‚æƒ³åƒï¼Œå¦‚æœä»Šå¤©è¦åˆ†ææ–°èå ±å°å’Œ ptt å…§å®¹ï¼Œä¸å¤ªå¯èƒ½ä½¿ç”¨ä¸€æ¨£çš„ context wordsï¼Œç•¢ç«Ÿpttç”¨èªå’Œæ–°èç”¨èªæœƒæœ‰å¾ˆå¤§çš„ä¸åŒã€‚ï¼ˆè›¤ï¼Ÿä½ èªªè¨˜è€…éƒ½æŠ„ ptt å…§å®¹å—ï¼ŸXDDDï¼‰</p>

<hr />

<h3 id="simple-applications-of-word-embeddings">Simple applications of word embeddings</h3>

<p>word embeddings çš„æ‡‰ç”¨</p>

<ul>
  <li>finding similar words. æ‰¾åˆ°ç›¸ä¼¼çš„å­—ï¼Œåƒæ˜¯ä¸Šé¢çš„ä¾‹å­ï¼Œæ‰¾å‡ºå“ªä¸€å€‹å­—å’Œ cheese æ¯”è¼ƒç›¸ä¼¼ã€‚</li>
  <li>answering â€˜odd one outâ€™ questions. æ‰¾å‡ºä¸ä¸€æ¨£çš„è©ï¼Œè­¬å¦‚èªª  lunch, breakfast, dinner, car å“ªä¸€å€‹è©å±¬æ–¼ä¸åŒé¡ï¼Ÿ (æ ¹æ“šä¸Šé¢æåˆ°çš„æ¦‚å¿µï¼Œlunch, breakfast, dinner é€™ä¸‰å€‹çš„ vector æ‡‰è©²æœƒæ¯”è¼ƒæ¥è¿‘ï¼Œæœƒåœ¨æ¯”è¼ƒæ¥è¿‘çš„ä¸Šä¸‹æ–‡ä¸­å‡ºç¾)</li>
</ul>

<h3 id="limitations-of-word-embeddings">Limitations of word embeddings</h3>

<ul>
  <li>
    <p>There are many different facets of â€˜similarityâ€™. Ex. Is a cat more similar to a dog or to a tiger? (åœ¨ä¸åŒæƒ…å¢ƒä¸‹ï¼Œcat å’Œ dog å¯èƒ½æ¯”è¼ƒç›¸ä¼¼ã€‚è­¬å¦‚èªªï¼Œè²“å’Œç‹—éƒ½æ˜¯å¯µç‰©ï¼Œä½†å¦‚æœä»¥ç”Ÿç‰©çš„è§’åº¦ä¾†çœ‹ï¼Œcat å’Œ tiger éƒ½å±¬æ–¼è²“ç§‘å‹•ç‰©ï¼Œé€™æ™‚å€™ cat å’Œ tiger æœƒæ¯”è¼ƒç›¸ä¼¼)</p>
  </li>
  <li>
    <p>Text data does not reflect many â€˜trivialâ€™ properties of words. Ex. more â€˜black sheepâ€™ than â€˜white sheepâ€™ (å¦‚æœåªåˆ†ææ–‡æœ¬ï¼Œå› ç‚ºå¤§éƒ¨åˆ†çš„ç¾Šéƒ½æ˜¯ç™½è‰²çš„ï¼Œæ‰€ä»¥åœ¨æåˆ°ç¾Šçš„æ™‚å€™ä¸¦ä¸æœƒç‰¹åˆ¥æåˆ°é¡è‰²ï¼Œä½†ç•¶æåˆ°æ¯”è¼ƒç¨€å°‘çš„é»‘ç¾Šæ™‚ï¼Œåè€Œæœƒç‰¹åˆ¥èªªåˆ° blackï¼Œé€™æœƒå°è‡´åœ¨åˆ†ææ™‚å¥½åƒé»‘ç¾Šå‡ºç¾çš„é »ç‡æ¯”ç™½ç¾Šå‡ºç¾çš„é »ç‡é«˜)</p>
  </li>
  <li>
    <p>Word vectors reflect social biases in the data used to train them. Ex. including gender and ethnic stereotypes (<a href="https://www.pnas.org/content/115/16/E3635">è«–æ–‡</a>åƒè€ƒ) å¾ˆå¤šè©èªä¸Šçš„ç”¨æ³•å…¶å¯¦å¸¶æœ‰éå¸¸å¤šçš„ç¤¾æœƒåè¦‹å’Œåˆ»æ¿å°è±¡ï¼Œè€Œé€™ä¹Ÿæœƒå°è‡´åˆ†æå‡ºçš„çµæœæœ‰æ‰€åå·®ã€‚</p>
  </li>
</ul>

<hr />

<p><strong>é‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿ</strong></p>

<p>åˆ°ç›®å‰ç‚ºæ­¢ï¼Œçœ‹èµ·ä¾†éƒ½éå¸¸åˆç†ï¼Œé‚£é‚„æœƒæœ‰ä»€éº¼å•é¡Œå‘¢ï¼Ÿ</p>

<p>é€™è£¡æœƒç¢°åˆ°å’Œä¹‹å‰æåˆ°éçš„ï¼ŒçŸ©é™£<em>ç¨€ç–æ€§</em>çš„å•é¡Œã€‚å¦‚æœä»Šå¤© context words æœ‰åè¬å€‹å­—ï¼Œé‚£éº¼ target words å°±æœƒæ˜¯åœ¨åè¬ç¶­åº¦çš„ç©ºé–“çš„ vectorsï¼Œè€Œä¸”å¯èƒ½æœƒæœ‰å¾ˆå¤šçš„å€¼éƒ½æ˜¯ 0 çš„ç‹€æ³ç™¼ç”Ÿã€‚é‚£é€™æ¨£è¦ç”¨ä»€éº¼æ–¹æ³•è§£æ±ºçŸ©é™£çš„ç¨€ç–æ€§ä¸¦ç”¢ç”Ÿ word embeddingsï¼ˆä¹Ÿå°±æ˜¯æ¯å€‹è©çš„å‘é‡ï¼‰ å‘¢ï¼Ÿ</p>

<p>å¾ä¸åŒçš„é¢å‘ä¾†çœ‹å¹¾å€‹å¸¸è¦‹çš„ word embedding æ–¹æ³•ï¼Œ</p>
<ul>
  <li>Learning word embeddings via <em>matrix factorization</em>
    <ol>
      <li>Singular Value Decomposition(SVD)</li>
      <li>Positive Pointwise mutual information(PPMI)</li>
    </ol>
  </li>
  <li>Learning word embeddings via <em>language models</em>
    <ol>
      <li>N-gram</li>
      <li>Neural language models(Ex. word2vec)</li>
    </ol>
  </li>
</ul>

<p>ä»¥ä¸‹å°±è¦ä¾†ä»‹ç´¹é€™å¹¾ç¨®æ–¹æ³•ã€‚</p>

<hr />

<h2 id="singular-value-decompositionsvd">Singular Value Decomposition(SVD)</h2>

<ul>
  <li>The rows of co-occurrence matrices are long and sparse. Instead, we would like to have word vectors that are short and dense. ç°¡å–®ä¾†èªªï¼Œco-occurrence matrices æœƒæœ‰ç¨€ç–æ€§çš„å•é¡Œã€‚</li>
  <li>One idea is to approximate the co-occurrence matrix by another matrix with fewer columns. Singular Value Decomposition çš„æƒ³æ³•æ˜¯ï¼Œå°‡é€™å€‹åˆé•·åˆè‡­çš„ co-occurrence matrix ç”¨å¦æ¯”è¼ƒå°‘ columns çš„ matrix å–ä»£ã€‚</li>
</ul>

<h3 id="ä»€éº¼æ˜¯-singular-value-decompositionå¥‡ç•°å€¼åˆ†è§£">ä»€éº¼æ˜¯ Singular value decompositionï¼ˆå¥‡ç•°å€¼åˆ†è§£ï¼‰?</h3>

<p>æ¨è–¦<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸</a></p>
<ul>
  <li><em>Singular value decomposition(SVD)</em> can be applied on any matrix. (ä¸éœ€è¦æ˜¯æ–¹é™£ã€‚æ¯”è¼ƒï¼šPCA ä¹Ÿæ˜¯ä¸€å€‹å¯é™ç¶­çš„æ–¹æ³•ï¼Œä½†å®ƒçš„çŸ©é™£å°±å¿…é ˆè¦æ˜¯æ–¹é™£ã€‚)</li>
</ul>

<p>SVD çš„æ¦‚å¿µå°±æ˜¯ï¼Œä»»ä¸€ä¸€å€‹çŸ©é™£ <script type="math/tex">A_{m \times n}</script>ï¼Œå®ƒéƒ½å¯ä»¥æ‹†è§£æˆä¸‰å€‹çŸ©é™£ï¼ˆ<script type="math/tex">U_{m \times n}, \Sigma_{m \times n}, V^T_{n \times n}</script>ï¼‰çš„ç›¸ä¹˜ã€‚</p>

<p>å…¶ä¸­ï¼Œ<script type="math/tex">U_{m \times n}</script> çš„ columns æ˜¯ <em>Orthonormal</em>ï¼Œè€Œ <script type="math/tex">V^T_{n \times n}</script> çš„ rows æ˜¯ <em>Orthonormal</em>ï¼Œ<script type="math/tex">\Sigma_{m \times n}</script> æ˜¯ <em>Diagonal</em>(åªæœ‰å°è§’ç·šæœ‰éè² çš„å€¼ï¼Œä¸”ç”±å¤§åˆ°å°)ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD.png" alt="" /></p>

<ul>
  <li><script type="math/tex">\Sigma_{m \times n}</script> æœƒæ˜¯ä¸€å€‹é•·å¾—åƒé€™æ¨£çš„çŸ©é™£ï¼Œä¸” <script type="math/tex">\sigma_1 \ge \sigma_1 \ge \ldots \ge \sigma_k</script></li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Sigma.png" alt="" /></p>

<p>è€Œ <script type="math/tex">\sigma_r, ~~where~~1 \le r \le k</script> æ˜¯å¥‡ç•°å€¼ï¼ˆsingular valueï¼‰ï¼Œè€Œ r è¶Šå°ä¹Ÿä»£è¡¨äº†è©²å€¼è¶Šé‡è¦ï¼Œæ›å¥è©±èªªï¼Œå«æœ‰è¶Šå¤šè¨Šæ¯ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥åªä¿ç•™ <script type="math/tex">\Sigma</script> è¼ƒé‡è¦çš„å‰é¢å¹¾è¡Œå¾—åˆ°ä¸€å€‹ç›¸ä¼¼çš„çŸ©é™£ <script type="math/tex">A</script>ã€‚</p>

<script type="math/tex; mode=display">A_{m \times n} = U_{m \times r} \times \Sigma_{r \times r} \times V^T_{r \times n}</script>

<p>åƒè€ƒ<a href="https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/">ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)</a>çš„åœ–ï¼Œ</p>

<p><img src="http://localhost:4000/blog/img/posts/svd2.jpg" alt="" /></p>

<p>å›åˆ°æˆ‘å€‘çš„ word-embeddingã€‚ä¹Ÿå°±æ˜¯èªªï¼Œä½¿ç”¨ SVD å¯ä»¥åˆ©ç”¨æ¸›å°‘ <script type="math/tex">\Sigma</script> çš„ç¶­åº¦ä¾†è™•ç†ç¨€ç–æ€§çš„å•é¡Œï¼Œé›–ç„¶åˆªé™¤äº†ä¸€äº›è©ä»èˆŠä¿ç•™é‡è¦çš„è©ã€‚</p>

<ul>
  <li>Each row of the (truncated) matrix ğ‘¼ is a k-dimensional vector that represents the â€˜most importantâ€™ information about a word.</li>
  <li>A practical problem is that computing the singular value decomposition for large matrices is expensive.</li>
</ul>

<hr />

<h2 id="positive-pointwise-mutual-informationppmi">Positive Pointwise mutual information(PPMI)</h2>

<h3 id="pointwise-mutual-informationpmi">Pointwise mutual information(PMI)</h3>

<ul>
  <li>Raw counts favour pairs that involve very common contexts. Ex.the cat, a cat will receive higher weight than cute cat, small cat.</li>
  <li>We want a measure that favours contexts in which the target word occurs more often than other words.</li>
  <li>A suitable measure is pointwise mutual information (PMI):</li>
</ul>

<script type="math/tex; mode=display">PMI(x, y) = log \frac{P(x, y)}{P(x) \times P(y)}</script>

<p>ç°¡å–®ä¾†èªªï¼Œæˆ‘å€‘å¯ä»¥ç”¨ PMI å…¬å¼ä¾†çœ‹å…©å€‹å­—ä¹‹é–“çš„é—œä¿‚ã€‚</p>

<p>ç¾åœ¨æˆ‘å€‘æŠŠ <script type="math/tex">x</script> çœ‹æˆæˆ‘å€‘çš„ target wordï¼Œ<script type="math/tex">y</script> çœ‹æˆæˆ‘å€‘çš„ context wordï¼Œ</p>

<ul>
  <li>We want to use PMI to measure the associative strength between a word <script type="math/tex">w</script> and a context <script type="math/tex">c</script> in a data set <script type="math/tex">D</script>:</li>
</ul>

<script type="math/tex; mode=display">PMI(w, c) = log \frac{P(w, c)}{P(w) \times P(c)} = log \frac{\#(w, c)/|D|}{\#(w)/|D| \cdot \#(c)/|D|} = log \frac{\#(w,c) \cdot |D|}{\#(w) \cdot \#(c)}</script>

<p>ä½†æ ¹æ“šä¸Šé¢çš„å…¬å¼ï¼Œæœƒç™¼ç¾ä¸€å€‹å•é¡Œï¼ŒPMI is infinitely small for unseen wordâ€“context pairs, and undefined for unseen target words. (å¦‚æœ <script type="math/tex">w</script> å’Œ <script type="math/tex">c</script> ä¸¦æ²’æœ‰å…±åŒå‡ºç¾éï¼Œå†å– logï¼Œæ•´å€‹å€¼æœƒè®Šæˆ -Inf)</p>

<p>æ‰€ä»¥é€™æ™‚å€™å°±æœ‰äº† <strong>Positive Pointwise mutual information(PPMI)</strong>ã€‚</p>

<ul>
  <li>In positive pointwise mutual information (PPMI), all negative and undefined values are replaced by zero:</li>
</ul>

<script type="math/tex; mode=display">ô°œô°œô°ô°ô°ô°‚ô° PPMI(w, c) = max(PMI(w, c), 0)</script>

<ul>
  <li>PPMI assigns high values to rare events, it is advisable to apply a count threshold or smooth the probabilities.</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Computing%20PPMI%20on%20a%20word%E2%80%93context%20matrix.png" alt="" /></p>

<hr />

<h2 id="language-models">Language models</h2>

<ul>
  <li>A <em>probabilistic language model</em> is a probability distribution over sequences of words in some language.</li>
  <li>Recent years have seen the rise of <em>neural language models</em>, which are based on distributed representations of words.</li>
  <li>By the chain rule, the probability of a sequence of ğ‘ words can be computed using conditional probabilities as</li>
</ul>

<script type="math/tex; mode=display">p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_1 \ldots w_{k-1})</script>

<ul>
  <li>To make probability estimates more robust, we can approximate the full history <script type="math/tex">w_1 \ldots w_N</script> by the last few words(é¦¬å¯å¤«éŠ):</li>
</ul>

<script type="math/tex; mode=display">p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_{k-n+1} \ldots w_{k-1})</script>

<h2 id="n-gram-models">N-gram models</h2>

<ul>
  <li>An n-gram is a contiguous sequence of n words or characters. Ex. unigram (Text), bigram (Text Mining), trigram (Text Mining course)</li>
  <li>An n-gram model is a language model defined on n-grams â€“ â€¨a probability distribution over sequences of n words.</li>
  <li>n-gram æ˜¯ä¸€ç¨®èªè¨€æ©Ÿç‡æ¨¡å‹ã€‚ä¸€å¥è©±å‡ºç¾çš„æ©Ÿç‡æ˜¯ä¸€å€‹è¯åˆæ¨¡å‹ã€‚å¦‚æœä¸€å€‹è©çš„å‡ºç¾åªè€ƒæ…®å‰é¢ä¸€å€‹å­—ï¼Œé‚£å°±æ˜¯ bi-gramï¼›å¦‚æœä¸€å€‹è©çš„å‡ºç¾è€ƒæ…®å‰é¢å…©å€‹å­—ï¼Œé‚£å°±æ˜¯ tri-gramã€‚</li>
</ul>

<p><strong>Formal definition of an n-gram model</strong></p>

<ul>
  <li><script type="math/tex">n</script>: the modelâ€™s order (1 = unigram, 2 = bigram, â€¦)</li>
  <li><script type="math/tex">V</script>: a set of possible words (character); the vocabulary</li>
  <li><script type="math/tex">P(w\mid u)</script>: a probability that specifies how likely it is to observe â€¨the word <script type="math/tex">w</script> after the context 
<br />(n âˆ’ 1)-gram <script type="math/tex">u</script></li>
</ul>

<hr />

<p><strong>Unigram model</strong></p>

<p><em>n = 1</em> ä¸è€ƒæ…®å‰é¢å‡ºç¾çš„å­—ã€‚
<img src="http://localhost:4000/blog/img/posts/Unigram%20model.png" alt="" /></p>

<p>Thus contexts are empty.</p>

<p><strong>MLE of unigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/MLE%20of%20unigram%20probabilities.png" alt="" /></p>

<hr />

<p><strong>Bigram models</strong></p>

<p><em>n = 2</em> è€ƒæ…®å‰é¢å‡ºç¾çš„ä¸€å€‹å­—ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/Bigram%20models.png" alt="" /></p>

<p>Thus contexts are unigrams.</p>

<p><strong>Estimating bigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Estimating%20bigram%20probabilities.png" alt="" /></p>

<p><strong>Example</strong>
(<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">source</a>)
<img src="http://localhost:4000/blog/img/posts/bigram%20example.png" alt="" /></p>

<hr />

<p><strong>Smoothing</strong></p>

<p>ç•¶åœ¨è¨ˆç®— bigram æ™‚å¯èƒ½æœƒç¢°åˆ°å…©å€‹å­—å®Œå…¨æ²’æœ‰ç›¸é„°çš„ç‹€æ³ï¼Œé€™æœƒå°è‡´ç®—å‡ºä¾†çš„æ©Ÿç‡ç­‰æ–¼ 0ã€‚ï¼ˆå¦‚ä¸‹åœ–ï¼‰</p>

<p><img src="http://localhost:4000/blog/img/posts/bigram%20smoothing.png" alt="" /></p>

<p>é€™ç¨®æ™‚å€™å°±éœ€è¦ç”¨åˆ°ï¼Œsmoothingã€‚</p>

<p><strong>Smoothing methods</strong></p>
<ul>
  <li>Additive smoothing</li>
  <li>Good-Turing estimate</li>
  <li>Jelinek-Mercer smoothing (interpolation) â€¢ Katz smoothing (backoff)</li>
  <li>Witten-Bell smoothing</li>
  <li>Absolute discounting</li>
  <li>Kneser-Ney smoothing</li>
</ul>

<hr />

<p>ä¸Šé¢çš„ç‹€æ³ç¢°åˆ°çš„æ˜¯ï¼Œâ€CHERâ€ å¾Œé¢æ²’æœ‰å‡ºç¾ â€œREADâ€ çš„ç‹€æ³ï¼Œè€Œå°è‡´æ©Ÿç‡ç­‰æ–¼0ï¼Œä½†å¦‚æœç¾åœ¨æ˜¯ â€œCHERâ€ é€™å€‹å­—å¾æœªå‡ºç¾åœ¨è³‡æ–™é›†ä¸­å‘¢ï¼Ÿé€™ç¨®ç‹€æ³æ™‚ï¼Œsmoothing ä¾¿æ´¾ä¸ä¸Šç”¨å ´äº†ã€‚</p>

<ul>
  <li>A simple way to deal with this is to introduce a special word type UNK, and to smooth it like any other word type in the vocabulary.</li>
  <li>When we compute the probability of a document, then we first replace every unknown word with UNK.</li>
</ul>

<hr />

<h2 id="neural-networks-as-language-models">Neural networks as language models</h2>

<h3 id="advantages-of-neural-language-models">Advantages of neural language models</h3>
<ul>
  <li>Neural models can achieve better perplexity than probabilistic models, and scale to much larger values of n.</li>
  <li>Words in different positions share parameters, making them share statistical strength. (Everything must pass through the hidden layer.)</li>
  <li>The network can learn that in some contexts, only parts of the â€¨n-gram are informative. (implicit smoothing, helps with unknown words)</li>
</ul>

<h3 id="word2vec">word2vec</h3>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1">è©å‘é‡ä»‹ç´¹</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">è‡ªç„¶èªè¨€è™•ç† â€“ Vector Space of Semantics</a>
<br />
<a href="https://zhuanlan.zhihu.com/p/26306795">[NLP] ç§’æ‡‚è¯å‘é‡Word2vecçš„æœ¬è´¨</a>
<br />
<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸ - SVD</a>
<br />
<a href="http://www.shuang0420.com/2017/03/21/NLP%20ç¬”è®°%20-%20å†è°ˆè¯å‘é‡/">NLP ç¬”è®° - å†è°ˆè¯å‘é‡</a>
<br />
<a href="https://www.twblogs.net/a/5c7bdd70bd9eee339918b4da">æ©Ÿå™¨å­¸ç¿’ç­†è¨˜ä¹‹äºŒåäºŒâ€”â€”PCAèˆ‡SVD</a>
<br />
<a href="https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/">ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">è‡ªç„¶èªè¨€è™•ç† â€“ Pointwise Mutual Information</a>
<br />
<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing</a>
<br />
<a href="https://kknews.cc/tech/83yx3qn.html">æ©Ÿå™¨å­¸ç¿’äº”åˆ†é˜ï¼šè‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰çš„N-gramæ¨¡å‹æ˜¯ä»€éº¼ï¼Ÿ</a></p>
 -->
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//text%20mining/2019/11/26/Text-Mining-Word-embeddings/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/26/Text-Mining-Word-embeddings/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//text%20mining/2019/11/26/Text-Mining-Word-embeddings/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/26/Text-Mining-Word-embeddings/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/">
        [Text Mining]Text clustering and topic modelling
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 21, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 21, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>


    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<p>åœ¨é€²å…¥ Text clustering å‰æˆ‘æƒ³è¦å…ˆä»‹ç´¹ <strong>Clustering</strong>ã€‚</p>

<h2 id="clustring">Clustring</h2>

<ul>
  <li>
    <p>Clustering (åˆ†ç¾¤ï¼‰æ˜¯ä¸€ç¨® unsupervised learningï¼ˆéç›£ç£å­¸ç¿’ï¼‰</p>
  </li>
  <li>Typical applications
    <ol>
      <li>As a stand-alone tool to get insight into data distribution</li>
      <li>As a preprocessing step for other algorithms</li>
    </ol>
  </li>
  <li>Cluster: a collection of data objectsã€‚ä½†ä»€éº¼æ¨£çš„ data æœƒè¢«æ­¸åœ¨åŒä¸€å€‹ cluster å‘¢ï¼Ÿ é€™å°±æ˜¯ clustering æœ€é‡è¦çš„æ¦‚å¿µï¼Œ<strong>Similar</strong> å’Œ <strong>Dissimilar</strong>ã€‚</li>
  <li><strong>Similar</strong> to one another within the same cluster</li>
  <li><strong>Dissimilar</strong> to the objects in other clusters</li>
</ul>

<p>=&gt; è€Œæåˆ° Similar å’Œ Dissimilar å°±è¦å¿…é ˆè¦æåˆ° distance (or similarity) measuresã€‚æœ‰äº† distanceï¼ˆsimilarityï¼‰æ‰æœ‰è¾¦æ³•å®šç¾© Similar å’Œ Dissimilarã€‚</p>

<h3 id="distancesimilarity">Distanceï¼ˆSimilarityï¼‰</h3>

<ul>
  <li>Distances are normally used to measure the similarity or dissimilarity between two data objects</li>
</ul>

<p>Distances å¯ä»¥æœ‰å¾ˆå¤šç¨®å®šç¾©æ–¹å¼ï¼Œä½†ä¸ç®¡æ€éº¼å®šç¾©ï¼Œéƒ½ä¸€å®šè¦ç¬¦åˆä»¥ä¸‹çš„ propertiesã€‚</p>
<ul>
  <li><script type="math/tex">d(i, j) \ge 0</script> (non-negativity)</li>
  <li><script type="math/tex">d(i, i) = 0</script> (identity of indiscernibles)</li>
  <li><script type="math/tex">d(i, j) = d(j, i)</script> (symmetry)</li>
  <li><script type="math/tex">d(i, j) \le d(i, k) + d(k, j)</script> (triangle inequality)</li>
</ul>

<p><strong>Minkowski distance</strong></p>

<p><script type="math/tex">d(i, j) = \sqrt[q]{(|x_{i_1} - x_{j_1}|^q + |x_{i_2} - x_{j2}|^q + \ldots + |x_{i_p} - x_{j_p}|^q)}</script>
, q is a positive integer</p>

<ul>
  <li>If q = 1, d is Manhattan distance</li>
  <li>If q = 2, d is Euclidean distance</li>
</ul>

<p><strong>Binary Variables</strong></p>

<ul>
  <li>symmetric binary variables: both states are equally important; 0/1</li>
  <li>asymmetric binary variables: one state is more important than the other (e.g. outcome of disease test); 1 is the important state, 0 the other</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Contingency%20tables%20for%20Binary%20Variables.png" alt="PMF_two coins.png" /></p>

<ul>
  <li>Accuracy for symmetric binary variables</li>
</ul>

<script type="math/tex; mode=display">sim(i, j) = \frac{a+d}{a+b+c+d}</script>

<ul>
  <li>Jaccard similarity for asymmetric binary variables</li>
</ul>

<script type="math/tex; mode=display">sim(i, j) = \frac{a}{a+b+c}</script>

<hr />

<h2 id="text-clustering">Text clustering</h2>

<p><img src="http://localhost:4000/blog/img/posts/Conceptual%20framework%20for%20text%20mining.png" alt="PMF_two coins.png" /></p>

<p>ä¸Šåœ–æ˜¯ Conceptual framework for text miningï¼Œè€Œé€™ç¯‡è¦ä»‹ç´¹çš„æ˜¯ Clustering å’Œ Topic Analysisã€‚</p>

<ul>
  <li>
    <p><strong>Text clustering</strong> is the task of grouping similar texts together. What is considered â€˜similarâ€™ depends on the application.</p>
  </li>
  <li>
    <p>Clustering is a central tool in exploratory data analysis, where it can help us to get insights into the distribution of a data set.</p>
  </li>
  <li>
    <p>Clustering is also useful as a pre-processing technique in knowledge-focused applications. Example: Brown clustering</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/The%20standard%20text%20clustering%20pipeline.png" alt="PMF_two coins.png" /></p>

<hr />

<h2 id="similarity-measures">Similarity measures</h2>

<p>å‰›å‰›å‰é¢æåˆ°çš„ Similarityï¼Œå¦‚ä½•ç”¨åœ¨ text ä¸Šå‘¢ï¼Ÿ</p>

<p><strong>Accuracy for symmetric binary vectors</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Accuracy%20for%20symmetric%20binary%20vectors.png" alt="PMF_two coins.png" /></p>

<p><strong>Jaccard similarity for asymmetric binary vectors</strong></p>

<hr />

<h2 id="hard-clustering-and-soft-clustering">Hard clustering and soft clustering</h2>

<p><strong>Hard clustering</strong></p>
<ul>
  <li>Each document either belongs to a cluster or not. Ex. Hierarchical clustering(brown clustring), Partitioning clustering(k-means), Density-Based clustering(DBSCAN)</li>
</ul>

<p><strong>Soft clustering</strong></p>
<ul>
  <li>Each document belongs to each cluster to a certain degree. Ex. LDA (topic model)</li>
</ul>

<hr />

<h2 id="an-overview-of-hard-clustering-methods">An overview of hard clustering methods</h2>

<h3 id="hierarchical-clustering">Hierarchical clustering</h3>

<p>This method does not require the number of clusters k as an input, but needs a termination condition</p>

<p><img src="http://localhost:4000/blog/img/posts/Hierarchical%20Clustering.png" alt="" /></p>

<ul>
  <li>
    <p><strong>Agglomerative</strong>: Each document starts in its own cluster. Hierarchy is created by merging pairs of clusters.
å°‡é»å€†å€†åˆä½µï¼Œæœ€å¾Œæ‰€æœ‰çš„é»æœƒå…¨éƒ¨åœ¨åŒä¸€å€‹ cluster è£¡ã€‚é‚£è‡³æ–¼è¦ç”¨ä»€éº¼æ±ºå®šè¦å°‡å…©å€‹é»åˆä½µå‘¢ï¼Ÿé€™æ™‚å€™åˆæœ‰ Linkage criteria ä¾†æ±ºå®šï¼Œeg. Single-link, Complete-link, Average-link ç­‰ç­‰ã€‚ä¸åŒçš„ linkage criteria æœƒå°è‡´ä¸åŒçš„åˆ†ç¾¤çµæœï¼Œå¦‚ä¸‹åœ–ã€‚</p>
  </li>
  <li>
    <p><strong>Divisive clustering</strong>: All documents start in one cluster. Hierarchy is created by splitting clusters recursively.</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Linkage%20criteria.png" alt="" /></p>

<ul>
  <li><strong>Brown clustring</strong></li>
</ul>

<h3 id="partitioning-clustering">Partitioning clustering</h3>
<ul>
  <li>
    <p><strong>K-means</strong></p>
  </li>
  <li>
    <p>Issues with the k-means algorithm</p>
    <ol>
      <li>The k-means algorithm always converges, but there is no guarantee that it finds a global optimum. (Solution: random restarts)</li>
      <li>The number of clusters needs to be specified in advance, or chosen based on heuristics and cross-validation. (Example: elbow method)</li>
      <li>The k-means algorithm is not good at handling outliers â€“ every document will eventually belong to some cluster.</li>
      <li>K-means is restricted to clusters with convex shapes =&gt; Density-Based clustering</li>
    </ol>
  </li>
</ul>

<h3 id="density-based-clustering">Density-Based clustering</h3>

<ul>
  <li>The basic idea behind density-based algorithms is that different regions of the vector space can be more or less densely populated.</li>
  <li>Under this view, clusters can take any shape; they are not constrained to convex clusters as in k-means.</li>
</ul>

<p><strong>Directly density-reachable</strong></p>

<ul>
  <li>DBSCAN</li>
</ul>

<hr />

<h2 id="evaluation-of-hard-clustering">Evaluation of hard clustering</h2>

<h3 id="intrinsic-and-extrinsic-evaluation">Intrinsic and extrinsic evaluation</h3>
<ul>
  <li>In <strong>intrinsic</strong> evaluation, a clustering is evaluated based on internal measures such as coherence and separation. <em>Are documents in the same cluster similar? Are clusters well-separated?</em></li>
  <li>In <strong>extrinsic</strong> evaluation, a clustering is evaluated based on data that was not used for the clustering, such as known class labels. <em>cluster purity, Rand index</em></li>
</ul>

<h3 id="rand-index">Rand index</h3>

<p>å‡è¨­ä¸€å€‹é›†åˆä¸­æœ‰Nç¯‡æ–‡ç« 
ä¸€å€‹é›†åˆä¸­æœ‰N(N-1)/2å€‹é›†åˆå°
TPï¼šåŒä¸€é¡çš„æ–‡ç« è¢«åˆ†åˆ°åŒä¸€å€‹ç°‡
TNï¼šä¸åŒé¡çš„æ–‡ç« è¢«åˆ†åˆ°ä¸åŒç°‡
FPï¼šä¸åŒé¡çš„æ–‡ç« è¢«åˆ†åˆ°åŒä¸€å€‹ç°‡
FNï¼šåŒä¸€é¡çš„æ–‡ç« è¢«åˆ†åˆ°ä¸åŒç°‡
Rand Indexåº¦é‡çš„æ­£ç¢ºçš„ç™¾åˆ†æ¯”
RI = ï¼ˆTP+TNï¼‰/ï¼ˆTP+FP+FN+TNï¼‰</p>

<hr />

<h2 id="topic-models">Topic models</h2>
<ul>
  <li>A topic model is a statistical model for representing the abstract topics that are expressed in a collection of documents.</li>
  <li>Topic models are examples of soft clustering techniques â€“ each document belongs to each cluster (topic) to a certain degree.</li>
</ul>

<h3 id="latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</h3>

<p>LDAæœ‰å…©å€‹åŸå‰‡ï¼Œ</p>
<ol>
  <li>æ¯å€‹ domcuments æ˜¯ç”±å¤šå€‹ Topic çµ„æˆï¼ˆeach document belongs to each cluster (topic) to a certain degreeï¼‰</li>
  <li>æ¯å€‹ä¸»é¡Œæœƒæœ‰ä¸åŒçš„ terms ä¾†æè¿°ï¼Œä¸”åŒæ¨£å°è©å¯ä»¥åŒæ™‚å‡ºç¾åœ¨ä¸åŒçš„ä¸»é¡Œã€‚</li>
</ol>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models.png" alt="" /></p>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models2.png" alt="" /></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://www.ida.liu.se/~732A75/info/courseinfo.en.shtml">732A75 Advanced Data Mining</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/238079-natural-language-processing-brown-clustering">è‡ªç„¶èªè¨€è™•ç† â€“ Brown Clustering</a>
<br />
<a href="https://medium.com/@tengyuanchang/ç›´è§€ç†è§£-lda-latent-dirichlet-allocation-èˆ‡æ–‡ä»¶ä¸»é¡Œæ¨¡å‹-ab4f26c27184">ç›´è§€ç†è§£ LDA (Latent Dirichlet Allocation) èˆ‡æ–‡ä»¶ä¸»é¡Œæ¨¡å‹</a>
<br />
<a href="https://www.twblogs.net/a/5b8c83e12b717718833363ae">èšé¡è©•åƒ¹æŒ‡æ¨™ Rand Index,RI,Recall,Precision,F1</a></p>
 -->
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/21/Text-Mining-Text-clustering-and-topic-modelling/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/text%20mining/2019/11/13/Text-Mining-Text-classification/">
        [Text Mining]Text classification
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 13, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 13, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>


    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="text-classification">Text classification</h2>

<ul>
  <li>Text classification is the task of categorising text documents into predefined classes.</li>
</ul>

<h2 id="evaluation-of-text-classifiers">Evaluation of text classifiers</h2>

<p>æœ€ç°¡å–®çš„æª¢è¦–é æ¸¬çµæœå¥½å£çš„æ–¹æ³•å°±æ˜¯å°‡ predict å‡ºä¾†çš„é¡åˆ¥èˆ‡çœŸå¯¦çš„é¡åˆ¥åšæ¯”è¼ƒã€‚ï¼ˆé æ¸¬å¿…é ˆè¦åœ¨ test dataï¼Œæ›å¥è©±èªªä¹‹å‰ä¸¦æ²’æœ‰åƒèˆ‡ä»»ä½• training çš„éç¨‹ã€‚é€™é»åœ¨åšæ‰€æœ‰ Machine Learning çš„æ–¹æ³•éƒ½å¾ˆé‡è¦ï¼Œåœ¨åš model æ¸¬è©¦å‰ä¸è¦ç¢° test dataã€‚ï¼‰</p>

<h3 id="accuracy">Accuracy</h3>
<p>The accuracy of a classifier is the proportion of documents for which the classifier predicts the gold-standard class:</p>

<script type="math/tex; mode=display">\textrm{accuarcy} = \frac{\textrm{number of correctly classified documents}}{\textrm{number of all documents}}</script>

<h3 id="accuracy-and-imbalanced-data-sets">Accuracy and imbalanced data sets</h3>

<p>ä¸Šé¢çš„ accuracy çœ‹èµ·ä¾†éå¸¸åˆç†å•Šï¼Œå»è¨ˆç®—åˆ†é¡æ­£ç¢ºçš„æ¯”ä¾‹ä¾†åˆ¤æ–·é€™å€‹åˆ†é¡å™¨æ˜¯å¦é æ¸¬æº–ç¢ºã€‚ä½†ï¼Œå¦‚æœå…¶å¯¦è³‡æ–™æœ¬èº«çš„é¡åˆ¥ä¸¦ä¸å¹³å‡å‘¢ï¼Ÿ</p>

<p><img src="http://localhost:4000/blog/img/posts/imbalanced%20data%20sets.png" alt="PMF_two coins.png" /></p>

<p>æ ¹æ“šä¸Šåœ–ï¼Œæˆ‘å€‘åªè¦æŠŠæ‰€æœ‰è³‡æ–™éƒ½çŒœç¶ è‰²çš„ classï¼Œé€™æ¨£ accuracy å°±èƒ½æœ‰ 80%ã€‚å¾é€™å€‹ä¾‹å­å¯ä»¥çŸ¥é“ï¼Œ</p>

<ul>
  <li>
    <p>Evaluation measures are no absolute measures of performance. å¦‚æœä»Šå¤©å¾—åˆ° accuracy æ˜¯ 80% æˆ‘å€‘ä¸¦ç„¡æ³•ç¢ºå®šé€™æ¨£çš„æº–ç¢ºç‡çš„å¥½å£ï¼Œè¦æ ¹æ“šæ¯å€‹å•é¡Œå»åˆ¤æ–·ã€‚</p>
  </li>
  <li>
    <p>Instead, we should ask for a classifierâ€™s performance relative to other classifiers, or other points of comparison. E.g.â€™Logistic Regression has a higher accuracy than Naive Bayes.â€™</p>
  </li>
  <li>
    <p>When other classifiers are not available, a simple baseline is to always predict the most frequent class in the training data.</p>
  </li>
</ul>

<h2 id="precision-and-recall">Precision and recall</h2>

<ul>
  <li>
    <p>Precision and recall â€˜zoom inâ€™ on how good a system is at identifying documents of a specific class.</p>
  </li>
  <li>
    <p>Precision is the proportion of correctly classified documents among all documents for which the system predicts class.</p>
  </li>
</ul>

<script type="math/tex; mode=display">\textrm{precision} = \frac{\textrm{# true positives}}{\textrm{# true positives + # false positives}}</script>

<ul>
  <li>Recall is the proportion of correctly classified documents among all documents with gold-standard class.</li>
</ul>

<script type="math/tex; mode=display">\textrm{recall} = \frac{\textrm{# true positives}}{\textrm{# true positives + # false negatives}}</script>

<h2 id="f1-measure">F1-measure</h2>
<p>A good classifier should balance between precision and recall. â€¨</p>

<script type="math/tex; mode=display">\textrm{F1} = \frac{2 \cdot \textrm{precision} \cdot \textrm{recall}}{\textrm{precision + recall}}</script>

<hr />

<h2 id="naive-bayes-classifier">Naive Bayes classifier</h2>

<h3 id="bayes-theorem">Bayesâ€™ theorem</h3>

<p>We know that <script type="math/tex">C</script> is classes, <script type="math/tex">x_i, i = 1, \cdots,n</script> is features. Using Bayesâ€™ theorem, the conditional probability can be decomposed as</p>

<script type="math/tex; mode=display">p(C|x_1,..., x_n) = \frac{p(C)~p(x_1, \cdots,x_n|C)}{p(x_1, \cdots,x_n)}</script>

<p>ä¹Ÿå°±æ˜¯ï¼Œ</p>

<script type="math/tex; mode=display">\textrm{posterior} = \frac{\textrm{prior} \times \textrm{likelihood}}{\textrm{evidence}}</script>

<p>æ ¹æ“šä¸Šå¼ï¼Œæˆ‘å€‘å¯ä»¥å°‡åˆ†æ¯è¦–ç‚ºå¸¸æ•¸ï¼Œå› ç‚º features <script type="math/tex">x_i, i = 1, \cdots,n</script> çš„å€¼æ˜¯çµ¦å®šçš„ï¼Œä¸”èˆ‡ <script type="math/tex">C</script> ç„¡é—œï¼Œæ‰€ä»¥å¯ä»¥å¾—åˆ°</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(C, x_1, \cdots,x_n) & = p(C)~p(x_1,\cdots,x_n|C) \\
& \propto p(C)~p(x_1|C)~p(x_2,\cdots,x_n|C,x_1) \\
& \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3,\cdots,x_n|C,x_1,x_2) \\ 
& \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3|C, x_1, x_2)~p(x_4,\cdots,x_n|C,x_1,x_2,x_3) \\
& \propto \cdots\\
& \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3|C, x_1, x_2) \cdots p(x_n|C,x_1,x_2,\cdots ,x_n) \\
\end{align} %]]></script>

<h3 id="naive-bayes-assumption">Naive Bayes assumption</h3>

<p>Naive Bayes å‡è¨­ 
<script type="math/tex">p(x_i|C, x_j) = p(x_i|C), \textrm{for} ~ i \ne j</script></p>

<p>æ ¹æ“š Naive Bayes çš„å‡è¨­ï¼Œå‰é¢çš„å¼å­æˆ‘å€‘å¯ä»¥å¯«æˆï¼Œ</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(C|x_1,..., x_n) & \propto p(C, x_1, \cdots,x_n) \\
& \propto p(C)~p(x_1|C)~p(x_2|C)~p(x_3|C) \cdots p(x_n|C) \\
& \propto p(C)~\prod_{i=1}^n p(x_i|C)
\end{align} %]]></script>

<p>æ ¹æ“šä¸Šé¢çš„æ¨å°éç¨‹ï¼Œæˆ‘å€‘å¯ä»¥å¾—åˆ°ï¼Œ</p>

<script type="math/tex; mode=display">p(C|x_1,..., x_n) = \frac{1}{Z}~p(C)~\prod_{i=1}^n p(x_i|C)</script>

<h3 id="naive-bayes-classifer">Naive Bayes classifer</h3>

<p>è€Œ Naive Bayes classifer å°±æ˜¯å–å„å€‹åˆ†é¡ <script type="math/tex">C_m, m = 1, \cdots, k</script> 
ä¸­ <script type="math/tex">p(C|x_1,..., x_n)</script> å€¼æœ€å¤§çš„ç‚ºæœ€å¾Œçš„åˆ†é¡çµæœã€‚æ›å¥è©±èªªï¼Œæˆ‘å€‘å¯ç”¨é€™æ¨£çš„å¼å­è¡¨ç¤º</p>

<script type="math/tex; mode=display">C_m = \mathop{\arg\max}_C p(C|x_1,..., x_n) =  \mathop{\arg\max}_C p(C)~\prod_{i=1}^n p(x_i|C)</script>

<p>è€Œ <script type="math/tex">C_m</script> å°±æ˜¯æœ€å¾Œçš„åˆ†é¡çµæœã€‚</p>

<h3 id="two-classic-naive-bayes-variants-for-text">Two Classic Naive Bayes Variants for Text</h3>

<ol>
  <li>Multinomial Naive Bayes
    <ul>
      <li>Data follows a multinomial distribution (å¤šé …åˆ†å¸ƒ)</li>
      <li>Each feature values is a count (word occurrence counts, TF-IDF weighting, â€¦)</li>
    </ul>
  </li>
  <li>Bernoulli Naive Bayes
    <ul>
      <li>Data follows a multivariate Bernoulli distribution</li>
      <li>Each feature is binary (word is present / absent)</li>
    </ul>
  </li>
</ol>

<hr />

<p><strong>Lab:</strong> <a href="https://github.com/shihs/732A92-TextMining/blob/master/Lab1/TM-L2.ipynb">Text classification Lab</a></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://www.ycc.idv.tw/confusion-matrix.html">å¦‚ä½•è¾¨åˆ¥æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„å¥½å£ï¼Ÿç§’æ‡‚Confusion Matrix</a>
<br />
<a href="https://zh.wikipedia.org/wiki/æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨">wikipeida - å–®ç´”è²æ°åˆ†é¡å™¨</a></p>

 -->
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//text%20mining/2019/11/13/Text-Mining-Text-classification/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/13/Text-Mining-Text-classification/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//text%20mining/2019/11/13/Text-Mining-Text-classification/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/13/Text-Mining-Text-classification/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/text%20mining/2019/11/05/Text-Mining-Information-Retrieval/">
        [Text Mining]Information Retrieval
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 5, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 5, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Text Mining">Text Mining</a>
        
      
    </span>


    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<p>é€™é–€èª²æ•™æˆçš„æ˜¯è‹±æ–‡çš„æ–‡å­—æ¢å‹˜ï¼Œä¸­æ–‡èˆ‡è‹±æ–‡æœ¬èº«åœ¨çµæ§‹ä¸Šæœ‰éå¸¸å¤§çš„å·®ç•°ï¼Œæ‰€ä»¥æœ‰äº›è§£æå’Œæ–·è©çš„æ–¹æ³•ä¸­æ–‡å¯èƒ½ä¸¦ä¸é©ç”¨ï¼Œä½†åœ¨åŸºæœ¬æ¦‚å¿µä¸Šé‚„æ˜¯æœ‰ç›¸åŒçš„åœ°æ–¹ã€‚</p>

<p>æ¯å€‹ä¸»é¡Œéƒ½æœ‰ä¸€å€‹ Lab é™„åœ¨æ–‡ç« æœ€å¾Œã€‚</p>

<hr />

<h2 id="information-retrieval-ir">Information Retrieval (IR)</h2>

<p>Information Retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).</p>

<p>ä¸­æ–‡ç¨±ç‚ºã€Œè³‡è¨Šæª¢ç´¢ã€ã€‚å¾ä¸€å †çš„ unstructured è³‡æ–™ä¸­ï¼ˆé€šå¸¸æ˜¯æ–‡å­—ï¼‰æ‰¾å‡ºç¬¦åˆæˆ‘å€‘éœ€è¦çš„æ¢ä»¶çš„è³‡æ–™ã€‚</p>

<p>ä¾‹å¦‚ï¼Œgoogle æœå°‹å°±æ˜¯ä¸€å€‹ä¾‹å­ã€‚æˆ‘å€‘åœ¨æœå°‹ bar è£¡è¼¸å…¥æˆ‘å€‘æƒ³è¦æ‰¾åˆ°çš„é—œéµå­—ï¼Œgoogle æœå°‹å¼•æ“æœƒå¾å®ƒå„²å­˜çš„æ‰€æœ‰ databaseï¼ˆç¶²ç«™å€‘ï¼‰ ä¸­æ‰¾å‡ºç¬¦åˆæˆ‘å€‘é—œéµå­—æ¢ä»¶çš„ç¶²ç«™çµ¦æˆ‘å€‘ã€‚</p>

<hr />

<p>é‚£æˆ‘å€‘è¦å¦‚ä½•æ‰¾åˆ°åŒ…å«é€™äº›é—œéµå­—çš„æ–‡ç« å‘¢ï¼Ÿ</p>

<p>æœ€ç›´è§€åœ°æƒ³æ‡‰è©²å°±æ˜¯æŠŠæœ‰æˆ‘å€‘æœå°‹çš„é—œéµå­—çš„æ–‡ç« æŠ“å‡ºä¾†å§ï¼é€™å°±æ˜¯ Boolean retrievalã€‚</p>

<p>åœ¨é€²å…¥ Boolean retrieval ä¹‹å‰å…ˆä¾†çœ‹ä¸€ä¸‹ The classic search modelã€‚</p>

<h3 id="the-classic-search-model">The classic search model</h3>

<ul>
  <li>To communicate her/his information need to an IR system, the user formulates a <em>search query</em>.</li>
  <li>The objective of the IR system is to find documents in the collection that â€˜matchâ€™ the search query.</li>
  <li>A good IR system finds documents that are also relevant for the userâ€™s information need.</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/The%20classic%20search%20model.png" alt="PMF_two coins.png" /></p>

<hr />

<h3 id="boolean-retrieval">Boolean retrieval</h3>

<p>The Boolean retrieval model is a model for information retrieval in which we can pose any query which is in the form of a Boolean expression of terms, that is, in which terms are combined with the operators AND, OR, and NOT. The model views each document as just a set of words.</p>

<p>å¾ˆç›´è¦ºçš„æ–¹æ³•è¦æ‰¾å‡ºåŒ…å«é—œéµå­—çš„å¯ä»¥ä½¿ç”¨ terms(é€šå¸¸æ˜¯å€‹å–®è©) æ˜¯å¦åŒ…å«åœ¨ documents è£¡ã€‚</p>

<p>ä¾‹å¦‚ï¼Œç¾åœ¨æœ‰ Sherlock Holmes å°èªªæ–‡å­—ã€‚æˆ‘æƒ³è¦æ‰¾å‡ºå“ªäº›ç¯‡ç« ç¬¦åˆå‡ºç¾ â€˜Moriartyâ€™ å’Œ â€˜Lestradeâ€™ ä½†ä¸åŒ…å« â€˜Adairâ€™ã€‚é‚£éº¼ä¸‹çš„ query å°±æœƒæ˜¯ <em>Moriarty AND Lestrade AND NOT Adair</em></p>

<p>é‚£å€‹æˆ‘å€‘å¯ä»¥å°‡ä¸Šé¢çš„æ¦‚å¿µå¯«æˆä¸€å€‹ <strong>Termâ€“document matrix</strong>ã€‚</p>

<p><strong>Termâ€“document matrix</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Term%E2%80%93document%20matrix.png" alt="PMF_two coins.png" /></p>

<p>æ¯ä¸€è¡Œä»£è¡¨ä¸€å€‹ç« ç¯€ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€å€‹æ¢ä»¶çš„å–®è©ï¼ˆtermï¼‰ã€‚ä»¥åˆ—ä¾†çœ‹ï¼Œ0 è¡¨ç¤ºè©²å–®è©æ²’æœ‰å‡ºç¾åœ¨é‚£ä¸€ç« ç¯€ä¸­ï¼Œ1 è¡¨ç¤ºè©²å–®è©æœ‰å‡ºç¾ã€‚
<br />
åƒæ˜¯ç¬¬ä¸€åˆ—ï¼Œâ€™Adairâ€™ åªæœ‰å‡ºç¾åœ¨ â€˜Empty houseâ€™ é€™å€‹ç« ç¯€ã€‚</p>

<p>é€™æ¨£çš„æ–¹æ³•çœ‹ä¼¼ç°¡å–®ï¼Œä½†ä»¥å¯¦å‹™ä¸Šä¾†èªªä¸¦ä¸æ˜¯å€‹å¥½è¾¦æ³•ã€‚å› ç‚ºï¼Œ</p>

<ul>
  <li>Termâ€“document matrices are sparse. å¯ä»¥å¾ä¸Šé¢çš„ matrix çœ‹åˆ°ï¼Œæœ‰å¾ˆå¤šçš„ 0 ï¼Œä¹Ÿå°±æ˜¯èªªå…¶å¯¦å¾ˆå¤šè³‡è¨Šä¸¦ä¸éœ€è¦ï¼Œä½†å»é‚„æ˜¯éœ€è¦å„²å­˜ã€‚</li>
  <li>å‡å¦‚ç¾åœ¨æœ‰ 1,000,000 ä»½ documentsï¼Œæœ‰ 500,000 ä¸åŒçš„ terms ï¼Œé€™æ¨£çš„æ¢ä»¶ä¸‹æœƒç”¢ç”Ÿ a matrix with 500,000,000,000 entries (62,5 GB)ã€‚</li>
</ul>

<p>æ‰€ä»¥æˆ‘å€‘å¿…é ˆå¾—ä½¿ç”¨åˆ¥çš„è¾¦æ³•æ¸›å°‘ä¸å¿…è¦çš„è³‡è¨Šå„²å­˜ã€‚é€™æ™‚å€™å°±æœ‰äº† Inverted indexã€‚</p>

<p><strong>Inverted index</strong></p>

<p>The inverted index is a keyâ€“value mapping, the basic idea is shown below.</p>

<ul>
  <li>the keys are search terms</li>
  <li>the values are sorted lists of document identifiers (ids)</li>
  <li>the list for terms identifies those documents that contain the terms</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Inverted%20index.png" alt="PMF_two coins.png" /></p>

<p>å¾ä¸Šåœ–æˆ‘å€‘å¯ä»¥çœ‹åˆ°ï¼Œå’Œ Termâ€“document matrix ä¸ä¸€æ¨£çš„æ˜¯ Inverted index åªå„²å­˜äº†æœ‰åŒ…å«è©²å–®è©çš„ document idsã€‚</p>

<hr />

<p>ç¾åœ¨æˆ‘å€‘çŸ¥é“æˆ‘å€‘è¦çš„æ˜¯å»çœ‹é‚£äº›é—œéµå­—æ˜¯å¦åŒ…å«åœ¨ documents ä¸­ï¼Œä½†æˆ‘å€‘ä¸¦ä¸æœƒæ¯ä¸€æ¬¡æ‰¾é—œéµå­—çš„æ™‚å€™ä¸€ç¯‡ä¸€ç¯‡æ–‡ç« æœç´¢ï¼Œæˆ‘å€‘æœƒå…ˆå»ºç«‹ä¸€å€‹å„²å­˜å¥½æ‰€æœ‰ term çš„ matrixï¼Œ
ä½†æˆ‘å€‘è¦æ€éº¼æ±ºå®šæ˜¯å“ªäº› terms è¦è¢«å„²å­˜å‘¢ï¼Ÿ</p>

<h2 id="index-construction">Index construction</h2>

<p>The major steps in index construction:</p>

<ol>
  <li>Collect the documents to be indexed.</li>
  <li>Tokenize the text.</li>
  <li>Do linguistic preprocessing of tokens.</li>
  <li>Index the documents that each term occurs in.</li>
</ol>

<p><img src="http://localhost:4000/blog/img/posts/building%20an%20index%20by%20sorting%20and%20grouping.png" alt="PMF_two coins.png" /></p>

<h3 id="tokenization">Tokenization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raw</span> <span class="o">=</span> <span class="s">"Apple is looking at buying U.K. startup for $1 billion."</span>
<span class="c"># tokenize raw text based on whitespace</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">raw</span><span class="o">.</span><span class="n">split</span><span class="p">():</span> 
	<span class="k">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>


<span class="c"># tokenize using spaCy</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">"en_core_web_sm"</span><span class="p">)</span> 
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">nlp</span><span class="p">(</span><span class="n">raw</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="c"># çµæœï¼š</span>
<span class="c"># Apple</span>
<span class="c"># is</span>
<span class="c"># looking</span>
<span class="c"># at</span>
<span class="c"># buying</span>
<span class="c"># U.K.</span>
<span class="c"># startup</span>
<span class="c"># for</span>
<span class="c"># $</span>
<span class="c"># 1</span>
<span class="c"># billion</span>
<span class="c"># . </span>
</code></pre></div></div>

<p>è‹±æ–‡æ¯å€‹å–®å­—å¯ä»¥å¾ˆå®¹æ˜“åœ°ä½¿ç”¨ç©ºç™½ï¼ˆ.split()ï¼‰ä¾†åˆ†å‰²å‡ºæ¯å€‹å–®å­—ï¼Œä½†åªä½¿ç”¨
split ç„¡æ³•æ’é™¤æ¨™é»ç¬¦è™Ÿçš„å•é¡Œï¼Œpython çš„ spacy å¯ä»¥å°‡æ¨™é»ç¬¦è™Ÿä¹Ÿå€åˆ†å‡ºä¾†ã€‚</p>

<h3 id="stop-words">Stop words</h3>
<p>A stop word is a word that is frequent but does not contribute much value for the application in question. For example: a, the, andâ€¦</p>

<p>ä½†æ ¹æ“šåˆ†æçš„ç›®çš„ä¸åŒï¼Œæœƒæœ‰ä¸åŒçš„ stop words çš„å®šç¾©ã€‚ä»¥åˆ†æ Sherlock Holmes çš„å°èªªç‚ºä¾‹ï¼Œè³‡æ–™è£¡é¢è‚¯å®šæœƒåŒ…å«éå¸¸å¤§é‡çš„ Sherlock Holmesï¼Œé‚£é€™ç¨®æ™‚å€™æˆ‘å€‘åˆ°åº•æˆ–è¨±å¯ä»¥æ ¹æ“šåˆ†æçš„ç›®çš„ Sherlock Holmes è€ƒæ…®å°‡ Sherlock Holmes å®šç¾©ç‚º stop wordsã€‚æ›å¥è©±èªªï¼Œä¸¦ä¸å­˜åœ¨ä¸€å€‹æ‰€æœ‰åˆ†æé€šç”¨çš„ stop words è³‡æ–™é›†ã€‚</p>

<p>æ—¢ç„¶ stop words å°æ–¼åˆ†ææ²’æœ‰åƒ¹å€¼è²¢ç»ï¼Œé‚£éº¼åœ¨åšåˆ†æå‰å°±å¿…é ˆæŠŠ stop words åˆªé™¤ï¼Œä»¥é¿å…å¹²æ“¾åˆ†æçµæœã€‚</p>

<h3 id="lexemes-and-lemmas">Lexemes and lemmas</h3>

<p>è‹±æ–‡çš„æ–‡æ³•æœ‰å¹¾å€‹è¦å‰‡ï¼Œ</p>
<ol>
  <li>å‹•è©æœƒæ ¹æ“šäººç¨±èˆ‡æ™‚æ…‹æœ‰æ‰€è®ŠåŒ–</li>
  <li>åè©çš„å–®æ•¸èˆ‡è¤‡æ•¸æœ‰è®ŠåŒ–</li>
</ol>

<p>ä½†åœ¨åˆ†æå‰å¿…é ˆè¦å°‡é€™äº›ç›¸åŒæ„ç¾©ä½†ä¸åŒå‹çš„å–®å­—ä¿®æ­£ç‚ºç›¸åŒä»¥ä¾¿åˆ†æã€‚</p>

<ul>
  <li>
    <p>The term <em>lexeme</em> refers to a set of word forms that all share the same fundamental meaning. For example: word forms run, runs, ran, running â€“ lexeme run</p>
  </li>
  <li>
    <p>The term <em>lemma</em> refers to the particular word form that is
chosen, by convention, to represent a given lexeme. For example: what you would put into a lexicon</p>
  </li>
</ul>

<hr />

<h2 id="ranked-retrieval">Ranked retrieval</h2>

<p>é€²è¡Œåˆ°é€™è£¡æˆ‘å€‘å·²ç¶“çŸ¥é“å¦‚ä½•è¦å…ˆå»ºç«‹ä¸€å€‹æœ‰æ‰€æœ‰ term çš„ matrix for æ‰€æœ‰çš„ documentsï¼Œç„¶å¾Œä½¿ç”¨ boolean retrieval æ‰¾å‡ºæˆ‘å€‘è¦çš„ documentsï¼Œä½†é€™æ¨£ä¸¦æ²’æœ‰è€ƒæ…®åˆ°å–®è©çš„é‡è¦æ€§ã€‚æ›å¥è©±èªªï¼Œä¸€å€‹å¾ˆå¸¸å‡ºç¾çš„å–®è©æœ‰å¯èƒ½ä¸¦æ²’æœ‰æ¯”ä¸€å€‹å‡ºç¾æ¬¡æ•¸è¼ƒå°‘ä½†å»åœ¨æŸ document å‡ºç¾é‚„è¦é‡è¦ï¼Œæˆ‘å€‘å¯èƒ½æ›´æƒ³è¦æŠ“å‡ºå«æœ‰é‚£å€‹å–®è©çš„æ–‡ç« ã€‚æ‰€ä»¥æˆ‘å€‘å¿…é ˆè¦ rank é‚£äº›æŠ“å‡ºä¾†çš„ termsã€‚</p>

<p><strong>Problems with Boolean retrieval</strong></p>

<ul>
  <li>Not many users are capable of writing high-quality Boolean queries, and many find the process too time-consuming.</li>
  <li>Feast or famine: Boolean queries tend to return either too many results, or no results at all.</li>
  <li>Intuitively, whether or not a document â€˜matchesâ€™ a search query is not a Boolean property, but is gradual in nature.</li>
</ul>

<p><strong>Ranked retrieval</strong></p>
<ul>
  <li>
    <p>A ranked retrieval system assigns scores to documents based on how well they match a given search query. (There are many possible ways of scoring.)</p>
  </li>
  <li>
    <p>Based on the score, a ranked retrieval system can return a list of the top documents in the collection with respect to the query.</p>
  </li>
</ul>

<p>å‰é¢æåˆ°çš„ boolean retrieval åªè€ƒæ…®äº†æ–‡ä»¶æ˜¯å¦åŒ…å«æˆ–ä¸åŒ…å«è¦çš„è©å½™ï¼Œç‚ºäº†é‚£å€‹è©å½™çš„ã€Œé‡è¦æ€§ã€ä¹Ÿè€ƒæ…®é€²å»ï¼Œæˆ‘å€‘è¦çµ¦äºˆæ¯å€‹ term ä¸åŒçš„ weightã€‚</p>

<h3 id="term-weighting">Term weighting</h3>

<ul>
  <li>The score of a document d with respect to a query q is the sum of the weights of all terms t that occur in both d and q.</li>
</ul>

<script type="math/tex; mode=display">\textrm{score(d, q)} = \sum_{t \in (d \bigcap q)} \textrm{weight(d, q)}</script>

<ul>
  <li>Any specific way to assign weights to terms is called a term weighting scheme.</li>
</ul>

<p>è¨ˆç®— weight çš„æ–¹æ³•æœ‰å¾ˆå¤šç¨®ï¼Œä¹Ÿå¯ä»¥è‡ªè¡Œå®šç¾©ï¼Œé€™é‚Šä»‹ç´¹ä¸€ç¨®å¸¸ç”¨çš„æ–¹æ³•ã€‚</p>

<h3 id="tdidf-weight">TDâ€“IDF weight</h3>

<p>TDâ€“IDF is Term frequencyâ€“inverse document frequency. This formula includes two parts: Term frequency &amp; Inverse document frequency.</p>

<p><strong>Term frequency</strong></p>

<p>The number of times a term t occurs in a document d is called the term frequency of t in d, and is denoted by tf(t, d).</p>

<p>-<em>A problem with term frequency</em></p>

<p>Relevance is not a linear function of term frequency. ä¾‹å¦‚ï¼Œä¸€å€‹å‡ºç¾20æ¬¡çš„å–®è©é›£é“å°±ä»£è¡¨å®ƒæ¯”ä¸€å€‹åªå‡ºç¾1æ¬¡çš„å–®è©é‡è¦20å€å—ï¼Ÿç‚ºäº†è¦é™ä½å‡ºç¾é »ç‡é€ æˆå¤ªå¤§çš„å½±éŸ¿ï¼Œæ‰€ä»¥æˆ‘å€‘å°‡ frequency å– logï¼Œé€™æ™‚å€™ weight(t, d) å®šç¾©æˆï¼Œ</p>

<script type="math/tex; mode=display">% <![CDATA[
weight(t, d) = \left\{ \begin{array}{rcl}
1 + log_{10}tf(t, d), & if ~ tf(t, d) > 0 \\
0, & others \\
\end{array}\right. %]]></script>

<p>ç¨±ä½œï¼Œ<em>log-frequency weighting</em></p>

<p>-<em>Another problem with term frequency</em></p>

<p>æ™‚å¸¸é‡è¤‡å‡ºç¾çš„æ…£ç”¨è©å½™å°ä¸€å€‹æ–‡ä»¶çš„å½±éŸ¿å¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œç¾åœ¨æƒ³è¦è€ƒæ…® Sherlock Holmes è£¡åŒ…å« Moriarty å’Œ Holmes çš„ç« ç¯€ï¼Œä½†ç•¶æˆ‘å€‘ä¸‹ query å»æ‰¾æ™‚æœƒç™¼ç¾ï¼ŒHolmes å‡ºç¾æœ€å¤šçš„ç« ç¯€å¹¾ä¹æ²’æœ‰ Moriartyï¼Œé€™æ˜¯å› ç‚º Holmes åœ¨é€™äº›æ–‡æœ¬è£¡å‡ºç¾çš„æ¯”ä¾‹å¯¦åœ¨å¤ªé«˜äº†ï¼Œå¦‚æœç›´æ¥ä»¥å®ƒå‡ºç¾çš„é »ç‡ä¾†åˆ†æè‚¯å®šæœƒé€ æˆå¾ˆå¤§çš„èª¤å·®ï¼Œé€²è€Œå½±éŸ¿äº†å‡ºç¾æ¬¡æ•¸å°‘å¾ˆå¤šçš„ Moriartyã€‚æ‰€ä»¥é€™æ™‚å€™ Inverse document frequency å°±éå¸¸é‡è¦äº†ï¼</p>

<p><strong>Inverse document frequency</strong></p>

<ul>
  <li>Let N be the total number of documents in the collection.</li>
  <li>The number of documents that contain a term t is called the
document frequency of t, and is denoted by df(t).</li>
</ul>

<p>It denotes by idf(t):</p>

<script type="math/tex; mode=display">\textrm{idf(t)} = log \frac{N}{df(t)}</script>

<p>df(t) è¶Šå¤§ä»£è¡¨è©²è©å½™å‡ºç¾åœ¨è¶Šå¤šæ–‡ç« ä¸­ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œå¦‚æœ df(t) è¶Šå¸¸å‡ºç¾ï¼Œå‰‡ idf(t) å°±æœƒè¶Šå°ï¼Œæ›å¥è©±èªªï¼Œè©²è©å½™çš„é‡è¦æ€§è¶Šå°ã€‚</p>

<p><strong>TDâ€“IDF Weight</strong></p>

<p>ç¾åœ¨æˆ‘å€‘æœ‰äº† td å’Œ idfï¼Œtfâ€“idf weight å°±æ˜¯å°‡é€™å…©å€‹æ•¸å­—ç›¸ä¹˜ï¼ˆå‡ºç¾æ¬¡æ•¸ä¹˜ä»¥é‡è¦æ€§ï¼‰ï¼Œ</p>

<p>The <strong>tfâ€“idf weight</strong> of a term t in a document d is defined as</p>

<script type="math/tex; mode=display">\textrm{tfâ€“idf(t, d)} = tf(t, d) \cdot log \frac{N}{df(t)}</script>

<p>where N denotes the number of documents in the collection.</p>

<p><strong>Variations of the tfâ€“idf weighting scheme</strong></p>

<p>å› ç‚º df(t) å¯èƒ½æ˜¯ 0ï¼Œæ‰€ä»¥æˆ‘å€‘æ”¹è‰¯ä¸€ä¸‹ä¸Šé¢çš„å¼å­ã€‚</p>

<p>In scikit-learn, the tfâ€“idf weight is computed as</p>

<script type="math/tex; mode=display">\textrm{tfâ€“idf(t, d)} = tf(t, d) \cdot (log \frac{1 + N}{1 + df(t)} + 1)</script>

<p>where N denotes the number of documents in the collection.</p>

<hr />

<h2 id="the-vector-space-model">The vector space model</h2>

<h3 id="document-representations">Document representations</h3>

<ul>
  <li>Documents as sets of terms
In Boolean retrieval, the only relevant information is whether or not a term is present in a document.</li>
  <li>Documents as bags of terms
In ranked retrieval based on term frequency, the only relevant information is how often a term is present in a document.</li>
</ul>

<h3 id="the-vector-space-model--idea-1">The vector space model â€“ idea 1</h3>
<p>Represent documents as vectors in a high-dimensional space:</p>

<ul>
  <li>The dimensions (axes) of the space correspond to the terms in
the vocabulary (potentially relevant terms). For example: could be set of all words in the collection, set of most frequent words, â€¦</li>
  <li>The values of the vector components depend on the term weighting scheme: Boolean values, counts, tfâ€“idf values, â€¦ (in scikit-learn: CountVectorizer, TfidfVectorizer)</li>
</ul>

<h3 id="the-vector-space-model--idea-2">The vector space model â€“ idea 2</h3>
<p>To rank documents in the vector space model,</p>

<ul>
  <li>we represent the query as a vector in the same space as the documents in the collection</li>
  <li>we compute the score of a candidate document as the similarity between its document vector and the query vector (similarity = proximity in the vector space)</li>
</ul>

<h3 id="cosine-similarity">Cosine similarity</h3>

<p>ä»¥ä¸‹èˆ‰ä¸€å€‹å¾ä¸€ç¯‡<a href="https://raymondyangsite.wordpress.com/2017/05/03/retrieval-model-vector-space-model1/">éƒ¨è½æ ¼</a>çœ‹åˆ°çš„ä¾‹å­ï¼Œ</p>

<p>ä¾‹å¦‚:
Query = {â€œHelloâ€}
Doc1 = {â€œFooâ€, â€œFooâ€}
Doc2 = {â€œHelloâ€, â€œWorldâ€}</p>

<p>åœ¨ç¶“éè¨ˆç®—ä¹‹å¾Œï¼Œæˆ‘å€‘çŸ¥é“å…¶å‘é‡åˆ†åˆ¥ç‚ºï¼ˆå¯ä»¥ä½¿ç”¨ tf-idf values, counts,â€¦ ç­‰ç­‰ï¼‰</p>

<p>Query = (1, 0, 0)
Doc1 = (0, 0, 2)
Doc2 = (1, 1, 0)</p>

<p>æˆ‘å€‘å¯ä»¥ä½¿ç”¨æ­å¹¾é‡Œå¾·è·é›¢ç®—å‡º query å’Œå…©å€‹ documents çš„è·é›¢åˆ†åˆ¥æ˜¯ï¼Œæ ¹è™Ÿ5å’Œ1ã€‚ä½†è¨ˆç®—æ­å¹¾é‡Œå¾·è·é›¢å¿½ç•¥æ‰äº†å–®è©å‡ºç¾çš„é »ç‡çš„å½±éŸ¿ã€‚</p>

<p>å¦‚æœèªªä»Šå¤©æŠŠä¸Šé¢çš„å‘é‡æ”¹ä¸€ä¸‹ï¼Œ</p>

<p>Query = (2, 0, 0)
Doc1 = (0, 0, 4)
Doc2 = (2, 2, 0)</p>

<p>é€™æ™‚å€™ æ‰€æœ‰çš„è·é›¢éƒ½è®Šæˆäº†å…©å€ï¼Œä½†å…¶å¯¦ä»–å€‘å®Œå…¨æ˜¯å¹³è¡Œçš„å‘é‡ã€‚</p>

<p>é‚£å¦‚æœæˆ‘å€‘è€ƒæ…®å‘é‡ä¹‹é–“çš„è§’åº¦å‘¢ï¼Ÿä¹Ÿå°±æ˜¯è¨ˆç®—å…©å€‹å‘é‡çš„ dotï¼Œé€™æ™‚å€™æœƒç™¼ç¾ï¼Œé•·åº¦é‚„æ˜¯å½±éŸ¿äº†æœ€å¾Œçš„æ•¸å€¼ã€‚</p>

<p>å› æ­¤ï¼Œä½¿ç”¨ <strong>Cosine similarity</strong> ä¾†å°‡æ•¸å€¼ normalizedã€‚ä½¿ç”¨ cosine å¾Œï¼Œæ•¸å€¼åªæœƒä»‹æ–¼ -1 å’Œ 1 ä¹‹é–“ï¼Œä¸¦ä¸”åªéœ€è¦è€ƒæ…®é0çš„æ•¸å€¼ã€‚</p>

<hr />

<h2 id="evaluation-of-information-retrieval-systems">Evaluation of information retrieval systems</h2>

<p>To evaluate an IR system we need:</p>
<ul>
  <li>a document collection</li>
  <li>a collection of queries</li>
  <li>a gold-standard relevance judgement</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Precision%20and%20recall%20for%20Boolean%20retrieval.png" alt="PMF_two coins.png" /></p>

<p><strong>Precision:</strong></p>

<p>Precision (P) is the fraction of retrieved documents that are relevant</p>

<script type="math/tex; mode=display">Precision = \frac{\textrm{#(relevant items retrieved)}}{\textrm{#(retrieved items)}} = P(\textrm{relevant|retrieved})</script>

<p><strong>Recall:</strong></p>

<p>Recall (R) is the fraction of relevant documents that are retrieved</p>

<script type="math/tex; mode=display">Recall = \frac{\textrm{#(relevant items retrieved)}}{\textrm{#(relevant items)}} = P(\textrm{retrieved|relevant})</script>

<hr />

<p><strong>Lab:</strong> <a href="https://github.com/shihs/732A92-TextMining/blob/master/Lab1/TM-L1.ipynb">Information Retrieval Lab</a></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html">Introduction to Information Retrieval</a>
<br />
<a href="https://taweihuang.hpd.io/2017/03/01/tfidf/">[æ–‡ä»¶æ¢å‹˜] TF-IDF æ¼”ç®—æ³•ï¼šå¿«é€Ÿè¨ˆç®—å–®å­—èˆ‡æ–‡ç« çš„é—œè¯</a>
<br />
<a href="https://raymondyangsite.wordpress.com/2017/05/03/retrieval-model-vector-space-model1/">Vector Space Model(1)</a></p>

 -->
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//text%20mining/2019/11/05/Text-Mining-Information-Retrieval/#disqus_thread';
        this.page.identifier = '/text%20mining/2019/11/05/Text-Mining-Information-Retrieval/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//text%20mining/2019/11/05/Text-Mining-Information-Retrieval/#disqus_thread" data-disqus-identifier="/text%20mining/2019/11/05/Text-Mining-Information-Retrieval/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/">
        [Machine Learning]Markov Chain Monte Carlo (MCMC)
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on March 12, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">March 12, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <p><strong>MCMC æ˜¯ä»€éº¼ï¼Ÿ</strong></p>

<p>Markov Chain Monte Carlo ï¼ˆMCMCï¼‰æ˜¯ä¸€ç¨®æŠ½æ¨£æ–¹æ³•ï¼Œç”¨ä¾†è§£æ±ºç„¡æ³•ç›´æ¥æŠ½æ¨£çš„åˆ†ä½ˆçš„éš¨æ©ŸæŠ½æ¨£å•é¡Œã€‚</p>

<p><strong>The Goal of MCMC</strong></p>

<p>We want to sample from some distribution p, or approximate an <script type="math/tex">E[f(x)] ~ where ~ (X \sim p)</script>
<br />
é€šå¸¸ p æ˜¯ä¸€å€‹å¾ˆè¤‡é›œçš„ distributionï¼Œè¦å¾é€™å€‹åˆ†ä½ˆå– sample æ ¹æœ¬ä¸å¯èƒ½ï¼Œæ‰€ä»¥é€™æ™‚å€™å°±æœƒéœ€è¦ MCMCã€‚</p>

<hr />

<p><strong>ç‚ºä»€éº¼è¦ä½¿ç”¨ MCMCï¼Ÿ</strong></p>

<p>åœ¨ Baysian Inference ä¸­å¸¸ä½¿ç”¨åˆ°é€™å€‹å…¬å¼</p>

<script type="math/tex; mode=display">p(\theta\mid y) = \frac{p(y \mid \theta) p(\theta)}{\int p(y \mid \theta) p(\theta)d\theta}</script>

<p>æˆ–æ˜¯ï¼Œå®ƒçš„åƒæ•¸ <script type="math/tex">\theta</script> çš„è²å¼ä¼°è¨ˆ</p>

<script type="math/tex; mode=display">\hat \theta = E[\theta \mid y] = \int \theta p(y \mid \theta) d \theta = \frac{\int \theta p(y \mid \theta) p(\theta)}{\int p(y \mid \theta) p(\theta)d\theta}</script>

<p>å…¶ä¸­çš„ <script type="math/tex">p(\theta)</script> ç‚ºå…ˆé©—æ©Ÿç‡ (prior probability)ï¼ŒåŸºæœ¬ä¸Šæˆ‘å€‘ç„¡æ³•ç²å¾— <script type="math/tex">p(\theta)</script> çš„å€¼ï¼Œé€™æ™‚å€™å°±éœ€è¦ MCMCã€‚</p>

<hr />

<p>MCMC ç”±å…©éƒ¨åˆ†çš„è§€å¿µ (æ­¥é©Ÿ) çµ„æˆ, ä¸€å€‹æ˜¯ ã€ŒMarkov Chainã€ ï¼Œå¦ä¸€å€‹å‰‡æ˜¯ã€ŒMonte Carlo integrationã€ï¼Œæ¥ä¸‹ä¾†å°‡èªªæ˜é€™å…©å€‹éƒ¨åˆ†ã€‚</p>

<hr />

<p><strong>Monte Carlo Integration</strong></p>

<p>Monte Carlo Integration å¯ä»¥ä»¥æŠ½æ¨£å¹³å‡çš„æ–¹å¼è¨ˆç®—ä¸Šé¢çš„æœŸæœ›å€¼å¼å­</p>

<script type="math/tex; mode=display">E[\theta \mid y] \approx \frac{1}{n}\sum_{i=1}^{t}{(\theta_i \mid y)}, ~ \theta_i \stackrel{iid}{\sim} p(\theta \mid y)</script>

<p>ä¹Ÿå°±æ˜¯èªªï¼Œä½¿ç”¨æ¨£æœ¬å¹³å‡æ•¸ä¾†ä¼°è¨ˆæœŸæœ›å€¼ã€‚é€™ä»¶äº‹æƒ…å¯ä»¥æˆç«‹æ˜¯å› ç‚ºï¼Œæ ¹æ“šå¤§æ•¸æ³•å‰‡ï¼Œç•¶æ¨£æœ¬æ•¸ n å¤ å¤§æ™‚ï¼Œæ¨£æœ¬çš„å¹³å‡æ•¸å°‡è¶¨è¿‘æ–¼æ¯é«”å¹³å‡æ•¸ã€‚</p>

<p>é€™æ¨£çœ‹èµ·ä¾† Monte Carlo Integration è®“æˆ‘å€‘çœå»äº†ä¸Šé¢é‚£å€‹çœ‹èµ·ä¾†è¤‡é›œçš„ç©åˆ†å¼å­ã€‚ä½†æ˜¯ï¼Œç¾å¯¦ç”Ÿæ´»ä¸­å¾ˆå¤šæ™‚å€™ä¸¦ç„¡æ³•å¾ <script type="math/tex">p(\theta \mid y)</script> é€™å€‹ distribution æŠ½æ¨£ï¼Œè¦ä¸å°±æ˜¯ä¸çŸ¥é“é€™å€‹ distributionï¼Œè¦ä¸å°±æ˜¯é€™å€‹ distribution çˆ†ç‚¸è¤‡é›œå•Šã€‚</p>

<p>æ‰€ä»¥èªªï¼Œå“ªæœ‰é€™éº¼å¥½çš„äº‹ï¼Ÿ</p>

<p>é€™æ™‚å€™å¯ä»¥æ¡ç”¨å…¶ä»–çš„æŠ½æ¨£æ–¹æ³•ï¼Œè­¬å¦‚ï¼Œ<em>rejection sampling</em>, <em>importance sampling</em> å’Œæœ¬æ–‡çš„é‡é» <em>MCMC</em>ã€‚</p>

<hr />

<p><strong>Markov Chain</strong></p>

<ul>
  <li>
    <p>Markov Chain (é¦¬å¯å¤«éŠ)ï¼šA Markov chain is a sequence <script type="math/tex">X_0, X_1, ...</script> of random variables such that the distribution of the next value depends only on the current on (and parameters). ç¾åœ¨æœ‰ä¸€éš¨æ©Ÿè®Šæ•¸æ•¸åˆ— <script type="math/tex">X_0, X_1, ...</script>ï¼Œä¸”æ¯ä¸€å€‹è®Šæ•¸åªå’Œå‰ä¸€å€‹è®Šæ•¸æœ‰é—œï¼Œä¹Ÿå°±æ˜¯ <script type="math/tex">X_{t+1}</script> ä¾†è‡ª <script type="math/tex">p(X_{t+1} \mid X_t)</script>ï¼Œåƒé€™æ¨£çš„æ•¸åˆ—æˆ‘å€‘å°±ç¨±ç‚ºé¦¬å¯å¤«éŠã€‚</p>
  </li>
  <li>
    <p><script type="math/tex">p(X_{t+1} \mid X_t)</script> è¢«ç¨±ç‚ºé€™å€‹é¦¬å¯å¤«éŠçš„è½‰æ›æ ¸å¿ƒ (transition kernel)</p>
  </li>
  <li>
    <p>A Markov chain is stationary, with stationary distribution <script type="math/tex">Î¦, if ~\forall k ~ X_k  \sim Î¦</script></p>
  </li>
  <li>
    <p>One shows (not trivial in general) that under <em>certain</em> conditions a Markov chain will converge to the stationary distribution in the limit. åœ¨ä¸€èˆ¬æ¢ä»¶å‡è¨­åº•ä¸‹ï¼Œé¦¬å¯å¤«éŠçš„è®Šæ•¸åˆ†é…å°‡æ”¶æ–‚åˆ°ç›®æ¨™æ©Ÿç‡å‡½æ•¸ <script type="math/tex">\pi(Â·)</script> ä¸¦ä¸”èˆ‡ <script type="math/tex">X_0</script> çš„é¸æ“‡ç„¡é—œã€‚</p>
  </li>
</ul>

<p><em>Monte Carlo Integration</em>å¯ä»¥çœ‹<a href="https://www.youtube.com/watch?v=MKnjsqYVG4Y">é€™è£¡</a></p>

<hr />

<p>Reference:
<br />
<a href="https://www.youtube.com/watch?v=3ZmW_7NXVvk">(ML 18.1) Markov chain Monte Carlo (MCMC) introduction-12eZWG0Z5gY.mp4</a>
<br />
<a href="http://web.ntpu.edu.tw/~ccw/statmath/M_mcmc.pdf">The Markov Chain Monte Carlo Simulations</a>
<br /> 
LinkÃ¶ping University - 732A90 Computational Statistics 2019 Lecture 4 slide
<br />
<a href="https://zhuanlan.zhihu.com/p/25610149">[æ•°æ®åˆ†æ] Markov Chain Monte Carlo</a>
<br />
<a href="https://www.youtube.com/watch?v=s8w8AsFK77c&amp;list=PLyAft-JyjIYq2SLTHO2ptmx-cChbE5GBm">å¾äº¦è¾¾æœºå™¨å­¦ä¹ è¯¾ç¨‹ Markov Chain Monte Carlo</a>
<br />
<a href="http://www.cnblogs.com/pinard/p/6625739.html">MCMC(ä¸€)è’™ç‰¹å¡ç½—æ–¹æ³•</a></p>
 -->
    <div class="post-excerpt"><p><strong>MCMC æ˜¯ä»€éº¼ï¼Ÿ</strong></p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/#disqus_thread';
        this.page.identifier = '/machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/#disqus_thread" data-disqus-identifier="/machine%20learning/2019/03/12/Machine-Learning-Markov-chain-Monte-Carlo-(MCMC)/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">
        [Machine Learning]Covolutional Neural Networks(CNN)(2)
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on March 2, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">March 2, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Machine Learning">Machine Learning</a>
        
      
    </span>


    </div>
    <!-- <p><a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">å‰é¢</a>ä»‹ç´¹äº† Convolution operationã€‚</p>

<ul>
  <li>Padding</li>
  <li>Strided Convolution</li>
  <li>Convolutions Over Volume
***</li>
</ul>

<h2 id="padding">Padding</h2>

<p><strong>ç‚ºä»€éº¼éœ€è¦ Padding?</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/padding.png" alt="padding.png" height="400px" width="600px" /></p>

<p>Convolution operation çš„ç¼ºé»ï¼š</p>
<ol>
  <li>ç¶“é convolution fitler å¾ŒåŸå§‹åœ–å°‡è®Šå°ã€‚å¦‚æœæœ‰å¾ˆå¤šå±¤ convolutional layerï¼Œè³‡è¨Šåœ¨æ¯æ¬¡é‹ç®—å¾Œä¸æ–·æå¤±ã€‚</li>
  <li>æœ€é‚Šç·£çš„ pixel åªæœƒè¢«ä½¿ç”¨åˆ°ä¸€æ¬¡ï¼Œä½†ä¸­é–“çš„ pixel æœƒä½¿ç”¨åˆ°è¼ƒå¤šæ¬¡ã€‚ä¹Ÿå°±æ˜¯èªªï¼Œæœƒæœ‰é‚Šç·£çš„å¾ˆå¤šè³‡è¨Šç¼ºå¤±ã€‚</li>
</ol>

<p>æ‰€ä»¥ç‚ºäº†è§£æ±ºä»¥ä¸Šçš„ç¼ºé»ï¼Œæˆ‘å€‘å°‡ input çš„åœ–çš„é‚Šç·£åŠ ä¸Šä¸€å€‹ pixelï¼Œé€™æ¨£ç¶“éå·ç©é‹ç®—å¾Œçš„åœ–åƒä»å’ŒåŸåœ–ä¸€æ¨£å¤§ã€‚é‚Šç·£ä¹Ÿæ˜¯å¯ä»¥åŠ  2 pixel çš„ã€‚</p>

<p>ä»¥ä¸Šåœ–ç‚ºä¾‹ï¼ŒåŸæœ¬ç‚º 6x6(nxn) çš„åœ–ï¼Œç¶“é 3x3(fxf) çš„ filter åšå·ç©é‹ç®—å¾Œï¼Œè®Šç‚º (n-f+1)x(n-f+1) çš„åœ–ã€‚</p>

<p>ä½†å¦‚æœä»Šå¤©é‚Šç·£åŠ ä¸Š p pixelï¼Œæœ€å¾Œåœ–æª”å°±æœƒè®Šç‚º (n+2p-f+1)x(n+2p-f+1) çš„åœ–ã€‚</p>

<hr />

<p><strong>The choice of padding - Vaild and Same convolutions</strong></p>

<ol>
  <li>Vaild - No padding</li>
  <li>Same - Pad so that output size is the same as the input size
<br />
æ ¹æ“šä¸Šé¢çš„å…¬å¼ï¼Œå¦‚æœåœ¨ input åœ–é‚Šç·£åŠ ä¸Š pixel å¾Œç¶“éå·ç©é‹ç®—è¦å’ŒåŸå§‹åœ–æª”å¤§å°ã„§æ¨£ï¼Œé‚£è¦ç¬¦åˆ n+2p-f+1 = nï¼Œé‚£å°±æœƒå¾—åˆ° p = (f-1)/2</li>
</ol>

<p>For computer vision, usually f is odd. 3 by 3 filters are very common.</p>

<hr />

<h2 id="strided-convolution">Strided Convolution</h2>

<p>å‰é¢æˆ‘å€‘åœ¨åš convolution æ™‚ filter éƒ½æ˜¯ä¸€æ¬¡è·³ä¸€æ ¼è¨ˆç®—ï¼Œstrided å‰‡æ˜¯ä¸€æ¬¡è·³ä¸åªä¸€æ ¼ã€‚</p>

<p>å¦‚ä¸‹åœ–ï¼Œé€™æ™‚å€™æœ€å¾Œçµæœçš„ pixel ä¹Ÿæœƒä¸åŒï¼Œè¨ˆç®—å…¬å¼å°±æœƒæ˜¯ ((n+2p-f)/2)+1 å– floorã€‚
<img src="http://localhost:4000/blog/img/posts/strided.png" alt="strided.png" height="400px" width="600px" /></p>

<hr />

<p><strong>cross-correlation vs. convolution</strong></p>

<ul>
  <li>cross-correlationï¼šæ•¸å­¸å®¶å–œæ­¡å°‡ convolutional operation ç¨±ç‚º cross-correlationã€‚åœ¨åšé‹ç®—æ™‚æœƒå°‡ filter åšæ°´å¹³èˆ‡å‚ç›´ç¿»è½‰ï¼Œå¦‚ä¸‹åœ–ã€‚</li>
  <li>convolutionï¼šåœ¨ deep learning é€šå¸¸éƒ½ç¨±ç‚º convolutionï¼Œä¸”ä¸æœƒå°‡ filter åšé¡å°„çš„å‹•ä½œã€‚</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/cross-correlation.png" alt="cross-correlation.png" height="400px" width="600px" /></p>

<p>é‚£é€™æ¨£å¹¹å˜›è¦ç¿»è½‰ï¼Ÿå› ç‚ºåœ¨åšè¨Šè™Ÿè™•ç†æ™‚ï¼Œç¿»è½‰å¾Œæœƒæœ‰ä¸€äº›å¥½çš„ç‰¹æ€§å¯ä»¥æ‹¿ä¾†ä½¿ç”¨ã€‚ä½†åœ¨åš deep learning æ™‚åŸºæœ¬ä¸Šæ²’æœ‰å½±éŸ¿ï¼Œæ‰€ä»¥åš deep learning æ™‚ä¸æœƒç¿»è½‰ filterã€‚</p>

<hr />

<h2 id="convolutions-over-volume">Convolutions Over Volume</h2>

<p><strong>Convolutions on RGB impages</strong></p>

<p>å‰é¢èˆ‰çš„ä¾‹å­éƒ½æ˜¯åœ¨ 2D ä¸Šè¨ˆç®— convolutionï¼Œç¾åœ¨è¦ä¾†çœ‹å¦‚ä½•åœ¨ 3D ä¸Šå¯¦ç¾ convolutionã€‚</p>

<hr />

<p>Reference:
<br />
<a href="https://www.coursera.org/learn/convolutional-neural-networks/home/welcome">Coursera - Convolutional Neural Networks(deeplearning.ai)</a></p>
 -->
    <div class="post-excerpt"><p><a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">å‰é¢</a>ä»‹ç´¹äº† Convolution operationã€‚</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/#disqus_thread';
        this.page.identifier = '/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/#disqus_thread" data-disqus-identifier="/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/">
        [Algorithm]åŸºå› æ¼”ç®—æ³•Genetic Algorithm
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on March 2, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">March 2, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog/category/#Algorithm">Algorithm</a>
        
      
    </span>


    </div>
    <!-- <p>åŸºå› æ¼”ç®—æ³•ï¼ˆGenetic Algorithmï¼‰æ˜¯ä¸€ç¨®æ±‚å‡½æ•¸æ¥µå€¼çš„æœ€ä½³åŒ–ï¼ˆå‡½æ•¸çš„æœ€å¤§æˆ–æœ€å°å€¼ï¼‰çš„æ–¹æ³•ã€‚</p>

<p>å®ƒçš„æƒ³æ³•æ˜¯ä¾†è‡ªæ–¼åŸºå› éºå‚³ï¼Œé€éç´°èƒåˆ†è£‚å°‡å¥½çš„åŸºå› ä¿ç•™ï¼Œä¸å¥½çš„åŸºå› æ·˜æ±°ï¼Œä¸€ä»£å‚³ä¸€ä»£ï¼Œæœ€å¾Œç•™ä¸‹æœ€é©åˆç”Ÿå­˜çš„ç‰©ç¨®ï¼Œæ‰€è¬‚ã€Œé©è€…ç”Ÿå­˜ã€ã€‚
å¯¦ç¾çš„æ–¹å¼å°±åƒåŸºå› çš„æ©Ÿåˆ¶é‚£æ¨£ï¼Œé€é<em>éš¨æ©Ÿ</em>é¸æ“‡å¾Œï¼Œå†é€²è¡Œ crossover å’Œ mutationï¼Œç¶“éå¤šæ¬¡è¿­ä»£å¾Œï¼Œæœ€å¾Œçµæœå°‡æœƒæ”¶æ–‚åˆ°ä¸€å€‹æœ€ä½³è§£ã€‚ä½†é€™æœ€ä½³è§£ä¸ã„§å®šæ˜¯ global maximumï¼ˆminimumï¼‰ï¼Œé€šå¸¸æ˜¯ local maximumï¼ˆminimumï¼‰ã€‚</p>

<p>ï¼ˆä½†å…¶å¯¦é€™èˆ‡åŸºå› çš„çœŸå¯¦è¡Œç‚ºå®Œå…¨ç„¡é—œï¼Œå°±åƒæ˜¯ Neural Network å’Œäººé¡ç¥ç¶“åæ‡‰çš„çœŸå¯¦è¡Œç‚ºç„¡é—œä¸€æ¨£ã€‚ï¼‰</p>

<hr />
<h3 id="æ¼”ç®—æ³•æµç¨‹åœ–">æ¼”ç®—æ³•æµç¨‹åœ–</h3>
<p><img src="http://localhost:4000/blog/img/posts/genetic%20algorithm.gif" alt="genetic algorithm.gif" height="380px" width="600px" />
<a href="https://dotblogs.com.tw/dragon229/2013/01/03/86692">Source</a></p>

<ol>
  <li>ä¸€é–‹å§‹éš¨æ©Ÿç”¢ç”Ÿnå€‹è®Šæ•¸ (nç”±ä½¿ç”¨è€…æ±ºå®š)</li>
  <li>åˆ©ç”¨é©æ‡‰å‡½æ•¸ï¼ˆfitness functionï¼‰è¨ˆç®—æ‰€æœ‰è®Šæ•¸çš„é©æ‡‰å€¼</li>
  <li>ä¾æ¯å€‹çš„é©æ‡‰å€¼é€²è¡Œã€Œé¸æ“‡ã€è¤‡è£½ã€</li>
  <li>å°ç•™ä¸‹çš„è®Šæ•¸é€²è¡Œäº¤é…ï¼ˆcrossoverï¼‰åŠçªè®Šï¼ˆmutationï¼‰çš„å‹•ä½œ</li>
</ol>

<hr />

<h3 id="ç¯„ä¾‹">ç¯„ä¾‹</h3>

<p>ä»¥ä¸‹ç¯„ä¾‹æ˜¯ä¾†è‡ª<a href="http://www.csie.ntnu.edu.tw/~u91029/Optimization.html">æ¼”ç®—æ³•ç­†è¨˜çš„ Optimization</a></p>

<p>1.
[åˆå§‹åŒ–]
ä¸€é–‹å§‹å…ˆéš¨ä¾¿å¼„å‡ºå¹¾å€‹xã€‚æœ¬ä¾‹æ˜¯å››å€‹ã€‚</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010101010
1011001011
1110101011
0010101000
</code></pre></div></div>

<p>2.
[fitness function]
æ ¹æ“šå•é¡Œç‰¹æ€§ï¼Œå®šç¾©å¥½å£ç¨‹åº¦ã€‚</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f(1010101010) = 678
</code></pre></div></div>

<p>3.
[selection]
éš¨ä¾¿æ‰¾å€‹ä½ç½®åˆ‡ä¸€åˆ€ï¼Œæ¯å€‹xéƒ½è¢«åˆ†æˆå…©æ®µã€‚</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010101  010
1011001  011
1110101  011
0010101  000
</code></pre></div></div>

<p>4.
[crossover]
éš¨ä¾¿æ‰¾å…©çµ„ä½ è¦ºå¾—å¤ å„ªè‰¯çš„xï¼Œäº¤å‰ç›¸æ¥è®Šæˆæ–°ç­”æ¡ˆã€‚
é‡è¤‡ä¸€ç›´åšï¼Œç›´åˆ°xæ•¸ç›®è·ŸåŸå…ˆä¸€æ¨£å¤šã€‚æœ¬ä¾‹æ˜¯å››å€‹ã€‚</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010101 \/ 010  -&gt;  1010101 -- 011
1011001 /\ 011      1011001 -- 010 


1010101011
1011001010
1110101010
1010101000
</code></pre></div></div>

<p>5.
[mutation]
æ¯å€‹xéƒ½éš¨ä¾¿æ‰¾ä¸€å€‹åœ°æ–¹æŠŠæ•¸å­—æ”¹æ‰ï¼Œä¹Ÿå¯ä»¥ä¸æ”¹ã€‚</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1010111011
1011001000
1110101010
1010101001
</code></pre></div></div>

<p>6.
é‡è¤‡3. 4. 5.ï¼Œç›´åˆ°è£¡é¢æœ‰ä¸€å€‹xæ˜¯ä½ æ»¿æ„çš„ï¼Œä»¤f(x)æœ€å¤§çš„é‚£å€‹xã€‚</p>
<ol>
  <li>éš¨æ©Ÿç”¢ç”ŸNå€‹xã€‚</li>
  <li>è¨ˆç®—fitness functionã€‚</li>
  <li>é‡è¤‡ä»¥ä¸‹æ­¥é©Ÿï¼Œç›´åˆ°æœ‰ä¸€å€‹xè®“äººæ»¿æ„ã€‚
<br />
ã€€ç”²ã€selectionã€‚
<br />
ã€€ä¹™ã€crossoverã€‚
<br />
ã€€ä¸™ã€mutationã€‚
<br />
ã€€ä¸ã€è¨ˆç®—fitness functionã€‚</li>
</ol>

<p>ä¸€é–‹å§‹çš„ x çš„è¶³å¤ è±å¯Œï¼Œå¤šæ¼”åŒ–å¹¾æ¬¡å°±å¯ä»¥å¾—åˆ°ä¸éŒ¯çš„çµæœã€‚ä¸€é–‹å§‹çš„ x è¶³å¤ è±å¯Œï¼Œå¯ä»¥é¿å…é€²å…¥å€åŸŸæ¥µå€¼ã€‚ mutation ç”¨æ–¼å¢åŠ  x çš„è±å¯Œæ€§ï¼Œä»¥è·³è„«å€åŸŸæ¥µå€¼ã€‚</p>

<hr />

<p>Reference:</p>

<p><a href="http://www.csie.ntnu.edu.tw/~u91029/Optimization.html">æ¼”ç®—æ³•ç­†è¨˜ - Optimization</a>
<br />
<a href="http://littledoa.wixsite.com/pcclab/single-post/2016/02/03/Genetic-Algorithm-åŸºå› æ¼”ç®—æ³•">Genetic Algorithm åŸºå› æ¼”ç®—æ³•</a>
<br />
<a href="https://www.youtube.com/watch?v=UE6YkRWBtZk">äººå·¥æ™ºæ…§ç³»åˆ—ä¹‹åŸºå› æ¼”ç®—æ³•</a></p>
 -->
    <div class="post-excerpt"><p>åŸºå› æ¼”ç®—æ³•ï¼ˆGenetic Algorithmï¼‰æ˜¯ä¸€ç¨®æ±‚å‡½æ•¸æ¥µå€¼çš„æœ€ä½³åŒ–ï¼ˆå‡½æ•¸çš„æœ€å¤§æˆ–æœ€å°å€¼ï¼‰çš„æ–¹æ³•ã€‚</p>

</div>
    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog//algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/#disqus_thread';
        this.page.identifier = '/algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog//algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/#disqus_thread" data-disqus-identifier="/algorithm/2019/03/02/Algorithm-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95Genetic-Algorithm/">0 Comments</a>
      </span>

    </div>
    
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>
    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
