<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blog/blog/atom.xml">
  <!-- æ•¸å­¸ç¬¦è™Ÿ -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category">Category</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2023. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts" id="begin">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2019/12/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5.html">
        [Database]Database åŸºç¤æ¦‚å¿µ
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 31, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 31, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Database">Database</a>
        
      
    </span>

    </div>
    <!-- <h2 id="è³‡æ–™dataèˆ‡è³‡è¨Šinformation">è³‡æ–™(Data)èˆ‡è³‡è¨Š(Information)</h2>

<ul>
  <li>è³‡æ–™(Data)ï¼šè³‡è¨Š(Information)çš„åŸå§‹å‹æ…‹ï¼Œæ˜¯æœªç¶“æ•´ç†å’Œåˆ†æçš„åŸå§‹æ•¸å€¼ã€æ–‡å­—æˆ–ç¬¦è™Ÿã€‚</li>
  <li>è³‡è¨Š(Information)ï¼šç¶“éæ•´ç†å’Œåˆ†æå¾Œçš„è³‡æ–™ï¼Œæ˜¯æœ‰å¯¦è³ªæ„ç¾©çš„è³‡æ–™ã€‚</li>
  <li>è³‡æ–™è™•ç†(Data Processing)ï¼šè³‡æ–™(Data) ç¶“éè³‡æ–™è™•ç†(Data Processing) å¾Œå°‡è³‡æ–™è½‰æ›ç‚ºè³‡è¨Š(Information)ã€‚</li>
</ul>

<h2 id="è³‡æ–™éšå±¤">è³‡æ–™éšå±¤</h2>

<p>è³‡æ–™éšå±¤çš„æœ€å°å„²å­˜å–®ä½æ˜¯ä½å…ƒï¼Œ8å€‹ä½å…ƒçµ„æˆä¸€å€‹ä½å…ƒçµ„ï¼Œä¹Ÿå°±æ˜¯ASCIIç¢¼çš„å­—å…ƒã€‚æ•¸å€‹ä½å…ƒçµ„çµåˆæˆæ¬„ä½ï¼Œå¤šå€‹æ¬„ä½çµ„æˆè¨˜éŒ„ï¼Œæœ€å¾Œå°‡ä¸€çµ„è¨˜éŒ„å„²å­˜æˆæª”æ¡ˆï¼Œè³‡æ–™åº«å°±æ˜¯ ä¸€çµ„ç›¸é—œæª”æ¡ˆçš„é›†åˆã€‚</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/%E8%B3%87%E6%96%99%E9%9A%8E%E5%B1%A4.png" alt="è³‡æ–™éšå±¤.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">è³‡æ–™éšå±¤</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>ç¬¬ä¸€éšå±¤ï¼šä½å…ƒ(Bits)</li>
  <li>ç¬¬äºŒéšå±¤ï¼šä½å…ƒçµ„(Bytes)</li>
  <li>ç¬¬ä¸‰éšå±¤ï¼šæ¬„ä½(Fields)</li>
  <li>ç¬¬å››éšå±¤ï¼šè¨˜éŒ„(Records)</li>
  <li>ç¬¬äº”éšå±¤ï¼šæª”æ¡ˆ(Files)</li>
  <li>ç¬¬å…­éšå±¤ï¼šè³‡æ–™åº«(Database)</li>
</ul>

<hr />

<h2 id="è³‡æ–™åº«ç³»çµ±database-system">è³‡æ–™åº«ç³»çµ±(Database System)</h2>

<p>è³‡æ–™åº«ç³»çµ±(Database System)æ˜¯ç”±ã€Œè³‡æ–™åº«ã€(Database)å’Œã€Œè³‡æ–™åº«ç®¡ç†ç³»çµ±ã€(Database Management Systemï¼ŒDBMS)æ‰€çµ„æˆã€‚</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Database%20System.png" alt="Database System.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">è³‡æ–™åº«ç³»çµ±</a></td>
    </tr>
  </tbody>
</table>

<h3 id="è³‡æ–™åº«database">è³‡æ–™åº«(Database)</h3>
<ol>
  <li>Represents some aspects of the real world (miniworld)</li>
  <li>è³‡æ–™åº«æ˜¯ä¸€å€‹å°çµæ§‹åŒ–è³‡è¨Šæˆ–è³‡æ–™çš„çµ„ç¹”æ€§æ”¶é›†ï¼Œé€šå¸¸ä»¥é›»å­æ–¹å¼å„²å­˜åœ¨é›»è…¦ç³»çµ±ã€‚</li>
  <li>è³‡æ–™åº«é€šå¸¸ç”±è³‡æ–™åº«ç®¡ç†ç³»çµ±(DBMS)æ§åˆ¶ã€‚</li>
  <li>ç›®å‰é‹è¡Œä¸­æœ€å¸¸è¦‹çš„è³‡æ–™åº«å‹æ…‹é€šå¸¸æ˜¯åœ¨ä¸€ç³»åˆ—çš„è¡¨æ ¼ä¸­é€²è¡Œè¡Œåˆ—é–“å»ºæ¨¡ï¼Œä½¿å¾—è™•ç†å’Œè³‡æ–™æŸ¥è©¢æ›´ç‚ºæœ‰æ•ˆã€‚</li>
  <li>å¤§éƒ¨åˆ†çš„è³‡æ–™åº«ä½¿ç”¨çµæ§‹åŒ–æŸ¥è©¢èªè¨€(SQL)ä¾†æ›¸å¯«æˆ–æŸ¥è©¢è³‡æ–™ã€‚</li>
  <li>è³‡æ–™åº«å®¹è¨±å¤šåç”¨æˆ¶åœ¨åŒä¸€æ™‚é–“å¿«é€Ÿã€å®‰å…¨åœ°ä»¥é«˜åº¦è¤‡é›œé‚è¼¯å’Œèªè¨€å–å¾—æˆ–æŸ¥è©¢è³‡æ–™ã€‚</li>
  <li>Meta-data: Database definition or descriptive information (Stored by the DBMS in a database catalog or data dictionary)</li>
</ol>

<h3 id="è³‡æ–™åº«ç®¡ç†ç³»çµ±dbms">è³‡æ–™åº«ç®¡ç†ç³»çµ±(DBMS)</h3>
<ol>
  <li>è³‡æ–™åº«é€šå¸¸éœ€è¦ä¸€å€‹ç¨±åšè³‡æ–™åº«ç®¡ç†ç³»çµ±(DBMS)çš„å…¨é¢è³‡æ–™åº«è»Ÿé«”ç¨‹å¼ã€‚</li>
  <li>DBMSæ˜¯ä½œç‚ºè³‡æ–™åº«å’Œå…¶ç”¨æˆ¶æˆ–ç¨‹å¼ä¹‹é–“çš„ä»‹é¢ï¼Œè®“ç”¨æˆ¶èƒ½æª¢ç´¢ã€æ›´æ–°å’Œç®¡ç†ï¼Œä½¿å¾—è³‡è¨Šçµ„ç¹”åŒ–å’Œæœ€ä½³åŒ–ã€‚</li>
  <li>DBMSäº¦å¯ä¿ƒé€²å°è³‡æ–™åº«çš„ç›£ç£å’Œæ§åˆ¶ï¼Œå¢å¼·å„æ–¹é¢çš„ç®¡ç†é‹ä½œï¼ŒåŒ…æ‹¬ç¸¾æ•ˆç›£æ§ã€æœ€ä½³åŒ–ã€å‚™ä»½ä»¥åŠç³»çµ±æ¢å¾©ã€‚</li>
</ol>

<hr />

<h2 id="database-system-design-process">Database System Design Process</h2>

<p>å¥½çš„è³‡æ–™åº«ç³»çµ±è¨­è¨ˆå°æ–¼è³‡æ–™åº«çš„ç¶­è­·ã€æ›´æ–°åŠä¿®æ”¹ç›¸ç•¶é‡è¦ï¼Œå¦å¤–ï¼Œå¦‚ä½•æœ‰æ•ˆçš„å„²å­˜ã€æå–è³‡æ–™ä¹Ÿæ˜¯è³‡æ–™åº«é‡è¦çš„ä¸€ç’°ï¼Œæ¥ä¸‹ä¾†è¦ä»‹ç´¹è³‡æ–™åº«çš„è¨­è¨ˆã€‚</p>

<p><strong>è³‡æ–™åº«ç³»çµ±è¨­è¨ˆï¼ˆDatabase System Processï¼‰</strong>æœ‰å…©å€‹ä¸»è¦çš„æ–¹å‘ï¼Œ</p>

<ol>
  <li><strong>Database design</strong>ï¼ˆè³‡æ–™åº«è¨­è¨ˆï¼‰
    <ul>
      <li>focuses on defining the database</li>
    </ul>
  </li>
  <li><strong>Application design</strong>ï¼ˆæ‡‰ç”¨è¨­è¨ˆï¼‰
    <ul>
      <li>focuses on the programs and interfaces that access the database</li>
    </ul>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Database%20System%20Design%20Process.png" alt="Database System Design Process.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology01-2019-6up.pdf">Database System Design Process</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="database-designè³‡æ–™åº«è¨­è¨ˆ-data-modelingè³‡æ–™å¡‘æ¨¡">Database Designï¼ˆè³‡æ–™åº«è¨­è¨ˆï¼‰/ Data Modelingï¼ˆè³‡æ–™å¡‘æ¨¡ï¼‰</h2>

<p>(åœ¨ <a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology01-2019-6up.pdf">Database Technology</a> è¬›ç¾©è£¡å¯«çš„æ˜¯ Database Designï¼Œä½†æ‰¾åˆ°å…©å€‹å°ç£çš„è¬›ç¾©ï¼ˆ<a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">ç¬¬1ç«  è³‡æ–™åº«çš„åŸºç¤</a>ã€<a href="https://www.mis.nsysu.edu.tw/db-book/PDF/Ch2.pdf">ç¬¬äºŒç«  å¯¦é«”é—œä¿‚æ¨¡å¼:åŸºæœ¬æ¦‚å¿µ</a>ï¼‰éƒ½æ˜¯ç”¨è³‡æ–™å¡‘æ¨¡ï¼ˆData Modelingï¼‰ï¼Œæ‰€ä»¥é€™å…©å€‹æ±è¥¿æˆ‘æŠŠå®ƒå€‘æ”¾åœ¨ä¸€èµ·)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Data%20Modeling.png" alt="Data Modeling.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">Data Modeling</a></td>
    </tr>
  </tbody>
</table>

<h3 id="è³‡æ–™åº«è¨­è¨ˆéšæ®µphases-for-designing-a-database">è³‡æ–™åº«è¨­è¨ˆéšæ®µï¼ˆPhases for designing a databaseï¼‰:</h3>
<ol>
  <li>Requirements specification and analysis</li>
  <li><strong>Conceptual design</strong>ï¼ˆConceptual modelï¼Œæ¦‚å¿µå¡‘æ¨¡)
    <ul>
      <li>e.g., using the Entity-Relationship model (ER Model)</li>
    </ul>
  </li>
  <li><strong>Logical design</strong>ï¼ˆLogical modelï¼Œé‚è¼¯å¡‘æ¨¡)
    <ul>
      <li>e.g., using the relational model (é—œè¯æ¨¡å‹)</li>
    </ul>
  </li>
  <li><strong>Physical design</strong>ï¼ˆPhysical modelï¼Œå¯¦é«”å¡‘æ¨¡)
    <ul>
      <li>e.g., è¨­å®šç´¢å¼•</li>
    </ul>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://www.guru99.com/images/DatabaseDesignProcess(1).png" alt="Database development life cycle" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.guru99.com/database-design.html">Database development life cycle</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="conceptual-modelæ¦‚å¿µå¡‘æ¨¡">Conceptual modelï¼ˆæ¦‚å¿µå¡‘æ¨¡ï¼‰</h2>

<p>æ¦‚å¿µå¡‘æ¨¡çš„ç›®çš„æ˜¯å°‡ç¾å¯¦ä¸­æŸéƒ¨åˆ†çš„è³‡æ–™é—œä¿‚ç”¨<strong>çµæ§‹åŒ–</strong>çš„æ–¹å¼å‘ˆç¾ï¼Œå»ºç«‹æ•´å€‹è³‡æ–™åº«é‚è¼¯çµæ§‹çš„æ¨¡å‹ï¼Œéç¨‹ä¸æ¶‰åŠä»»ä½•è³‡æ–™åº«ç®¡ç†ç³»çµ±ã€è³‡æ–™åº«ç¨®é¡ã€è»Ÿé«”å’Œå¯¦éš›å„²å­˜çµæ§‹ã€‚æœ€å¸¸ä½¿ç”¨<em>å¯¦é«”é—œä¿‚åœ–</em>ï¼ˆEntity Relationship Diagramï¼‰ä¾†ç¹ªè£½<em>å¯¦é«”é—œè¯æ¨¡å‹</em>ï¼ˆEntity-Relationship Modelï¼‰ã€‚</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/ER%20Model.png" alt="ER Model.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">ER Model</a></td>
    </tr>
  </tbody>
</table>

<h3 id="entity-relationship-model-er-model">Entity-Relationship model (ER Model)</h3>

<ol>
  <li>ER Model å‰‡æ˜¯ç”¨ä¾†ç¹ªè£½<strong>çµæ§‹åŒ–</strong>è³‡æ–™çš„æ¦‚å¿µåœ–ã€‚</li>
  <li>ER Model çµ„æˆå…ƒä»¶åŒ…æ‹¬å¯¦é«”ï¼ˆEntityï¼‰å’Œé—œä¿‚ï¼ˆRelationshipï¼‰ã€‚
    <ul>
      <li>å¯¦é«”ï¼ˆEntityï¼‰- æ˜¯åœ¨çœŸå¯¦ä¸–ç•Œè­˜åˆ¥å‡ºçš„æ±è¥¿ï¼Œä¾‹å¦‚ï¼šè€å¸«ã€å­¸ç”Ÿã€è»Šå­ã€å“ç‰Œã€‚</li>
      <li>é—œè¯æ€§ï¼ˆRelationshipsï¼‰- åœ¨äºŒå€‹æˆ–å¤šå€‹å¯¦é«”é–“æ“æœ‰çš„é—œä¿‚ï¼Œä¸»è¦åˆ†ç‚ºä¸‰ç¨®ï¼šä¸€å°ä¸€ã€ä¸€å°å¤šã€å¤šå°å¤šã€‚</li>
    </ul>
  </li>
</ol>

<p><a href="https://shihs.github.io/blog/database/2020/01/01/Database-ER-Modelå¯¦é«”é—œä¿‚åœ–/">ä¸‹ä¸€ç¯‡</a>æœ‰è©³ç´°è§£èªª</p>

<hr />

<h2 id="logical-modelé‚è¼¯å¡‘æ¨¡">Logical modelï¼ˆé‚è¼¯å¡‘æ¨¡ï¼‰</h2>

<p>ä½¿ç”¨çš„å·¥å…·æ˜¯é—œè¯æ¨¡å‹ï¼ˆRelational Modelï¼‰ï¼Œæœ€å¾Œæœƒç”¢ç”Ÿè³‡æ–™è¡¨çš„å®šç¾©é—œè¯ç¶±ç›®ï¼ˆschemaï¼‰ã€‚</p>

<p>é‚è¼¯æ¨¡å‹ä¸»è¦æ˜¯ç”±ä¸‰ç¨®å…ƒç´ æ‰€çµ„æˆï¼Œå¦‚ä¸‹æ‰€ç¤º:</p>
<ul>
  <li>è³‡æ–™çµæ§‹ï¼ˆData Structuresï¼‰ï¼šè³‡æ–™çš„çµ„æˆæ–¹å¼ï¼Œå°±æ˜¯æ¬„å’Œåˆ—çµ„æˆè¡¨æ ¼çš„é—œè¯è¡¨(Relations)</li>
  <li>è³‡æ–™æ“ä½œæˆ–é‹ç®—ï¼ˆData Manipulation æˆ– Operationsï¼‰ï¼šè³‡æ–™çš„ç›¸é—œæ“ä½œï¼Œé—œè¯å¼ä»£æ•¸(Relational Algebra)å’Œé—œè¯å¼è¨ˆç®—(Relational Calculus)</li>
  <li>å®Œæ•´æ€§é™åˆ¶æ¢ä»¶ï¼ˆIntegrity Constraintsï¼‰ï¼šç¶­è­·è³‡æ–™å®Œæ•´æ€§çš„æ¢ä»¶ï¼Œå…¶ç›®çš„æ˜¯ç¢ºä¿å„²å­˜çš„è³‡æ–™æ˜¯åˆæ³•çš„è³‡æ–™</li>
</ul>

<p>è³‡æ–™åº«ç³»çµ±æ¼”é€²å„å¹´ä»£çš„è³‡æ–™åº«ç³»çµ±ä¸­ï¼Œå…¶ä½¿ç”¨çš„è³‡æ–™åº«æ¨¡å‹å°±æ˜¯é‚è¼¯è³‡æ–™æ¨¡å‹ï¼Œä¸»è¦æœ‰å››ç¨®é‚è¼¯è³‡æ–™æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤º:</p>
<ul>
  <li>éšå±¤å¼æ¨¡å‹ï¼ˆHierarchical Modelï¼‰</li>
  <li>ç¶²è·¯å¼æ¨¡å‹ï¼ˆNetwork Modelï¼‰</li>
  <li><strong>é—œè¯å¼æ¨¡å‹ï¼ˆRelational Modelï¼‰</strong><a href="(https://zh.wikipedia.org/wiki/å…³ç³»æ¨¡å‹)">wiki</a></li>
  <li>ç‰©ä»¶å°å‘å¼æ¨¡å‹ï¼ˆObject-Oriented Modelï¼‰</li>
</ul>

<h3 id="relational-model-relational-data-model">Relational Model (Relational Data Model)</h3>

<ul>
  <li>ä¸­æ–‡ç¨±ã€Œé—œè¯ï¼ˆå¼ï¼‰æ¨¡å‹ã€ã€‚</li>
  <li>Relational database é—œè¯å¼è³‡æ–™åº«: represent data as a collection of relationsã€‚ã€Œé—œè¯å¼è³‡æ–™åº«ã€ä½¿ç”¨å¤šå€‹é—œè¯è¡¨ï¼ˆrelationsï¼‰ä¾†å‘ˆç¾è³‡æ–™ã€‚</li>
</ul>

<p><strong>Relationï¼ˆé—œè¯è¡¨ï¼‰</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/relation.png" alt="relation.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology02-2019-6up.pdf">relation (é—œè¯è¡¨)</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>Relation æ˜¯é—œè¯è¡¨ï¼ˆå¦‚ä¸Šåœ–ï¼‰ï¼Œæ˜¯åŒ…å« data çš„ tableï¼ˆè¡¨æ ¼ï¼‰</p>
  </li>
  <li>Each row (tuple) represents a record of related data values
    <ul>
      <li>ä¸€åˆ—ï¼ˆrowï¼‰ç¨±ç‚ºä¸€å€‹å€¼çµ„ï¼ˆtupleï¼‰ï¼Œç´€éŒ„å„å€‹ data çš„å€¼</li>
    </ul>
  </li>
  <li>Each column (attribute) holds a corresponding value for each row
    <ul>
      <li>Columns associated with a data type (domain)</li>
      <li>Each column header: attribute name</li>
    </ul>
  </li>
  <li><strong>Domain</strong>
    <ul>
      <li>Domain is a set of <em>atomic values</em>. 
<br />
e.g., \(\{ 0, 1, 2, ... \}\), \(\{\text{Jo Smith}, \text{Dana Jones}, \text{Ashley Wong}, \text{Y. K. Lee}, ...\}\)</li>
      <li><em>atomic values</em>: Each value indivisibleï¼ˆä¸å¯åˆ†å‰²ï¼‰</li>
      <li>Domains specified by <em>data type</em>(integer, string, date, real, etc.)</li>
    </ul>
  </li>
  <li><strong>Relation Schema</strong>
    <ul>
      <li>Schema describes the relationï¼ˆé—œè¯è¡¨ï¼‰</li>
      <li>é—œè¯è¡¨ç¶±è¦ï¼ˆrelation schemaï¼‰æ˜¯ç”±é—œè¯è¡¨åç¨± Rï¼ˆrelation nameï¼‰å’Œä¸€é€£ä¸²å±¬æ€§ \((A_1,A_2,â€¦.,A_n)\)æ¸…å–®ï¼ˆlistï¼‰é‚„æœ‰å®šç¾©åŸŸï¼ˆdomainï¼‰æ‰€çµ„æˆ</li>
      <li><strong>integrity constraints</strong>ï¼ˆå®Œæ•´æ€§é™åˆ¶ï¼‰</li>
      <li>Denoted by \(R(A_1, A_2, ..., A_n)\)</li>
    </ul>
  </li>
  <li><strong>Attribute</strong> \(A_i\)
    <ul>
      <li>Name of a role in the relation schema R</li>
      <li>ä½¿ç”¨ \(dom(A_i)\) çš„ç¬¦è™Ÿä¾†èªªæ˜å±¬æ€§ \(A_i\) çš„å®šç¾©åŸŸçš„ç¯„åœæ˜¯ä»€éº¼</li>
      <li>å±¬æ€§åç¨±ä¸å¯é‡è¤‡ï¼Œä½† domain å¯ä»¥é‡è¤‡</li>
    </ul>
  </li>
  <li><strong>NULL Values</strong>
    <ul>
      <li>Each domain may be augmented with a special value called NULL</li>
    </ul>
  </li>
  <li><em>Question:</em> A relation schema consists of?
    <ul>
      <li>Ans: relation name, attribute names and domains, and integrity constraints</li>
    </ul>
  </li>
</ul>

<h3 id="integrity-constraintså®Œæ•´æ€§é™åˆ¶">Integrity Constraintsï¼ˆå®Œæ•´æ€§é™åˆ¶ï¼‰</h3>

<ul>
  <li>å»ºç«‹æª¢æŸ¥è³‡æ–™åº«å„²å­˜è³‡æ–™çš„ä¾æ“šå’Œä¿éšœè³‡æ–™çš„æ­£ç¢ºæ€§ã€‚ä¸ä½†å¯ä»¥é˜²æ­¢æˆæ¬Šä½¿ç”¨è€…å°‡ä¸åˆæ³•è³‡æ–™å­˜å…¥è³‡æ–™åº«ï¼Œé‚„èƒ½å¤ é¿å…é—œè¯è¡¨é–“çš„è³‡æ–™ä¸ä¸€è‡´ã€‚</li>
  <li>é—œè¯å¼è³‡æ–™åº«æ¨¡å‹çš„å®Œæ•´æ€§é™åˆ¶æ¢ä»¶æœ‰å¾ˆå¤šç¨®ï¼Œé©ç”¨æ‰€æœ‰é—œè¯å¼è³‡æ–™åº«çš„å®Œæ•´æ€§é™åˆ¶æ¢ä»¶æœ‰å››ç¨®ï¼Œå¦‚ä¸‹æ‰€ç¤º:
    <ul>
      <li>éµé™åˆ¶æ¢ä»¶(Key Constraints)</li>
      <li>å®šç¾©åŸŸé™åˆ¶æ¢ä»¶(Domain Constraints)</li>
      <li>å¯¦é«”å®Œæ•´æ€§(Entity Integrity)</li>
      <li>åƒè€ƒå®Œæ•´æ€§(Referential Integrity)</li>
    </ul>
  </li>
</ul>

<p><strong>éµé™åˆ¶æ¢ä»¶ï¼ˆKey Constraintsï¼‰</strong></p>

<p>éµé™åˆ¶æ¢ä»¶ï¼ˆKey Constraintsï¼‰æ˜¯æŒ‡é—œè¯è¡¨ä¸€å®šæ“æœ‰ä¸€å€‹å”¯ä¸€å’Œæœ€å°çš„ä¸»éµï¼ˆPrimary Keyï¼‰</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/keys.png" alt="keys.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">Key</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>è¶…éµï¼ˆSuperkeysï¼‰
    <ul>
      <li>æ˜¯é—œè¯è¡¨ç¶±è¦çš„å–®ä¸€å±¬æ€§æˆ–å±¬æ€§å€¼é›†åˆï¼Œè¶…éµéœ€è¦æ»¿è¶³å”¯ä¸€æ€§ï¼ˆUniquenessï¼‰</li>
      <li>Superkeys å¯ä»¥ç”±ä¸€å€‹æˆ–å¤šå€‹è¡Œï¼ˆå±¬æ€§ï¼‰çµ„æˆï¼Œåªè¦å”¯ä¸€å°±å¯ä»¥</li>
      <li>ä¸‹æ–¹ç¯„ä¾‹é—œè¯è¡¨ä¾†çœ‹ï¼Œç¬¦åˆçš„æœ‰(sid)ã€(SSN)ã€(sid, SSN)ã€(sid, ename)ã€(SSN, cname)ã€(sid, tel)ã€(SSN, cname, postcode)â€¦â€¦</li>
    </ul>
  </li>
  <li>å€™é¸éµï¼ˆCandidate Keysï¼‰
    <ul>
      <li>å€™é¸éµï¼ˆCandidate Keysï¼‰æ˜¯ä¸€å€‹è¶…éµï¼Œåœ¨æ¯ä¸€å€‹é—œè¯è¡¨è‡³å°‘æ“æœ‰ä¸€å€‹å€™é¸éµï¼Œä¸åªæ»¿è¶³è¶…éµçš„å”¯ä¸€æ€§ï¼Œé‚„éœ€è¦æ»¿è¶³æœ€å°æ€§ï¼ˆMinimalityï¼‰</li>
      <li>Candidate Keys æ˜¯è¶…éµçš„å­é›†åˆï¼Œä½†ä¸åŒçš„æ˜¯ï¼Œå€™é¸éµé‚„å¿…é ˆè¦æœ‰æœ€å°æ€§ï¼Œæ‰€ä»¥å–®ä¸€å±¬æ€§çš„è¶…éµä¸€å®šæ˜¯å€™é¸éµ</li>
      <li>èˆ‰å€‹ä¾‹å­ï¼Œå¦‚æœä¸‹é¢çš„ç¯„ä¾‹å¤šäº†ä¸€å€‹å­¸ç”Ÿ sid = 5, ename = Janeï¼Œé€™æ™‚å€™ (sid, ename) å°±ä¸æ˜¯å€™é¸éµï¼Œå› ç‚ºå»æ‰ ename ä¹Ÿèƒ½å€åˆ†æ¯ä¸€è¡Œï¼Œæ‰€ä»¥ ename æ˜¯å¤šé¤˜çš„</li>
    </ul>
  </li>
  <li>ä¸»éµï¼ˆPrimary Keyï¼‰
    <ul>
      <li>ç¬¦åˆçš„æœ‰(sid)ã€(SSN)</li>
      <li>å¾å€™é¸éµä¸­é¸å‡ºä¸€å€‹ä½œç‚ºä¸»éµï¼ŒæŒ‘é¸ä¸»éµçš„åŸå‰‡å¦‚ä¸‹æ‰€ç¤º:
        <ul>
          <li>ä¸å¯ç‚ºç©ºå€¼(Not Null)</li>
          <li>æ°¸é ä¸æœƒæ”¹è®Š(Never Change)</li>
          <li>éè­˜åˆ¥å€¼(Nonidentifying Value)</li>
          <li>ç°¡çŸ­ä¸”ç°¡å–®çš„å€¼(Brevity and Simplicity)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>æ›¿ä»£éµï¼ˆAlternate Keysï¼‰
    <ul>
      <li>åœ¨å€™é¸éµä¸­ä¸æ˜¯ä¸»éµçš„å…¶ä»–å€™é¸éµç¨±ç‚ºæ›¿ä»£éµ(Alternate Keys)ï¼Œå› ç‚ºé€™äº›æ˜¯å¯ä»¥ç”¨ä¾†æ›¿ä»£ä¸»éµçš„å€™é¸éµ</li>
      <li>å¦‚æœ (sid) æ˜¯ä¸»éµï¼Œå‰‡ (SSN) æ˜¯æ›¿ä»£éµ</li>
    </ul>
  </li>
  <li>å¤–ä¾†éµï¼ˆForeign Keysï¼‰
    <ul>
      <li>æ˜¯é—œè¯è¡¨çš„å–®ä¸€æˆ–å¤šå€‹å±¬æ€§çš„é›†åˆï¼Œå®ƒçš„å±¬æ€§å€¼æ˜¯åƒè€ƒåˆ°å…¶ä»–é—œè¯è¡¨çš„ä¸»éµï¼Œç”¨ä¾†å»ºç«‹å…©å€‹é—œè¯è¡¨é–“çš„é€£æ¥</li>
      <li>æ›å¥è©±èªªï¼ŒForeign Keys æ˜¯å…¶ä»–é—œè¯è¡¨çš„ä¸»éµï¼Œå®ƒï¼ˆå€‘ï¼‰å¯ä»¥æ˜¯è©²é—œè¯è¡¨çš„ä¸»éµï¼Œä½†ä¸ä¸€å®šæ˜¯</li>
      <li>å¤–ä¾†éµå’Œåƒè€ƒçš„ä¸»éµå±¬æ–¼ç›¸åŒå®šç¾©åŸŸï¼Œä¸éå±¬æ€§åç¨±å¯ä»¥ä¸åŒ</li>
      <li>å¤–ä¾†éµå¯ä»¥æ˜¯ç©ºå€¼NULL</li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Students.png" alt="Students.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">ç¯„ä¾‹é—œè¯è¡¨</a></td>
    </tr>
  </tbody>
</table>

<p><strong>å®šç¾©åŸŸé™åˆ¶æ¢ä»¶(Domain Constraints)</strong></p>
<ul>
  <li>æ¯å€‹å€¼éƒ½å¿…é ˆæ˜¯åŸºå…ƒå€¼ä¸”å¿…é ˆåœ¨å®šç¾©åŸŸå…§</li>
  <li>ä¾‹å¦‚ï¼Œå±¬æ€§ age çš„å®šç¾©åŸŸæ˜¯ intï¼Œå±¬æ€§å€¼å¯ä»¥ç‚º5 ï¼Œä½†ä¸å¯ä»¥æ˜¯4.5</li>
</ul>

<p><strong>å¯¦é«”å®Œæ•´æ€§(Entity Integrity)</strong></p>
<ul>
  <li>å¯¦é«”å®Œæ•´æ€§æ˜¯é—œè¯è¡¨å…§éƒ¨çš„å®Œæ•´æ€§æ¢ä»¶ï¼Œä¸»è¦æ˜¯ç”¨ä¾†è¦ç¯„é—œè¯è¡¨ä¸»éµçš„ä½¿ç”¨è¦å‰‡</li>
  <li>ä¸»éµï¼ˆprimary  keyï¼‰ä¸å¯ä»¥æ˜¯ Null</li>
  <li>ä¾‹å¦‚ï¼Œï¼ˆename, cnameï¼‰æ˜¯ä¸»éµï¼Œename å±¬æ€§ä¸å¯ç‚ºç©ºå€¼ï¼›cname å±¬æ€§ä¹Ÿä¸å¯æ˜¯ç©ºå€¼ã€‚</li>
</ul>

<p><strong>åƒè€ƒå®Œæ•´æ€§(Referential Integrity)</strong></p>
<ul>
  <li>é—œè¯è¡¨çš„æ‰€æœ‰å¤–ä¾†éµå€¼ï¼Œéƒ½å¿…é ˆèƒ½åƒè€ƒåˆ°å¦ä¸€é—œè¯è¡¨çš„ä¸»éµå€¼</li>
</ul>

<hr />

<h2 id="physical-modelå¯¦é«”å¡‘æ¨¡">Physical Modelï¼ˆå¯¦é«”å¡‘æ¨¡ï¼‰</h2>

<ul>
  <li>é‡å°æŒ‡å®šè³‡æ–™åº«ç®¡ç†ç³»çµ±å»ºç«‹å¯¦éš›è³‡æ–™åº«çµæ§‹çš„è³‡æ–™æ¨¡å‹ï¼Œä¾‹å¦‚:SQL Server</li>
  <li>å°æ–¼é—œè¯å¼è³‡æ–™åº«æ¨¡å‹çš„å¯¦é«”æ¨¡å‹ä¾†èªªï¼Œå°±æ˜¯åœ¨è³‡æ–™åº«ç®¡ç†ç³»çµ±è»Ÿé«”å»ºç«‹é—œè¯è¡¨(Relation) çš„è¡¨æ ¼ã€é—œè¯æ€§(Relationship)å’Œç´¢å¼•ç­‰å®šç¾©è³‡æ–™</li>
  <li>ç°¡å–®ä¾†èªªï¼Œå°±æ˜¯å°‡ Conceptual model å’Œ Logical model å¯¦éš›å»ºç«‹å‡ºä¾†</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/%E5%AF%A6%E9%AB%94%E6%A8%A1%E5%9E%8B.png" alt="å¯¦é«”æ¨¡å‹.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">å¯¦é«”æ¨¡å‹</a></td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="http://www.csie.sju.edu.tw/cm/course/db/ch01.pdf">ç¬¬1ç«  è³‡æ–™åº«çš„åŸºç¤</a>
<br />
<a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">ç¬¬3ç«  é—œè¯å¼è³‡æ–™åº«æ¨¡å‹</a>
<br />
<a href="https://www.oracle.com/tw/database/what-is-database.html">Oracle - ä»€éº¼æ˜¯è³‡æ–™åº«ï¼Ÿ</a>
<br />
<a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology01-2019-6up.pdf">Database Technology - Topic 1: Introduction</a>
<br />
<a href="https://www.ida.liu.se/~TDDD37/fo/DBTechnology02-2019-6up.pdf">Database Technology - Topic 2: Relational Databases</a>
<br />
<a href="http://championdatablog.com/è‡ªå‹•è‰ç¨¿/">æ·±å…¥äº†è§£é—œè¯å¼è³‡æ–™æ¨¡å‹(Relational Data Model)</a>
<br />
<a href="https://www.mis.nsysu.edu.tw/db-book/PDF/Ch2.pdf">ç¬¬äºŒç«  å¯¦é«”é—œä¿‚æ¨¡å¼:åŸºæœ¬æ¦‚å¿µ</a>
<br />
<a href="https://www.mysql.tw/2015/04/data-modeling.html">Data Modeling (è³‡æ–™å¡‘æ¨¡) : æ¦‚å¿µå¡‘æ¨¡ã€é‚è¼¯å¡‘æ¨¡ã€å¯¦é«”å¡‘æ¨¡</a>
<br />
<a href="https://www.itread01.com/content/1545320464.html">å¿«é€Ÿç†è§£è³‡æ–™åº«è¶…éµï¼Œå€™é¸éµï¼Œä¸»éµ</a></p>

 -->
    
    <div class="post-excerpt"><h2 id="è³‡æ–™dataèˆ‡è³‡è¨Šinformation">è³‡æ–™(Data)èˆ‡è³‡è¨Š(Information)</h2>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2019/12/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5.html#disqus_thread';
        this.page.identifier = '/2019/12/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2019/12/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5.html#disqus_thread" data-disqus-identifier="/2019/12/Database-Database-%E5%9F%BA%E7%A4%8E%E6%A6%82%E5%BF%B5.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2019/12/NLP-NLTK-%E7%AD%86%E8%A8%98.html">
        [NLP]NLTK & spaCy ç­†è¨˜
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on December 17, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">December 17, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#NLP">NLP</a>
        
      
    </span>

    </div>
    <!-- <p>å¸¸è¦‹çš„ NLP package æœ‰ NLTKã€spaCyã€Scikit-learn ç­‰ç­‰ï¼Œåƒè€ƒä¸‹é¢çš„åŠŸèƒ½æ¯”è¼ƒåœ–ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/NLPlibraresPython.png" alt="" /></p>

<p>é€™è£¡å‰‡è¦åˆ†åˆ¥ä»‹ç´¹ NLTK å’Œ spaCy çš„ä¸€äº›ç”¨æ³•ã€‚å¾ä¸Šé¢çš„æ¯”è¼ƒåœ–æˆ‘å€‘çŸ¥é“ï¼Œé€™å…©å€‹ package åŸºæœ¬ä¸ŠåŠŸèƒ½ç›¸ä¼¼ï¼Œåœ¨ preprocessing æ™‚ä¸€å®šæœƒä½¿ç”¨åˆ°ï¼Œä½†å…©è€…æœ‰å„è‡ªçš„å„ªç¼ºé»ã€‚</p>

<h3 id="nltk">NLTK</h3>

<p>ä¸»è¦ç”¨æ–¼è™•ç†æ–·è©ã€è©é »ã€ pos ç­‰ç­‰ï¼Œæ›´å¤šç”¨æ³•è«‹åƒç…§ä¸‹é¢é€£çµã€‚</p>

<p><a href="https://github.com/shihs/732A92-TextMining/blob/master/note/NLTK.ipynb">NLTK</a></p>

<h3 id="spacy">spaCy</h3>

<p>spaCy æ˜¯ NLTK çš„ä¸»è¦ç«¶çˆ­è€…ï¼Œå®ƒå€‘çš„åŠŸèƒ½é¡ä¼¼ã€‚</p>

<p><a href="https://github.com/shihs/732A92-TextMining/blob/master/note/spaCy.ipynb">spaCy</a></p>

 -->
    
    <div class="post-excerpt"><p>å¸¸è¦‹çš„ NLP package æœ‰ NLTKã€spaCyã€Scikit-learn ç­‰ç­‰ï¼Œåƒè€ƒä¸‹é¢çš„åŠŸèƒ½æ¯”è¼ƒåœ–ã€‚</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2019/12/NLP-NLTK-%E7%AD%86%E8%A8%98.html#disqus_thread';
        this.page.identifier = '/2019/12/NLP-NLTK-%E7%AD%86%E8%A8%98.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2019/12/NLP-NLTK-%E7%AD%86%E8%A8%98.html#disqus_thread" data-disqus-identifier="/2019/12/NLP-NLTK-%E7%AD%86%E8%A8%98.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2019/11/Text-Mining-Word-embeddings.html">
        [Text Mining]Word embeddings
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 26, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 26, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Text Mining">Text Mining</a>
        
      
    </span>

    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="word-embeddings">Word embeddings</h2>

<p>å°æ–¼äººä¾†èªªï¼Œè¦ç†è§£æ–‡å­—ä¸¦ä¸æ˜¯ä»¶å›°é›£çš„äº‹ï¼Œä½†å°é›»è…¦ä¾†èªªï¼Œæ¯å€‹å­—ä¸éæ˜¯ä¸€ä¸² stringï¼Œæ‰€ä»¥ç•¶æˆ‘å€‘è¦åš text mining æ™‚å°±å¿…é ˆè¦å°‡é€™äº› string è½‰åŒ–æˆé›»è…¦å¯ä»¥ç†è§£çš„æ–¹å¼ã€‚
è€Œ word embedding(word vector or word representation) çš„æ¦‚å¿µå°±æ˜¯å°‡æ–‡å­—è½‰æ›æˆ vector ï¼Œå¥½è®“é›»è…¦å¯ä»¥è®€æ‡‚æ–‡å­—é–“çš„é—œä¿‚ã€‚</p>

<p>è­¬å¦‚èªªï¼Œäººé¡å¯ä»¥ç†è§£ pretty å’Œ beautiful æ˜¯ç›¸è¿‘è©ï¼Œä½†å¦‚æœåªæ˜¯çµ¦é›»è…¦é€™å…©å€‹å–®å­—ï¼Œå°æ–¼é›»è…¦ä¾†èªªï¼Œé€™åªæ˜¯å…©å€‹ä¸åŒé•·åº¦çš„ string ç½·äº†ã€‚word embedding æœƒå°‡é€™å…©å€‹å­—è½‰æ›æˆä¸åŒçš„ vector æ˜ å°„åˆ°ä¸€å€‹é«˜ç¶­ç©ºé–“ï¼Œç•¶é€™å…©å€‹ vector è¶Šæ¥è¿‘ï¼ˆå¯ä»¥ä½¿ç”¨ consine similarityï¼‰å°±è¡¨ç¤ºé€™å…©å€‹è©è¶Šç›¸è¿‘ã€‚é€™å°±æ˜¯ word embedding ä¸»è¦çš„æ¦‚å¿µã€‚</p>

<ul>
  <li>A word embedding is a mapping of words to points in a vector space such that nearby words (points) are similar in terms of their distributional properties.</li>
</ul>

<h3 id="the-distributional-principle">The distributional principle</h3>

<p>word embedding æ–¹æ³•å¯ä»¥ä½¿ç”¨æœ€é‡è¦çš„å°±æ˜¯å› ç‚ºæœ‰ distributional hypothesis é€™å€‹å‡è¨­ã€‚</p>

<ul>
  <li>The distributional principle states that words that occur in similar contexts tend to have similar meanings.</li>
</ul>

<p>é€™è£¡çš„æ¦‚å¿µæ˜¯èªªï¼Œè©(target words)å‡ºç¾åœ¨é¡ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­(context words)ï¼Œå‰‡å®ƒå€‘å¾ˆæœ‰å¯èƒ½æœ‰ç›¸ä¼¼çš„æ„æ€ã€‚
è­¬å¦‚èªªï¼Œ
<br />
-ã€Œé‚£éš»ã€è²“ã€å¥½å¯æ„›ã€
<br />
-ã€Œé‚£éš»ã€ç‹—ã€å¥½å¯æ„›ã€
<br />
é€™æ™‚å€™é™¤äº†ã€è²“ã€å’Œã€ç‹—ã€å¤–ï¼Œé€™å…©å¥è©±çš„ä¸Šä¸‹æ–‡æ˜¯ä¸€æ¨£çš„ï¼Œæ ¹æ“š distributional principleï¼Œé€™å…©å€‹è©æ‡‰è©²æ˜¯ç›¸ä¼¼çš„ã€‚</p>

<h3 id="co-occurrence-matrix">Co-occurrence matrix</h3>

<p><img src="http://localhost:4000/blog/img/posts/Co-occurrence%20matrix.png" alt="" /></p>

<p>ä¸Šåœ–ä¸­ï¼Œcontext words å°±æ˜¯ä¸Šä¸‹æ–‡ï¼Œè€Œ target words å°±æ˜¯æˆ‘å€‘æƒ³è¦åˆ†æçš„å­—è©ã€‚
<br />
ç¾åœ¨ä¾†çœ‹ cheese é€™å€‹å­—ï¼Œå¯ä»¥çœ‹åˆ°å’Œ butter, cake, cow, deer é€™å¹¾å€‹å­—ä¸€èµ·å‡ºç¾çš„æ¬¡æ•¸åˆ†åˆ¥æ˜¯ï¼Œ12, 2, 1å’Œ0æ¬¡ã€‚çœ‹èµ·ä¾†å’Œ butter é‚„æœ‰ cake é€£çµæ€§æ¯”è¼ƒå¼·ã€‚
å†ä¾†çœ‹ bread é€™å€‹å­—ï¼ŒåŒæ¨£çš„åœ¨ butter å’Œ cake ä¸Šçš„é€£çµä¹Ÿæ¯”è¼ƒå¼·ã€‚å¦‚æœæˆ‘å€‘æŠŠé€™å…©å€‹å–®å­—ç”¨å‘é‡è¡¨ç¤ºå°±æœƒæ˜¯ï¼Œ(12, 2, 1, 0) å’Œ (5, 5, 0, 0)ï¼Œå¯ä»¥å»æ¯”è¼ƒå’Œå…¶ä»–å…©å€‹å–®å­—çš„ cosine similarityï¼Œé€™å…©å€‹çš„é—œä¿‚æ˜¯æ¯”è¼ƒå¼·çš„ã€‚</p>

<p>ç¾åœ¨æˆ‘å€‘æŠŠå®ƒå€‘ç•«å‡ºä¾†ï¼ˆå› ç‚ºå®ƒå€‘è¢«æ˜ å°„åˆ°çš„æ˜¯å››ç¶­ç©ºé–“ï¼Œæ‰€ä»¥è€å¸«çš„ slide åªçœ‹ cow å’Œ cake é€™å…©å€‹ context wordsï¼‰</p>

<p><img src="http://localhost:4000/blog/img/posts/From%20co-occurrences%20to%20word%20vectors.png" alt="" /></p>

<p>å¦‚æœé‚„æ˜¯ä¸å¤ªæ˜ç™½ï¼Œ<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">é€™ç¯‡éƒ¨è½æ ¼</a>æ‡‰è©²å¯ä»¥çœ‹æ‡‚ã€‚</p>

<p><strong>è£œå……ï¼š</strong>
å¾ä¸Šé¢çš„ matrix å¯èƒ½æœƒæƒ³åˆ°ä¸€ä»¶äº‹ï¼Œ<em>context words</em> ç­‰æ–¼æ˜¯æ±ºå®šäº† target words çš„å‘é‡å•Šï¼
æ›å¥è©±èªªï¼Œç•¶æˆ‘å€‘åˆ†æä¸åŒçš„æ–‡æœ¬çš„æ™‚å€™ï¼Œæœƒéœ€è¦ä¸åŒçš„ context wordsä¾†ç®—å‡º word embeddingsã€‚æƒ³åƒï¼Œå¦‚æœä»Šå¤©è¦åˆ†ææ–°èå ±å°å’Œ ptt å…§å®¹ï¼Œä¸å¤ªå¯èƒ½ä½¿ç”¨ä¸€æ¨£çš„ context wordsï¼Œç•¢ç«Ÿpttç”¨èªå’Œæ–°èç”¨èªæœƒæœ‰å¾ˆå¤§çš„ä¸åŒã€‚ï¼ˆè›¤ï¼Ÿä½ èªªè¨˜è€…éƒ½æŠ„ ptt å…§å®¹å—ï¼ŸXDDDï¼‰</p>

<hr />

<h3 id="simple-applications-of-word-embeddings">Simple applications of word embeddings</h3>

<p>word embeddings çš„æ‡‰ç”¨</p>

<ul>
  <li>finding similar words. æ‰¾åˆ°ç›¸ä¼¼çš„å­—ï¼Œåƒæ˜¯ä¸Šé¢çš„ä¾‹å­ï¼Œæ‰¾å‡ºå“ªä¸€å€‹å­—å’Œ cheese æ¯”è¼ƒç›¸ä¼¼ã€‚</li>
  <li>answering â€˜odd one outâ€™ questions. æ‰¾å‡ºä¸ä¸€æ¨£çš„è©ï¼Œè­¬å¦‚èªª  lunch, breakfast, dinner, car å“ªä¸€å€‹è©å±¬æ–¼ä¸åŒé¡ï¼Ÿ (æ ¹æ“šä¸Šé¢æåˆ°çš„æ¦‚å¿µï¼Œlunch, breakfast, dinner é€™ä¸‰å€‹çš„ vector æ‡‰è©²æœƒæ¯”è¼ƒæ¥è¿‘ï¼Œæœƒåœ¨æ¯”è¼ƒæ¥è¿‘çš„ä¸Šä¸‹æ–‡ä¸­å‡ºç¾)</li>
</ul>

<h3 id="limitations-of-word-embeddings">Limitations of word embeddings</h3>

<ul>
  <li>
    <p>There are many different facets of â€˜similarityâ€™. Ex. Is a cat more similar to a dog or to a tiger? (åœ¨ä¸åŒæƒ…å¢ƒä¸‹ï¼Œcat å’Œ dog å¯èƒ½æ¯”è¼ƒç›¸ä¼¼ã€‚è­¬å¦‚èªªï¼Œè²“å’Œç‹—éƒ½æ˜¯å¯µç‰©ï¼Œä½†å¦‚æœä»¥ç”Ÿç‰©çš„è§’åº¦ä¾†çœ‹ï¼Œcat å’Œ tiger éƒ½å±¬æ–¼è²“ç§‘å‹•ç‰©ï¼Œé€™æ™‚å€™ cat å’Œ tiger æœƒæ¯”è¼ƒç›¸ä¼¼)</p>
  </li>
  <li>
    <p>Text data does not reflect many â€˜trivialâ€™ properties of words. Ex. more â€˜black sheepâ€™ than â€˜white sheepâ€™ (å¦‚æœåªåˆ†ææ–‡æœ¬ï¼Œå› ç‚ºå¤§éƒ¨åˆ†çš„ç¾Šéƒ½æ˜¯ç™½è‰²çš„ï¼Œæ‰€ä»¥åœ¨æåˆ°ç¾Šçš„æ™‚å€™ä¸¦ä¸æœƒç‰¹åˆ¥æåˆ°é¡è‰²ï¼Œä½†ç•¶æåˆ°æ¯”è¼ƒç¨€å°‘çš„é»‘ç¾Šæ™‚ï¼Œåè€Œæœƒç‰¹åˆ¥èªªåˆ° blackï¼Œé€™æœƒå°è‡´åœ¨åˆ†ææ™‚å¥½åƒé»‘ç¾Šå‡ºç¾çš„é »ç‡æ¯”ç™½ç¾Šå‡ºç¾çš„é »ç‡é«˜)</p>
  </li>
  <li>
    <p>Word vectors reflect social biases in the data used to train them. Ex. including gender and ethnic stereotypes (<a href="https://www.pnas.org/content/115/16/E3635">è«–æ–‡</a>åƒè€ƒ) å¾ˆå¤šè©èªä¸Šçš„ç”¨æ³•å…¶å¯¦å¸¶æœ‰éå¸¸å¤šçš„ç¤¾æœƒåè¦‹å’Œåˆ»æ¿å°è±¡ï¼Œè€Œé€™ä¹Ÿæœƒå°è‡´åˆ†æå‡ºçš„çµæœæœ‰æ‰€åå·®ã€‚</p>
  </li>
</ul>

<hr />

<p><strong>é‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿ</strong></p>

<p>åˆ°ç›®å‰ç‚ºæ­¢ï¼Œçœ‹èµ·ä¾†éƒ½éå¸¸åˆç†ï¼Œé‚£é‚„æœƒæœ‰ä»€éº¼å•é¡Œå‘¢ï¼Ÿ</p>

<p>é€™è£¡æœƒç¢°åˆ°å’Œä¹‹å‰æåˆ°éçš„ï¼ŒçŸ©é™£<em>ç¨€ç–æ€§</em>çš„å•é¡Œã€‚å¦‚æœä»Šå¤© context words æœ‰åè¬å€‹å­—ï¼Œé‚£éº¼ target words å°±æœƒæ˜¯åœ¨åè¬ç¶­åº¦çš„ç©ºé–“çš„ vectorsï¼Œè€Œä¸”å¯èƒ½æœƒæœ‰å¾ˆå¤šçš„å€¼éƒ½æ˜¯ 0 çš„ç‹€æ³ç™¼ç”Ÿã€‚é‚£é€™æ¨£è¦ç”¨ä»€éº¼æ–¹æ³•è§£æ±ºçŸ©é™£çš„ç¨€ç–æ€§ä¸¦ç”¢ç”Ÿ word embeddingsï¼ˆä¹Ÿå°±æ˜¯æ¯å€‹è©çš„å‘é‡ï¼‰ å‘¢ï¼Ÿ</p>

<p>å¾ä¸åŒçš„é¢å‘ä¾†çœ‹å¹¾å€‹å¸¸è¦‹çš„ word embedding æ–¹æ³•ï¼Œ</p>
<ul>
  <li>Learning word embeddings via <em>matrix factorization</em>
    <ol>
      <li>Singular Value Decomposition(SVD)</li>
      <li>Positive Pointwise mutual information(PPMI)</li>
    </ol>
  </li>
  <li>Learning word embeddings via <em>language models</em>
    <ol>
      <li>N-gram</li>
      <li>Neural language models(Ex. word2vec)</li>
    </ol>
  </li>
</ul>

<p>ä»¥ä¸‹å°±è¦ä¾†ä»‹ç´¹é€™å¹¾ç¨®æ–¹æ³•ã€‚</p>

<hr />

<h2 id="matrix-factorization---singular-value-decompositionsvd">Matrix factorization - Singular Value Decomposition(SVD)</h2>

<ul>
  <li>The rows of co-occurrence matrices are long and sparse. Instead, we would like to have word vectors that are short and dense. ç°¡å–®ä¾†èªªï¼Œco-occurrence matrices æœƒæœ‰ç¨€ç–æ€§çš„å•é¡Œã€‚</li>
  <li>One idea is to approximate the co-occurrence matrix by another matrix with fewer columns. Singular Value Decomposition çš„æƒ³æ³•æ˜¯ï¼Œå°‡é€™å€‹åˆé•·åˆè‡­çš„ co-occurrence matrix ç”¨å¦æ¯”è¼ƒå°‘ columns çš„ matrix å–ä»£ã€‚</li>
</ul>

<h3 id="ä»€éº¼æ˜¯-singular-value-decompositionå¥‡ç•°å€¼åˆ†è§£">ä»€éº¼æ˜¯ Singular value decompositionï¼ˆå¥‡ç•°å€¼åˆ†è§£ï¼‰?</h3>

<p>æ¨è–¦<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸</a></p>
<ul>
  <li><em>Singular value decomposition(SVD)</em> can be applied on any matrix. (ä¸éœ€è¦æ˜¯æ–¹é™£ã€‚æ¯”è¼ƒï¼šPCA(ç‰¹å¾µå€¼åˆ†è§£) ä¹Ÿæ˜¯ä¸€å€‹é™ç¶­çš„æ–¹æ³•ï¼Œä½†å®ƒçš„çŸ©é™£å°±å¿…é ˆè¦æ˜¯æ–¹é™£ã€‚)</li>
</ul>

<p>SVD çš„æ¦‚å¿µå°±æ˜¯ï¼Œä»»ä¸€ä¸€å€‹çŸ©é™£ \(A_{m \times n}\)ï¼Œå®ƒéƒ½å¯ä»¥æ‹†è§£æˆä¸‰å€‹çŸ©é™£ï¼ˆ\(U_{m \times n}, \Sigma_{m \times n}, V^T_{n \times n}\)ï¼‰çš„ç›¸ä¹˜ã€‚</p>

<p>å…¶ä¸­ï¼Œ\(U_{m \times n}\) çš„ columns æ˜¯ <em>Orthonormal</em>ï¼Œè€Œ \(V^T_{n \times n}\) çš„ rows æ˜¯ <em>Orthonormal</em>ï¼Œ\(\Sigma_{m \times n}\) æ˜¯ <em>Diagonal</em>(åªæœ‰å°è§’ç·šæœ‰éè² çš„å€¼ï¼Œä¸”ç”±å¤§åˆ°å°)ã€‚</p>

<p><em>(åœ¨ç·šæ€§ä»£æ•¸ä¸­ï¼Œä¸€å€‹å…§ç©ç©ºé–“çš„æ­£äº¤åŸºï¼ˆorthogonal basisï¼‰æ˜¯å…ƒç´ å…©å…©æ­£äº¤çš„åŸºã€‚ç¨±åŸºä¸­çš„å…ƒç´ ç‚ºåŸºå‘é‡ã€‚ å‡è‹¥ï¼Œä¸€å€‹æ­£äº¤åŸºçš„åŸºå‘é‡çš„æ¨¡é•·éƒ½æ˜¯å–®ä½é•·åº¦1ï¼Œå‰‡ç¨±é€™æ­£äº¤åŸºç‚ºæ¨™æº–æ­£äº¤åŸºæˆ–â€è¦ç¯„æ­£äº¤åŸºâ€ï¼ˆOrthonormal basisï¼‰ã€‚)</em></p>

<p><img src="http://localhost:4000/blog/img/posts/SVD.png" alt="" /></p>

<ul>
  <li>\(\Sigma_{m \times n}\) æœƒæ˜¯ä¸€å€‹é•·å¾—åƒé€™æ¨£çš„çŸ©é™£ï¼Œä¸” \(\sigma_1 \ge \sigma_1 \ge \ldots \ge \sigma_k\)</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Sigma.png" alt="" /></p>

<p>è€Œ \(\sigma_r, ~~where~~1 \le r \le k\) æ˜¯å¥‡ç•°å€¼ï¼ˆsingular valueï¼‰ï¼Œè€Œ r è¶Šå°ä¹Ÿä»£è¡¨äº†è©²å€¼è¶Šé‡è¦ï¼Œæ›å¥è©±èªªï¼Œå«æœ‰è¶Šå¤šè¨Šæ¯ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥åªä¿ç•™ \(\Sigma\) è¼ƒé‡è¦çš„å‰é¢å¹¾è¡Œå¾—åˆ°ä¸€å€‹ç›¸ä¼¼çš„çŸ©é™£ \(A\)ã€‚ç”¨è¼ƒå°çš„å„²å­˜ç©ºé–“å°±å¯ä»¥å¾—åˆ°æ¥è¿‘åŸå§‹çš„çŸ©é™£ \(A\)ã€‚</p>

\[A_{m \times n} \approx U_{m \times r} \times \Sigma_{r \times r} \times V^T_{r \times n}\]

<p>åƒè€ƒ<a href="https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/">ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)</a>çš„åœ–ï¼Œ</p>

<p><img src="http://localhost:4000/blog/img/posts/svd2.jpg" alt="" /></p>

<p>å›åˆ°æˆ‘å€‘çš„ word-embeddingã€‚æˆ‘å€‘å¯ä»¥åˆ©ç”¨ SVD é€²è¡Œå»å™ªåŠé™ç¶­ï¼Œåˆªé™¤ä¸€äº›ä¸é‚£éº¼é‡è¦çš„è¨Šæ¯ï¼Œç”¨ä¾†è§£æ±º Co-occurrence matrix ç¨€ç–æ€§çš„å•é¡Œã€‚</p>

<p>æˆ‘å€‘ä¹Ÿä¸éœ€è¦å†å°‡ç›¸ä¹˜çŸ©é™£ï¼Œç›´æ¥ä½¿ç”¨ ğ‘¼ å°±å¥½ï¼Œæ¯ä¸€åˆ—å°±ä»£è¡¨ä¸€å€‹ target wordã€‚</p>

<ul>
  <li>Each row of the (truncated) matrix ğ‘¼ is a k-dimensional vector that represents the â€˜most importantâ€™ information about a word.</li>
  <li>A practical problem is that computing the singular value decomposition for large matrices is expensive.</li>
</ul>

<p>é€™é‚Šçœ‹ä¸€å€‹<a href="https://www.itread01.com/content/1569742263.html">ä¾‹å­</a>ï¼Œ</p>

<p>ä¸‹åœ–æ˜¯ä¸€å€‹ Co-occurrence matrix \(~A_{m \times n}\)</p>

<p><img src="http://localhost:4000/blog/img/posts/term-document.png" alt="" /></p>

<p>å°‡ä¸Šé¢çš„çŸ©é™£ \(A\) ä½¿ç”¨ SVD åˆ†è§£ã€é™ç¶­ï¼Œåªç•™ä¸‹å‰ä¸‰å€‹ç‰¹å¾µå€¼ã€‚æ¯å€‹ç‰¹å¾µå€¼çš„å¤§å°è¡¨ç¤ºå°æ‡‰ä½ç½®çš„å±¬æ€§å€¼çš„é‡è¦æ€§å¤§å°ï¼Œå·¦å¥‡ç•°çŸ©é™£çš„æ¯ä¸€åˆ—å³ä»£è¡¨æ¯å€‹è©çš„ç‰¹å¾µå‘é‡ï¼Œå³å¥‡ç•°çŸ©é™£çš„æ¯ä¸€è¡Œè¡¨ç¤ºæ¯å€‹æ–‡ä»¶çš„ç‰¹å¾µå‘é‡ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD%20example.png" alt="" /></p>

<p>å–æ¯å€‹å‘é‡å¾Œå…©ç¶­çš„å°æ‡‰å€¼æŠ•å½±åˆ°ä¸€å€‹äºŒç¶­ç©ºé–“ï¼Œå¦‚ä¸‹æ‰€ç¤º</p>

<p><img src="http://localhost:4000/blog/img/posts/SVD%20example%202.png" alt="" /></p>

<p>ä¸Šåœ–ä¸­ï¼Œä¸€å€‹ç´…è‰²çš„é»å°æ‡‰ä¸€å€‹è©ï¼Œä¸€å€‹è—è‰²çš„é»å°æ‡‰ä¸€å€‹æ–‡ä»¶ã€‚ç•¶é€™äº›é»è¢«æŠ•å½±åˆ°ç©ºé–“ä¸­ï¼Œæˆ‘å€‘å¯ä»¥å°é€™äº›è©å’Œæ–‡ä»¶é€²è¡Œåˆ†é¡ï¼Œæ¯”å¦‚èªªstockå’Œmarketå¯ä»¥æ”¾åœ¨ä¸€é¡ï¼Œrealå’Œestateå¯ä»¥æ”¾åœ¨ä¸€é¡ï¼ŒæŒ‰é€™æ¨£çš„åˆ†é¡çµæœï¼Œæˆ‘å€‘å°±å¯ä»¥çŸ¥é“æ–‡ä»¶ä¸­å“ªäº›äº‹ç›¸è¿‘çš„è©ï¼Œæ‰€ä»¥ç•¶ä½¿ç”¨è€…åˆ©ç”¨è©æœå°‹æ–‡ä»¶çš„æ™‚å€™ï¼Œæˆ‘å€‘å°±å¯ä»¥åˆ©ç”¨ç›¸è¿‘çš„è©ï¼ˆåœ¨å‘é‡ç©ºé–“ä¸­ç›¸è¿‘çš„è©ã€è¢«æ­¸ç‚ºåŒä¸€é¡çš„è©ï¼‰é€²è¡Œæª¢ç´¢ï¼Œè€Œä¸æ˜¯åªæ˜¯ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è©æœå°‹ã€‚</p>

<hr />

<h2 id="matrix-factorization---positive-pointwise-mutual-informationppmi">Matrix factorization - Positive Pointwise mutual information(PPMI)</h2>

<h3 id="pointwise-mutual-informationpmi">Pointwise mutual information(PMI)</h3>

<ul>
  <li>
    <p>Raw counts favour pairs that involve very common contexts. 
<br />
E.g. the cat, a cat will receive higher weight than cute cat, small cat.</p>
  </li>
  <li>
    <p>We want a measure that favours contexts in which the target word occurs more often than other words.</p>
  </li>
  <li>
    <p>A suitable measure is pointwise mutual information (PMI):</p>
  </li>
</ul>

\[PMI(x, y) = log \frac{P(x, y)}{P(x) \times P(y)}\]

<p>ç°¡å–®ä¾†èªªï¼Œæˆ‘å€‘å¯ä»¥ç”¨ PMI å…¬å¼ä¾†çœ‹å…©å€‹å­—ä¹‹é–“çš„é—œä¿‚ã€‚</p>

<p>ç¾åœ¨æˆ‘å€‘æŠŠ \(x\) çœ‹æˆæˆ‘å€‘çš„ target wordï¼Œ\(y\) çœ‹æˆæˆ‘å€‘çš„ context wordï¼Œ</p>

<ul>
  <li>We want to use PMI to measure the associative strength between a word \(w\) and a context \(c\) in a data set \(D\):</li>
</ul>

\[PMI(w, c) = log \frac{P(w, c)}{P(w) \times P(c)} = log \frac{\#(w, c)/|D|}{\#(w)/|D| \cdot \#(c)/|D|} = log \frac{\#(w,c) \cdot |D|}{\#(w) \cdot \#(c)}\]

<p>ä½†æ ¹æ“šä¸Šé¢çš„å…¬å¼ï¼Œæœƒç™¼ç¾ä¸€å€‹å•é¡Œï¼ŒPMI is infinitely small for unseen wordâ€“context pairs, and undefined for unseen target words. (å¦‚æœ \(w\) å’Œ \(c\) ä¸¦æ²’æœ‰å…±åŒå‡ºç¾éï¼Œå†å– logï¼Œæ•´å€‹å€¼æœƒè®Šæˆ -Inf)</p>

<p>æ‰€ä»¥é€™æ™‚å€™å°±æœ‰äº† <strong>Positive Pointwise mutual information(PPMI)</strong>ã€‚</p>

<ul>
  <li>In positive pointwise mutual information (PPMI), all negative and undefined values are replaced by zero:</li>
</ul>

\[ô°œô°œô°ô°ô°ô°‚ô° PPMI(w, c) = max(PMI(w, c), 0)\]

<ul>
  <li>PPMI assigns high values to rare events, it is advisable to apply a count threshold or smooth the probabilities.</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Computing%20PPMI%20on%20a%20word%E2%80%93context%20matrix.png" alt="" /></p>

<p>çœ‹ä¸€å€‹<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">ä¾‹å­</a>ï¼Œ</p>

<p>ä¸‹åœ–æ˜¯ä¸€å€‹ Co-occurrence matrixï¼Œåˆ—æ˜¯ target wordsï¼Œè¡Œæ˜¯ context words
<img src="http://localhost:4000/blog/img/posts/PMI%20example.png" alt="" /></p>

<p>å‡è¨­é€™ç¯‡æ–‡ç« ç¸½å…±åªæœ‰ 19 å€‹å­—ï¼Œé€™è£¡æˆ‘å€‘è¨ˆç®— x = informationï¼Œy = data çš„ PMI å€¼ï¼Œ
<img src="http://localhost:4000/blog/img/posts/PMI%20example%202.png" alt="" /></p>

<p>æ ¹æ“šåŒæ¨£çš„æ–¹å¼å¯ä»¥æ±‚å‡ºæ‰€æœ‰ target words å°æ‡‰çš„ context words çš„ PMI å€¼ã€‚</p>

<hr />

<h2 id="language-models">Language models</h2>

<ul>
  <li>A <em>probabilistic language model</em> is a probability distribution over sequences of words in some language.</li>
  <li>Recent years have seen the rise of <em>neural language models</em>, which are based on distributed representations of words.</li>
  <li>By the chain rule, the probability of a sequence of ğ‘ words can be computed using conditional probabilities as</li>
</ul>

\[p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_1 \ldots w_{k-1})\]

<ul>
  <li>To make probability estimates more robust, we can approximate the full history \(w_1 \ldots w_N\) by the last few words (é¦¬å¯å¤«éŠ):</li>
</ul>

\[p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_{k-n+1} \ldots w_{k-1})\]

<hr />

<h2 id="language-models---n-gram-models">Language models - N-gram models</h2>

<ul>
  <li>
    <p>An n-gram is a contiguous sequence of n words or characters. 
<br />
E.g. unigram (Text), bigram (Text Mining), trigram (Text Mining course)</p>
  </li>
  <li>
    <p>An n-gram model is a language model defined on n-grams â€“ â€¨a probability distribution over sequences of n words.</p>
  </li>
  <li>
    <p>n-gram æ˜¯ä¸€ç¨®èªè¨€æ©Ÿç‡æ¨¡å‹ã€‚ä¸€å¥è©±å‡ºç¾çš„æ©Ÿç‡æ˜¯ä¸€å€‹è¯åˆæ¨¡å‹ã€‚å¦‚æœä¸€å€‹è©çš„å‡ºç¾åªè€ƒæ…®å‰é¢ä¸€å€‹å­—ï¼Œé‚£å°±æ˜¯ bi-gramï¼›å¦‚æœä¸€å€‹è©çš„å‡ºç¾è€ƒæ…®å‰é¢å…©å€‹å­—ï¼Œé‚£å°±æ˜¯ tri-gramã€‚</p>
  </li>
</ul>

<p><strong>Formal definition of an n-gram model</strong></p>

<ul>
  <li>\(n\): the modelâ€™s order (1 = unigram, 2 = bigram, â€¦)</li>
  <li>\(V\): a set of possible words (character); the vocabulary</li>
  <li>\(P(w\mid u)\): a probability that specifies how likely it is to observe the word \(w\) after the context 
<br />(n âˆ’ 1)-gram \(u\)</li>
</ul>

<p><strong>Unigram model</strong></p>

<p><em>n = 1</em> ä¸è€ƒæ…®å‰é¢å‡ºç¾çš„å­—ã€‚
<img src="http://localhost:4000/blog/img/posts/Unigram%20model.png" alt="" /></p>

<p>Thus contexts are empty.</p>

<p><strong>MLE of unigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/MLE%20of%20unigram%20probabilities.png" alt="" /></p>

<hr />

<p><strong>Bigram models</strong></p>

<p><em>n = 2</em> è€ƒæ…®å‰é¢å‡ºç¾çš„ä¸€å€‹å­—ã€‚</p>

<p><img src="http://localhost:4000/blog/img/posts/Bigram%20models.png" alt="" /></p>

<p>Thus contexts are unigrams.</p>

<p><strong>Estimating bigram probabilities</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Estimating%20bigram%20probabilities.png" alt="" /></p>

<p><strong>Example</strong>
(<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">source</a>)
<img src="http://localhost:4000/blog/img/posts/bigram%20example.png" alt="" /></p>

<hr />

<p><strong>Smoothing</strong></p>

<p>ç•¶åœ¨è¨ˆç®— bigram æ™‚å¯èƒ½æœƒç¢°åˆ°å…©å€‹å­—å®Œå…¨æ²’æœ‰ç›¸é„°çš„ç‹€æ³ï¼Œé€™æœƒå°è‡´ç®—å‡ºä¾†çš„æ©Ÿç‡ç­‰æ–¼ 0ã€‚ï¼ˆå¦‚ä¸‹åœ–ï¼‰</p>

<p><img src="http://localhost:4000/blog/img/posts/bigram%20smoothing.png" alt="" /></p>

<p>é€™ç¨®æ™‚å€™å°±éœ€è¦ç”¨åˆ°ï¼Œsmoothingã€‚</p>

<p><strong>Smoothing methods</strong></p>
<ul>
  <li>Additive smoothing</li>
  <li>Good-Turing estimate</li>
  <li>Jelinek-Mercer smoothing (interpolation)</li>
  <li>Katz smoothing (backoff)</li>
  <li>Witten-Bell smoothing</li>
  <li>Absolute discounting</li>
  <li>Kneser-Ney smoothing</li>
</ul>

<p>ä¸Šé¢çš„ç‹€æ³ç¢°åˆ°çš„æ˜¯ï¼Œâ€CHERâ€ å¾Œé¢æ²’æœ‰å‡ºç¾ â€œREADâ€ çš„ç‹€æ³ï¼Œè€Œå°è‡´æ©Ÿç‡ç­‰æ–¼0ï¼Œä½†å¦‚æœç¾åœ¨æ˜¯ â€œCHERâ€ é€™å€‹å­—å¾æœªå‡ºç¾åœ¨è³‡æ–™é›†ä¸­å‘¢ï¼Ÿé€™ç¨®ç‹€æ³æ™‚ï¼Œsmoothing ä¾¿æ´¾ä¸ä¸Šç”¨å ´äº†ã€‚</p>

<ul>
  <li>In addition to unseen words, a new text may even contain <em>unknown words</em>. For these, smoothing will not help.</li>
</ul>

<p><strong>Unknown words</strong>
<br />
å»ºç«‹ä¸€å€‹ token <UNK>ï¼Œå¦‚æœæ˜¯ unknown words å°±ç”¨ <UNK> ç•¶ä½œæ™®é€šçš„å–®è©è™•ç†ã€‚</UNK></UNK></p>

<ul>
  <li>A simple way to deal with this is to introduce a special word type <em>UNK</em>, and to smooth it like any other word type in the vocabulary.</li>
  <li>When we compute the probability of a document, then we first replace every unknown word with <em>UNK</em>.</li>
</ul>

<hr />

<h2 id="language-models---neural-networks-as-language-models">Language models - Neural networks as language models</h2>

<h3 id="advantages-of-neural-language-models">Advantages of neural language models</h3>
<ul>
  <li>Neural models can achieve better perplexity than probabilistic models, and scale to much larger values of n.</li>
  <li>Words in different positions share parameters, making them share statistical strength. (Everything must pass through the hidden layer.)</li>
  <li>The network can learn that in some contexts, only parts of the â€¨n-gram are informative. (implicit smoothing, helps with unknown words)</li>
</ul>

<h3 id="word2vec">word2vec</h3>

<ul>
  <li>word2vec æ˜¯ word embedding çš„ä¸€ç¨®</li>
  <li>
    <p>word2Vec ä¸»è¦æœ‰ CBOW (continuous bag-of-words) å’Œ skip-gram å…©ç¨®æ¨¡å‹</p>
  </li>
  <li>CBOW æ˜¯çµ¦å®šä¸Šä¸‹æ–‡ï¼Œä¾†é æ¸¬è¼¸å…¥çš„å­—è©ï¼›Skip-gram å‰‡æ˜¯çµ¦å®šè¼¸å…¥å­—è©å¾Œï¼Œä¾†é æ¸¬ä¸Šä¸‹æ–‡</li>
</ul>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1">è©å‘é‡ä»‹ç´¹</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics">è‡ªç„¶èªè¨€è™•ç† â€“ Vector Space of Semantics</a>
<br />
<a href="https://zhuanlan.zhihu.com/p/26306795">[NLP] ç§’æ‡‚è¯å‘é‡Word2vecçš„æœ¬è´¨</a>
<br />
<a href="https://www.youtube.com/watch?v=OEJ0wxxLO7M">æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸ - SVD</a>
<br />
<a href="http://www.shuang0420.com/2017/03/21/NLP%20ç¬”è®°%20-%20å†è°ˆè¯å‘é‡/">NLP ç¬”è®° - å†è°ˆè¯å‘é‡</a>
<br />
<a href="https://www.twblogs.net/a/5c7bdd70bd9eee339918b4da">æ©Ÿå™¨å­¸ç¿’ç­†è¨˜ä¹‹äºŒåäºŒâ€”â€”PCAèˆ‡SVD</a>
<br />
<a href="https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/">ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information">è‡ªç„¶èªè¨€è™•ç† â€“ Pointwise Mutual Information</a>
<br />
<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing</a>
<br />
<a href="https://kknews.cc/tech/83yx3qn.html">æ©Ÿå™¨å­¸ç¿’äº”åˆ†é˜ï¼šè‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰çš„N-gramæ¨¡å‹æ˜¯ä»€éº¼ï¼Ÿ</a>
<br />
<a href="https://www.itread01.com/content/1569742263.html">è©å‘é‡(one-hot/SVD/NNLM/Word2Vec/GloVe)</a></p>
 -->
    
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2019/11/Text-Mining-Word-embeddings.html#disqus_thread';
        this.page.identifier = '/2019/11/Text-Mining-Word-embeddings.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2019/11/Text-Mining-Word-embeddings.html#disqus_thread" data-disqus-identifier="/2019/11/Text-Mining-Word-embeddings.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2019/11/Text-Mining-Text-clustering-and-topic-modelling.html">
        [Text Mining]Text clustering and topic modelling
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 21, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 21, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Text Mining">Text Mining</a>
        
      
    </span>

    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<p>åœ¨é€²å…¥ Text clustering å‰æˆ‘æƒ³è¦å…ˆä»‹ç´¹ <strong>Clustering</strong>ã€‚</p>

<h2 id="clustring">Clustring</h2>

<ul>
  <li>
    <p>Clustering (åˆ†ç¾¤ï¼‰æ˜¯ä¸€ç¨® unsupervised learningï¼ˆéç›£ç£å­¸ç¿’ï¼‰</p>
  </li>
  <li>Typical applications
    <ol>
      <li>As a stand-alone tool to get insight into data distribution</li>
      <li>As a preprocessing step for other algorithms</li>
    </ol>
  </li>
  <li>Cluster: a collection of data objectsã€‚ä½†ä»€éº¼æ¨£çš„ data æœƒè¢«æ­¸åœ¨åŒä¸€å€‹ cluster å‘¢ï¼Ÿ é€™å°±æ˜¯ clustering æœ€é‡è¦çš„æ¦‚å¿µï¼Œ<strong>Similar</strong> å’Œ <strong>Dissimilar</strong>ã€‚</li>
  <li><strong>Similar</strong> to one another within the same cluster</li>
  <li><strong>Dissimilar</strong> to the objects in other clusters</li>
</ul>

<p>=&gt; è€Œæåˆ° Similar å’Œ Dissimilar å°±è¦å¿…é ˆè¦æåˆ° distance (or similarity) measuresã€‚æœ‰äº† distanceï¼ˆsimilarityï¼‰æ‰æœ‰è¾¦æ³•å®šç¾© Similar å’Œ Dissimilarã€‚</p>

<h3 id="distancesimilarity">Distanceï¼ˆSimilarityï¼‰</h3>

<ul>
  <li>Distances are normally used to measure the similarity or dissimilarity between two data objects</li>
</ul>

<p>Distances å¯ä»¥æœ‰å¾ˆå¤šç¨®å®šç¾©æ–¹å¼ï¼Œä½†ä¸ç®¡æ€éº¼å®šç¾©ï¼Œéƒ½ä¸€å®šè¦ç¬¦åˆä»¥ä¸‹çš„ propertiesã€‚</p>
<ul>
  <li>\(d(i, j) \ge 0\) (non-negativity)</li>
  <li>\(d(i, i) = 0\) (identity of indiscernibles)</li>
  <li>\(d(i, j) = d(j, i)\) (symmetry)</li>
  <li>\(d(i, j) \le d(i, k) + d(k, j)\) (triangle inequality)</li>
</ul>

<p><strong>Minkowski distance</strong></p>

<p>\(d(i, j) = \sqrt[q]{(|x_{i_1} - x_{j_1}|^q + |x_{i_2} - x_{j2}|^q + \ldots + |x_{i_p} - x_{j_p}|^q)}\)
, q is a positive integer</p>

<ul>
  <li>If q = 1, d is Manhattan distance</li>
  <li>If q = 2, d is Euclidean distance</li>
</ul>

<p><strong>Binary Variables</strong></p>

<ul>
  <li>symmetric binary variables: both states are equally important; 0/1</li>
  <li>asymmetric binary variables: one state is more important than the other (e.g. outcome of disease test); 1 is the important state, 0 the other</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Contingency%20tables%20for%20Binary%20Variables.png" alt="PMF_two coins.png" /></p>

<ul>
  <li>Accuracy for symmetric binary variables</li>
</ul>

\[sim(i, j) = \frac{a+d}{a+b+c+d}\]

<ul>
  <li>Jaccard similarity for asymmetric binary variables</li>
</ul>

\[sim(i, j) = \frac{a}{a+b+c}\]

<hr />

<h2 id="text-clustering">Text clustering</h2>

<p><img src="http://localhost:4000/blog/img/posts/Conceptual%20framework%20for%20text%20mining.png" alt="PMF_two coins.png" /></p>

<p>ä¸Šåœ–æ˜¯ Conceptual framework for text miningï¼Œè€Œé€™ç¯‡è¦ä»‹ç´¹çš„æ˜¯ Clustering å’Œ Topic Analysisã€‚</p>

<ul>
  <li>
    <p><strong>Text clustering</strong> is the task of grouping similar texts together. What is considered â€˜similarâ€™ depends on the application.</p>
  </li>
  <li>
    <p>Clustering is a central tool in exploratory data analysis, where it can help us to get insights into the distribution of a data set.</p>
  </li>
  <li>
    <p>Clustering is also useful as a pre-processing technique in knowledge-focused applications. Example: Brown clustering</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/The%20standard%20text%20clustering%20pipeline.png" alt="PMF_two coins.png" /></p>

<hr />

<h2 id="similarity-measures">Similarity measures</h2>

<p>å‰›å‰›å‰é¢æåˆ°çš„ Similarityï¼Œå¦‚ä½•ç”¨åœ¨ text ä¸Šå‘¢ï¼Ÿ</p>

<p><strong>Accuracy for symmetric binary vectors</strong></p>

<p><img src="http://localhost:4000/blog/img/posts/Accuracy%20for%20symmetric%20binary%20vectors.png" alt="PMF_two coins.png" /></p>

<p><strong>Jaccard similarity for asymmetric binary vectors</strong></p>

<hr />

<h2 id="hard-clustering-and-soft-clustering">Hard clustering and soft clustering</h2>

<p><strong>Hard clustering</strong></p>
<ul>
  <li>Each document either belongs to a cluster or not. Ex. Hierarchical clustering(brown clustring), Partitioning clustering(k-means), Density-Based clustering(DBSCAN)</li>
</ul>

<p><strong>Soft clustering</strong></p>
<ul>
  <li>Each document belongs to each cluster to a certain degree. Ex. LDA (topic model)</li>
</ul>

<hr />

<h2 id="an-overview-of-hard-clustering-methods">An overview of hard clustering methods</h2>

<h3 id="hierarchical-clustering">Hierarchical clustering</h3>

<p>This method does not require the number of clusters k as an input, but needs a termination condition</p>

<p><img src="http://localhost:4000/blog/img/posts/Hierarchical%20Clustering.png" alt="" /></p>

<ul>
  <li>
    <p><strong>Agglomerative</strong>: Each document starts in its own cluster. Hierarchy is created by merging pairs of clusters.
å°‡é»å€†å€†åˆä½µï¼Œæœ€å¾Œæ‰€æœ‰çš„é»æœƒå…¨éƒ¨åœ¨åŒä¸€å€‹ cluster è£¡ã€‚é‚£è‡³æ–¼è¦ç”¨ä»€éº¼æ±ºå®šè¦å°‡å…©å€‹é»åˆä½µå‘¢ï¼Ÿé€™æ™‚å€™åˆæœ‰ Linkage criteria ä¾†æ±ºå®šï¼Œeg. Single-link, Complete-link, Average-link ç­‰ç­‰ã€‚ä¸åŒçš„ linkage criteria æœƒå°è‡´ä¸åŒçš„åˆ†ç¾¤çµæœï¼Œå¦‚ä¸‹åœ–ã€‚</p>
  </li>
  <li>
    <p><strong>Divisive clustering</strong>: All documents start in one cluster. Hierarchy is created by splitting clusters recursively.</p>
  </li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/Linkage%20criteria.png" alt="" /></p>

<ul>
  <li><strong>Brown clustring</strong></li>
</ul>

<h3 id="partitioning-clustering">Partitioning clustering</h3>
<ul>
  <li>
    <p><strong>K-means</strong></p>
  </li>
  <li>
    <p>Issues with the k-means algorithm</p>
    <ol>
      <li>The k-means algorithm always converges, but there is no guarantee that it finds a global optimum. (Solution: random restarts)</li>
      <li>The number of clusters needs to be specified in advance, or chosen based on heuristics and cross-validation. (Example: elbow method)</li>
      <li>The k-means algorithm is not good at handling outliers â€“ every document will eventually belong to some cluster.</li>
      <li>K-means is restricted to clusters with convex shapes =&gt; Density-Based clustering</li>
    </ol>
  </li>
</ul>

<h3 id="density-based-clustering">Density-Based clustering</h3>

<ul>
  <li>The basic idea behind density-based algorithms is that different regions of the vector space can be more or less densely populated.</li>
  <li>Under this view, clusters can take any shape; they are not constrained to convex clusters as in k-means.</li>
</ul>

<p><strong>Directly density-reachable</strong></p>

<ul>
  <li>DBSCAN</li>
</ul>

<hr />

<h2 id="evaluation-of-hard-clustering">Evaluation of hard clustering</h2>

<h3 id="intrinsic-and-extrinsic-evaluation">Intrinsic and extrinsic evaluation</h3>
<ul>
  <li>In <strong>intrinsic</strong> evaluation, a clustering is evaluated based on internal measures such as coherence and separation. <em>Are documents in the same cluster similar? Are clusters well-separated?</em></li>
  <li>In <strong>extrinsic</strong> evaluation, a clustering is evaluated based on data that was not used for the clustering, such as known class labels. <em>cluster purity, Rand index</em></li>
</ul>

<h3 id="rand-index">Rand index</h3>

<p>å‡è¨­ä¸€å€‹é›†åˆä¸­æœ‰Nç¯‡æ–‡ç« 
ä¸€å€‹é›†åˆä¸­æœ‰N(N-1)/2å€‹é›†åˆå°
TPï¼šåŒä¸€é¡çš„æ–‡ç« è¢«åˆ†åˆ°åŒä¸€å€‹ç°‡
TNï¼šä¸åŒé¡çš„æ–‡ç« è¢«åˆ†åˆ°ä¸åŒç°‡
FPï¼šä¸åŒé¡çš„æ–‡ç« è¢«åˆ†åˆ°åŒä¸€å€‹ç°‡
FNï¼šåŒä¸€é¡çš„æ–‡ç« è¢«åˆ†åˆ°ä¸åŒç°‡
Rand Indexåº¦é‡çš„æ­£ç¢ºçš„ç™¾åˆ†æ¯”
RI = ï¼ˆTP+TNï¼‰/ï¼ˆTP+FP+FN+TNï¼‰</p>

<hr />

<h2 id="topic-models">Topic models</h2>
<ul>
  <li>A topic model is a statistical model for representing the abstract topics that are expressed in a collection of documents.</li>
  <li>Topic models are examples of soft clustering techniques â€“ each document belongs to each cluster (topic) to a certain degree.</li>
</ul>

<h3 id="latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</h3>

<p>LDAæœ‰å…©å€‹åŸå‰‡ï¼Œ</p>
<ol>
  <li>æ¯å€‹ domcuments æ˜¯ç”±å¤šå€‹ Topic çµ„æˆï¼ˆeach document belongs to each cluster (topic) to a certain degreeï¼‰</li>
  <li>æ¯å€‹ä¸»é¡Œæœƒæœ‰ä¸åŒçš„ terms ä¾†æè¿°ï¼Œä¸”åŒæ¨£å°è©å¯ä»¥åŒæ™‚å‡ºç¾åœ¨ä¸åŒçš„ä¸»é¡Œã€‚</li>
</ol>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models.png" alt="" /></p>

<p><img src="http://localhost:4000/blog/img/posts/Topic%20models2.png" alt="" /></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://www.ida.liu.se/~732A75/info/courseinfo.en.shtml">732A75 Advanced Data Mining</a>
<br />
<a href="http://cpmarkchang.logdown.com/posts/238079-natural-language-processing-brown-clustering">è‡ªç„¶èªè¨€è™•ç† â€“ Brown Clustering</a>
<br />
<a href="https://medium.com/@tengyuanchang/ç›´è§€ç†è§£-lda-latent-dirichlet-allocation-èˆ‡æ–‡ä»¶ä¸»é¡Œæ¨¡å‹-ab4f26c27184">ç›´è§€ç†è§£ LDA (Latent Dirichlet Allocation) èˆ‡æ–‡ä»¶ä¸»é¡Œæ¨¡å‹</a>
<br />
<a href="https://www.twblogs.net/a/5b8c83e12b717718833363ae">èšé¡è©•åƒ¹æŒ‡æ¨™ Rand Index,RI,Recall,Precision,F1</a></p>
 -->
    
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2019/11/Text-Mining-Text-clustering-and-topic-modelling.html#disqus_thread';
        this.page.identifier = '/2019/11/Text-Mining-Text-clustering-and-topic-modelling.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2019/11/Text-Mining-Text-clustering-and-topic-modelling.html#disqus_thread" data-disqus-identifier="/2019/11/Text-Mining-Text-clustering-and-topic-modelling.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2019/11/Text-Mining-Text-classification.html">
        [Text Mining]Text classification
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on November 13, 2019</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">November 13, 2019</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Text Mining">Text Mining</a>
        
      
    </span>

    </div>
    <!-- <p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>

<hr />

<h2 id="text-classification">Text classification</h2>

<ul>
  <li>Text classification is the task of categorising text documents into predefined classes.</li>
</ul>

<h2 id="evaluation-of-text-classifiers">Evaluation of text classifiers</h2>

<p>æœ€ç°¡å–®çš„æª¢è¦–é æ¸¬çµæœå¥½å£çš„æ–¹æ³•å°±æ˜¯å°‡ predict å‡ºä¾†çš„é¡åˆ¥èˆ‡çœŸå¯¦çš„é¡åˆ¥åšæ¯”è¼ƒã€‚ï¼ˆé æ¸¬å¿…é ˆè¦åœ¨ test dataï¼Œæ›å¥è©±èªªä¹‹å‰ä¸¦æ²’æœ‰åƒèˆ‡ä»»ä½• training çš„éç¨‹ã€‚é€™é»åœ¨åšæ‰€æœ‰ Machine Learning çš„æ–¹æ³•éƒ½å¾ˆé‡è¦ï¼Œåœ¨åš model æ¸¬è©¦å‰ä¸è¦ç¢° test dataã€‚ï¼‰</p>

<h3 id="accuracy">Accuracy</h3>
<p>The accuracy of a classifier is the proportion of documents for which the classifier predicts the gold-standard class:</p>

\[\textrm{accuarcy} = \frac{\textrm{number of correctly classified documents}}{\textrm{number of all documents}}\]

<h3 id="accuracy-and-imbalanced-data-sets">Accuracy and imbalanced data sets</h3>

<p>ä¸Šé¢çš„ accuracy çœ‹èµ·ä¾†éå¸¸åˆç†å•Šï¼Œå»è¨ˆç®—åˆ†é¡æ­£ç¢ºçš„æ¯”ä¾‹ä¾†åˆ¤æ–·é€™å€‹åˆ†é¡å™¨æ˜¯å¦é æ¸¬æº–ç¢ºã€‚ä½†ï¼Œå¦‚æœå…¶å¯¦è³‡æ–™æœ¬èº«çš„é¡åˆ¥ä¸¦ä¸å¹³å‡å‘¢ï¼Ÿ</p>

<p><img src="http://localhost:4000/blog/img/posts/imbalanced%20data%20sets.png" alt="PMF_two coins.png" /></p>

<p>æ ¹æ“šä¸Šåœ–ï¼Œæˆ‘å€‘åªè¦æŠŠæ‰€æœ‰è³‡æ–™éƒ½çŒœç¶ è‰²çš„ classï¼Œé€™æ¨£ accuracy å°±èƒ½æœ‰ 80%ã€‚å¾é€™å€‹ä¾‹å­å¯ä»¥çŸ¥é“ï¼Œ</p>

<ul>
  <li>
    <p>Evaluation measures are no absolute measures of performance. å¦‚æœä»Šå¤©å¾—åˆ° accuracy æ˜¯ 80% æˆ‘å€‘ä¸¦ç„¡æ³•ç¢ºå®šé€™æ¨£çš„æº–ç¢ºç‡çš„å¥½å£ï¼Œè¦æ ¹æ“šæ¯å€‹å•é¡Œå»åˆ¤æ–·ã€‚</p>
  </li>
  <li>
    <p>Instead, we should ask for a classifierâ€™s performance relative to other classifiers, or other points of comparison. E.g.â€™Logistic Regression has a higher accuracy than Naive Bayes.â€™</p>
  </li>
  <li>
    <p>When other classifiers are not available, a simple baseline is to always predict the most frequent class in the training data.</p>
  </li>
</ul>

<h2 id="precision-and-recall">Precision and recall</h2>

<ul>
  <li>
    <p>Precision and recall â€˜zoom inâ€™ on how good a system is at identifying documents of a specific class.</p>
  </li>
  <li>
    <p>Precision is the proportion of correctly classified documents among all documents for which the system predicts class.</p>
  </li>
</ul>

\[\textrm{precision} = \frac{\textrm{# true positives}}{\textrm{# true positives + # false positives}}\]

<ul>
  <li>Recall is the proportion of correctly classified documents among all documents with gold-standard class.</li>
</ul>

\[\textrm{recall} = \frac{\textrm{# true positives}}{\textrm{# true positives + # false negatives}}\]

<h2 id="f1-measure">F1-measure</h2>
<p>A good classifier should balance between precision and recall. â€¨</p>

\[\textrm{F1} = \frac{2 \cdot \textrm{precision} \cdot \textrm{recall}}{\textrm{precision + recall}}\]

<hr />

<h2 id="naive-bayes-classifier">Naive Bayes classifier</h2>

<h3 id="bayes-theorem">Bayesâ€™ theorem</h3>

<p>We know that \(C\) is classes, \(x_i, i = 1, \cdots,n\) is features. Using Bayesâ€™ theorem, the conditional probability can be decomposed as</p>

\[p(C|x_1,..., x_n) = \frac{p(C)~p(x_1, \cdots,x_n|C)}{p(x_1, \cdots,x_n)}\]

<p>ä¹Ÿå°±æ˜¯ï¼Œ</p>

\[\textrm{posterior} = \frac{\textrm{prior} \times \textrm{likelihood}}{\textrm{evidence}}\]

<p>æ ¹æ“šä¸Šå¼ï¼Œæˆ‘å€‘å¯ä»¥å°‡åˆ†æ¯è¦–ç‚ºå¸¸æ•¸ï¼Œå› ç‚º features \(x_i, i = 1, \cdots,n\) çš„å€¼æ˜¯çµ¦å®šçš„ï¼Œä¸”èˆ‡ \(C\) ç„¡é—œï¼Œæ‰€ä»¥å¯ä»¥å¾—åˆ°</p>

\[\begin{align}
p(C, x_1, \cdots,x_n) &amp; = p(C)~p(x_1,\cdots,x_n|C) \\
&amp; \propto p(C)~p(x_1|C)~p(x_2,\cdots,x_n|C,x_1) \\
&amp; \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3,\cdots,x_n|C,x_1,x_2) \\ 
&amp; \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3|C, x_1, x_2)~p(x_4,\cdots,x_n|C,x_1,x_2,x_3) \\
&amp; \propto \cdots\\
&amp; \propto p(C)~p(x_1|C)~p(x_2|C, x_1)~p(x_3|C, x_1, x_2) \cdots p(x_n|C,x_1,x_2,\cdots ,x_n) \\
\end{align}\]

<h3 id="naive-bayes-assumption">Naive Bayes assumption</h3>

<p>Naive Bayes å‡è¨­ 
\(p(x_i|C, x_j) = p(x_i|C), \textrm{for} ~ i \ne j\)</p>

<p>æ ¹æ“š Naive Bayes çš„å‡è¨­ï¼Œå‰é¢çš„å¼å­æˆ‘å€‘å¯ä»¥å¯«æˆï¼Œ</p>

\[\begin{align}
p(C|x_1,..., x_n) &amp; \propto p(C, x_1, \cdots,x_n) \\
&amp; \propto p(C)~p(x_1|C)~p(x_2|C)~p(x_3|C) \cdots p(x_n|C) \\
&amp; \propto p(C)~\prod_{i=1}^n p(x_i|C)
\end{align}\]

<p>æ ¹æ“šä¸Šé¢çš„æ¨å°éç¨‹ï¼Œæˆ‘å€‘å¯ä»¥å¾—åˆ°ï¼Œ</p>

\[p(C|x_1,..., x_n) = \frac{1}{Z}~p(C)~\prod_{i=1}^n p(x_i|C)\]

<h3 id="naive-bayes-classifer">Naive Bayes classifer</h3>

<p>è€Œ Naive Bayes classifer å°±æ˜¯å–å„å€‹åˆ†é¡ \(C_m, m = 1, \cdots, k\) 
ä¸­ \(p(C|x_1,..., x_n)\) å€¼æœ€å¤§çš„ç‚ºæœ€å¾Œçš„åˆ†é¡çµæœã€‚æ›å¥è©±èªªï¼Œæˆ‘å€‘å¯ç”¨é€™æ¨£çš„å¼å­è¡¨ç¤º</p>

\[C_m = \mathop{\arg\max}_C p(C|x_1,..., x_n) =  \mathop{\arg\max}_C p(C)~\prod_{i=1}^n p(x_i|C)\]

<p>è€Œ \(C_m\) å°±æ˜¯æœ€å¾Œçš„åˆ†é¡çµæœã€‚</p>

<h3 id="two-classic-naive-bayes-variants-for-text">Two Classic Naive Bayes Variants for Text</h3>

<ol>
  <li>Multinomial Naive Bayes
    <ul>
      <li>Data follows a multinomial distribution (å¤šé …åˆ†å¸ƒ)</li>
      <li>Each feature values is a count (word occurrence counts, TF-IDF weighting, â€¦)</li>
    </ul>
  </li>
  <li>Bernoulli Naive Bayes
    <ul>
      <li>Data follows a multivariate Bernoulli distribution</li>
      <li>Each feature is binary (word is present / absent)</li>
    </ul>
  </li>
</ol>

<hr />

<p><strong>Lab:</strong> <a href="https://github.com/shihs/732A92-TextMining/blob/master/Lab1/TM-L2.ipynb">Text classification Lab</a></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a>
<br />
<a href="https://www.ycc.idv.tw/confusion-matrix.html">å¦‚ä½•è¾¨åˆ¥æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„å¥½å£ï¼Ÿç§’æ‡‚Confusion Matrix</a>
<br />
<a href="https://zh.wikipedia.org/wiki/æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨">wikipeida - å–®ç´”è²æ°åˆ†é¡å™¨</a></p>

 -->
    
    <div class="post-excerpt"><p>This post is based on the <a href="https://www.ida.liu.se/~732A92/index.en.shtml">732A92 Texting Mining</a> course, given by Marco Kuhlmann at LiU in 2019.</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2019/11/Text-Mining-Text-classification.html#disqus_thread';
        this.page.identifier = '/2019/11/Text-Mining-Text-classification.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2019/11/Text-Mining-Text-classification.html#disqus_thread" data-disqus-identifier="/2019/11/Text-Mining-Text-classification.html">0 Comments</a>
      </span>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page6">Older</a>
  
  
    <!--  -->
      <a class="pagination-item newer" href="/blog/page4">Newer</a>
    <!--  -->
  
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
