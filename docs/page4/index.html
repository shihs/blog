<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>M.C. Shih &middot; BLOG</title>  
  

  <!-- CSS -->
  <link rel="stylesheet" href="/blog/public/css/poole.css">
  <link rel="stylesheet" href="/blog/public/css/syntax.css">
  <link rel="stylesheet" href="/blog/public/css/hyde.css">
  <link rel="stylesheet" href="/blog/public/css/custom.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.css">
  <link rel="stylesheet" href="/blog/public/css/fontawesome-all.min.css">
  <!-- <link href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" rel="stylesheet"> -->


  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="16x16" href="/blog/public/apple-touch-icon-16-precomposed.png">
  <link rel="apple-touch-icon" sizes="24x24" href="/blog/public/apple-touch-icon-24-precomposed.png">
  <link rel="apple-touch-icon" sizes="32x32" href="/blog/public/apple-touch-icon-32-precomposed.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/blog/public/apple-touch-icon-48-precomposed.png">
  <link rel="apple-touch-icon" sizes="57x57" href="/blog/public/apple-touch-icon-57-precomposed.png">
  <link rel="apple-touch-icon" sizes="64x64" href="/blog/public/apple-touch-icon-64-precomposed.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/blog/public/apple-touch-icon-72-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="114x114" href="/blog/public/apple-touch-icon-114-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="120x120" href="/blog/public/apple-touch-icon-120-precomposed.png"> 
  <link rel="apple-touch-icon" sizes="144x144" href="/blog/public/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/blog/public/apple-touch-icon-152-precomposed.png">
  <link rel="apple-touch-icon" sizes="512x512" href="/blog/public/apple-touch-icon-512-precomposed.png">
  <link rel="shortcut icon" href="/blog/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/blog/blog/atom.xml">
  <!-- 數學符號 -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>


  <body>
  	
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/blog/">
          M.C. Shih
        </a>
        <img src="/blog/img/headshot.png" alt="Author Image" class="headshot">
      </h1>
      <p class="lead">Just for learning.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/blog/">Blog</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/about">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/archive">Archives</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/category">Category</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/search">Search</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>
    <div class="links">
      <ul>
        <li><a href="https://shihs.github.io" class="homepage" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-home-6-240.png"></a></li>
        <li><a href="https://github.com/shihs" class="github" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-github-3-240.png"></a></li>
        <li><a href="https://www.linkedin.com/in/min-chun-shih-6647779a/" class="linkedin" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-linkedin-3-240.png"></a></li>
        <li><a href="https://www.instagram.com/itakephotos_tw/" class="instagram" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-instagram-8-240.png"></a></li>
        <li><a href="https://www.flickr.com/photos/mcshihs/" class="flickr" target="_blank"><img src="/blog/img/link%20icon/iconmonstr-flickr-3-240.png"></a></li>
      </ul>
    </div>
    

    <p class = "rights">&copy; 2023. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts" id="begin">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2020/02/NLP-CNN-in-NLP.html">
        [NLP]CNN in NLP
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on February 2, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">February 2, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#NLP">NLP</a>
        &nbsp;
      
        <a target="_blank" href="/blog//category/#Machine Learning">Machine Learning</a>
        &nbsp;
      
        <a target="_blank" href="/blog//category/#Neural Network">Neural Network</a>
        
      
    </span>

    </div>
    <!-- <p><a href="https://shihs.github.io/blog/machine%20learning/neural%20network/2020/02/01/Machine-Learning-Convolution-Neural-N卷積神經網路/">前一篇</a>介紹了 CNN 的概念，這篇要來介紹 CNN 如何運用在 NLP 中。</p>

<hr />

<p>Yoon Kim 在 2014 年發表論文 <a href="https://arxiv.org/abs/1408.5882">Convolutional Neural Networks for Sentence Classification</a> 提出將 CNN 運用在 NLP。</p>

<p><img src="http://localhost:4000/blog/img/posts/CNN%20in%20NLP.png" alt="" /></p>

<p>上圖就是 Yoon Kim 論文提出的方法概念，和在圖像運用的 CNN 一樣，有 convolution 和 pooling，然後最後是一個 softmax function。</p>

<p>我們先來看第一步，</p>

<h2 id="convolution">Convolution</h2>

<p>在圖片中，我們將圖片看成一個 nxm 的矩陣，每一格都是一個數字，而我們可以使用 filter(kernel) 來獲取想要的特徵。而在 NLP 中，如果想要將文字表示成數字，可以使用 word embedding，例如 word2vect、glove，或是 one-hot encode 的詞彙到索引詞向量表示方法。</p>

<p>上圖中的第一個矩陣是 ‘I love this movie very much!’ 的 word embedding 矩陣，每一列代表一個 word 的向量。總共有七個字，每一個字用一個 5 維的向量表示，所以用一個 7x5 的矩陣表示。</p>

<p><strong>filter</strong></p>

<p>現在知道了輸入的表示法，那在 convolution 中最重要的 filter 要如何使用呢？</p>

<p>一般 CNN 的 filter 會是一個方正矩陣，從左往右再往下滑動，但在 NLP 中，filter 矩陣會和向量的維度一樣，但一次可能滑動兩個字 (2-grams)、三個字 (3-grams)等等，這個稱作 region size，看上圖的第二大行，有紅色系、綠色系、黃色系三種 filter，分別代表 region size 4、3 和 2，且各有兩個，也就是總共有六個 filter。(像這樣移動的方式只有一個方向叫做，<a href="https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf">1D converlution</a>)</p>

<p>整張大圖的第三行，就是輸入矩陣 (channel) 乘上 filter 之後的結果。</p>

<h2 id="pooling">Pooling</h2>

<p>Convolution 的下一步是 pooling，上圖使用的是 max pooling，也就是取最大的值。</p>

<p>pooling 後 flatten 所有值，最後一步 softmax function 便完成了。</p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.youtube.com/watch?v=wNBaNhvL4pg">Simple Deep Neural Networks for Text Classification</a>
<br />
<a href="https://kiseliu.github.io/2016/09/22/understanding-convolutional-neural-networks-for-nlp/">Understanding Convolutional Neural Networks for NLP
</a>
<br />
<a href="https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf">Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences</a></p>
 -->
    
    <div class="post-excerpt"><p><a href="https://shihs.github.io/blog/machine%20learning/neural%20network/2020/02/01/Machine-Learning-Convolution-Neural-N卷積神經網路/">前一篇</a>介紹了 CNN 的概念，這篇要來介紹 CNN 如何運用在 NLP 中。</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2020/02/NLP-CNN-in-NLP.html#disqus_thread';
        this.page.identifier = '/2020/02/NLP-CNN-in-NLP.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2020/02/NLP-CNN-in-NLP.html#disqus_thread" data-disqus-identifier="/2020/02/NLP-CNN-in-NLP.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2020/02/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.html">
        [Neural Network]Convolution Neural Network 卷積神經網路
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on February 1, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">February 1, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Machine Learning">Machine Learning</a>
        &nbsp;
      
        <a target="_blank" href="/blog//category/#Neural Network">Neural Network</a>
        
      
    </span>

    </div>
    <!-- <p>之前寫了兩篇的 CNN 課程筆記，但後來因為太忙沒有繼續看完課程就中斷了 T_T。
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">[Machine Learning]Covolutional Neural Networks(CNN)(1)</a>
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">[Machine Learning]Covolutional Neural Networks(CNN)(2)</a></p>

<p>卷積神經網絡(Convolutional Neural Network)簡稱 CNN，主要應用在影像辨識，這篇想要簡單的說明 CNN 的概念。</p>

<hr />

<p>先看下面兩張 CNN 概念圖</p>

<p><a href="https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/3">圖片來源</a>
<img src="http://localhost:4000/blog/img/posts/CNN%20%E6%A6%82%E5%BF%B5%E5%9C%96%201.png" alt="" /></p>

<p><a href="https://medium.com/jameslearningnote/資料分析-機器學習-第5-1講-卷積神經網絡介紹-convolutional-neural-network-4f8249d65d4f">圖片來源</a>
<img src="http://localhost:4000/blog/img/posts/CNN%20%E6%A6%82%E5%BF%B5%E5%9C%96%202.png" alt="" /></p>

<p>從上圖可以看到整個流程各經過兩次的 Convolution、Pooling 和 Fully Connected，所以其實只要搞懂 <strong>Convolution</strong>、<strong>Pooling</strong> 和 <strong>Fully Connected</strong> 分別在做什麼就可以掌握 CNN 了。</p>

<p>整個流程是，前面的 Convolution 和 Pooling (又稱為 subsampling) 在做的是 feature extraction (特徵擷取)，而 Fully connection 在做的是 classification (分類辨識)。</p>

<hr />

<h2 id="1-convolution-layer-卷積層">1. Convolution Layer 卷積層</h2>

<p>Convolution Layer 由 filter 和激活函數 (ex. Relu) 所構成。</p>

<p>下圖是常見的美圖效果，其實就是使用 filter 達成的
<a href="https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-convolution-neural-network-卷積神經網路-bfa8566744e9">圖片來源</a>
<img src="http://localhost:4000/blog/img/posts/%E7%BE%8E%E5%9C%96.png" alt="" /></p>

<p>卷積 (Convolution) 做的是特徵擷取，而不同的效果在做的就是對不同的特徵做擷取。例如，銳化效果在做的就是強化邊緣的特徵，也就是加強擷取出邊緣的特徵。</p>

<p>那所以卷積到底是如何擷取不同的特徵的呢？
<a href="https://icecreamlabs.com/2018/08/19/3x3-convolution-filters%E2%80%8A-%E2%80%8Aa-popular-choice/">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/Convolution.gif" alt="" /></p>

<p>特徵擷取的方式是使用 Filter (或稱作 Kernel) 來萃取圖片中的特徵，上圖是一個 3x3 window 的 filter，而算法就是像圖那樣移動 (移動的步數是 stride)，將數字相乘後再相加 (詳情可以看之前的<a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">筆記1</a>、<a href="https://shihs.github.io/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">筆記2</a>，包括 padding 和 stride 的概念)。美圖的不同效果就是使用不同的 Filter (Kernel) 做出來的。</p>

<p>下圖是不同的 filter 做出來的效果
<a href="https://icecreamlabs.com/2018/08/19/3x3-convolution-filters%E2%80%8A-%E2%80%8Aa-popular-choice/">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/kernels.png" alt="" /></p>

<p>現在明白了 filter，但再回去看最一開始的流程圖，為什麼 converlution 有三個呢？這是因為使用了三個 filter 去擷取不同的特徵。</p>

<p>看完 filter 再看 Relu。可以看到最上圖圖中有個 Relu，也就是將所有計算出來結果的負值都變為 0。
<a href="https://medium.com/jameslearningnote/資料分析-機器學習-第5-1講-卷積神經網絡介紹-convolutional-neural-network-4f8249d65d4f">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/Relu.png" alt="" /></p>

<p>如果是彩色的圖，通常會有三個 channel (RGB)，也就是一開始輸入的 image 有三個，可以看下圖
<a href="https://zhuanlan.zhihu.com/p/42559190">圖片來源</a></p>

<p><img src="http://localhost:4000/blog/img/posts/three%20images%20convolution.jpg" alt="" /></p>

<h2 id="2-pooling-layer-池化層">2. Pooling Layer 池化層</h2>

<p>pooling 又稱為 subsampling，是為了提取特定區域主要特徵，但又想要減少參數的數量，以防止模型過擬合，常用的是 max pooling，也就是取該區域最大的值，也有 average pooling，也就是取該區域的平均值。
<a href="https://zhuanlan.zhihu.com/p/42559190">圖片來源</a></p>

<p>下圖是取 max pooling，使用 2x2 的 window，且 stride 為 2</p>

<p><img src="http://localhost:4000/blog/img/posts/pooling.jpg" alt="" /></p>

<h2 id="3-fully-connected-layer-全連接層">3. Fully Connected Layer 全連接層</h2>

<p>Fully Connected Layer 主要是進行 Flattening 平坦化，將最後的矩陣轉換成一維，然後再使用傳統的 neural network + softmax()。</p>

<p>參考我之前的<a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">[Neural Network]Neural Network 1</a></p>

<hr />

<p>以上就是 CNN 的流程。</p>

<p>最後提一下，從上面的過程我們可以知道，CNN 整個過程中需要求解的參數是</p>

<ol>
  <li>Convolution layer filters</li>
  <li>Full Connected Layer filters</li>
</ol>

<p><a href="http://doremi2016.logdown.com/posts/2017/01/25/convolutional-neural-networks-cnn">來源</a></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://medium.com/jameslearningnote/資料分析-機器學習-第5-1講-卷積神經網絡介紹-convolutional-neural-network-4f8249d65d4f">[資料分析&amp;機器學習] 第5.1講: 卷積神經網絡介紹(Convolutional Neural Network)
</a>
<br />
<a href="https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-convolution-neural-network-卷積神經網路-bfa8566744e9">[機器學習 ML NOTE]Convolution Neural Network 卷積神經網路</a>
<br />
<a href="https://zhuanlan.zhihu.com/p/42559190">【DL笔记6】从此明白了卷积神经网络（CNN）</a>
<br />
<a href="http://doremi2016.logdown.com/posts/2017/01/25/convolutional-neural-networks-cnn">Convolutional Neural Networks’ (CNN) Backward Propagation</a></p>
 -->
    
    <div class="post-excerpt"><p>之前寫了兩篇的 CNN 課程筆記，但後來因為太忙沒有繼續看完課程就中斷了 T_T。
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/02/25/Machine-Learning-Covolutional-Neural-Networks(CNN)/">[Machine Learning]Covolutional Neural Networks(CNN)(1)</a>
<br />
<a href="https://shihs.github.io/blog/machine%20learning/2019/03/02/Machine-Learning-Covolutional-Neural-Networks(CNN)(2)/">[Machine Learning]Covolutional Neural Networks(CNN)(2)</a></p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2020/02/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.html#disqus_thread';
        this.page.identifier = '/2020/02/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2020/02/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.html#disqus_thread" data-disqus-identifier="/2020/02/Machine-Learning-Convolution-Neural-N%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2020/01/Neural-Network-Neural-Network-2.html">
        [Neural Network]Neural Network 2 - Gradient descent
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on January 23, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">January 23, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Neural Network">Neural Network</a>
        
      
    </span>

    </div>
    <!-- <p><a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">上一篇</a>說明了神經網路的基本原理，這篇我們要講解神經網路是如何學習的 - Gradient descent。</p>

<p>這篇是<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">Neural networks</a>的課程筆記。</p>

<hr />

<p>在 Machine Learning 裡我們使用 training data 訓練得到一個 model，但這「訓練」到底是怎麼訓練的呢？訓練的過程，通常是要找到最小的 cost，也就是最後的結果與真實值差距越小越好。</p>

<p>在 Neural Network 我們要訓練的變數就是<a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">前面一篇</a>提到的 weight 和 bias。一開始我們隨機的給予這些數字，然後在一次次的學習中去改進 weight 和 bias 的值，找到最小的成本函數。</p>

<p>在課程的例子裡，Neural Network function 的 input 是 784 個 pixel 的數字，output 是 10 個數字，0 到 9 的機率，parameters 是 weight 和 bias。但在cost function 中，weight 和 bias 變成了 input，而 output 就是 cost 的值，parameters 則是很多的 training pairs。</p>

<p>我們可以把 cost function 這樣表示，</p>

\[C(w_1, w_2,..., b_1, b_2, ....) = cost\]

<p>cost 的算法是 Neural Network 算出來的值，扣掉正確值然後平方相加。學習過程中，我們將所有 training data 的 cost 相加平均（average cost of all training data），希望這個值越小越好。</p>

<p><img src="http://localhost:4000/blog/img/posts/cost.png" alt="" /></p>

<p>那我們要如何找到最小的 cost 值？</p>

<p>如果 cost function 是一個像下圖的函數圖形，我們知道微分等於 0 可以找到最小值，但這並非一件容易的事，尤其是當我們有很多變數時。</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima.png" alt="" /></p>

<p>所以我們可以用的方法是，先隨機到函數圖形上的一點，然後根據該點的切線斜率來決定該往哪個方向移動可以找到最低點。這樣的方法，可以讓我們到 local minimum，我們無法確認找到的最小是否是 global minimum，因為這取決與一開始的 initial value。</p>

<p><img src="http://localhost:4000/blog/img/posts/local%20minima%202.png" alt="" /></p>

<hr />

<p>在進入 Gradient descent 之前，我想要先介紹一下基礎觀念。以下將會講解，</p>

<ol>
  <li>導函數 Derivative function 與導數 Derivative</li>
  <li>方向導數 Directional derivative</li>
  <li>偏導數 Partial derivative</li>
  <li>梯度 Gradient</li>
</ol>

<h3 id="導數-derivative">導數 Derivative</h3>

<p>切線的斜率稱為「導數」（Derivative），也就是微分。</p>

<p>定義成，</p>

\[m = \text{lim}_{\Delta x \to 0}\frac{f(x+\Delta x) - f(x) }{\Delta x} = f'(x) = \frac{\text{d} f(x)}{\text{d}x}\]

<h3 id="偏導數-partial-derivative">偏導數 Partial derivative</h3>

<p>斜率求的是單變數的微分，而偏導數是在多變數中的某個變數方向的斜率。</p>

<p>以兩個自變數為例， \(z = f(x, y)\)</p>

<p>\(f_x(a, b)\) 為對 \(x\) 的偏導數，定義為</p>

\[f_x(a, b) = \text{lim}_{h \to 0} \frac{f(a + h, b) - f(a, b)}{h}\]

<p>\(f_y(a, b)\) 為對 \(y\) 的偏導數，定義為</p>

\[f_y(a, b) = \text{lim}_{k \to 0} \frac{f(a, b + k) - f(a, b)}{k}\]

<p>也就是對 x 或 y 方向做偏微分。</p>

<h3 id="方向導數-directional-derivative">方向導數 Directional derivative</h3>

<p>斜率求的是單變數的微分，在多變數中要求斜率必須要看是哪個方向的斜率，因為各個方向的斜率都不相同，這時要求的就是方向導數。</p>

<p>假設函數 \(z = f(x, y)\)，在定義域 \(xy\) 平面上有一點 \((x, y)\) 及單位向量 \(\overset{\rightharpoonup}{u} = (u_1, u_2)\)。則曲面 \(z = f(x, y)\) 在 \((a, b)\) 處，沿 \(\overset{\rightharpoonup}{u}\) 的方向的斜率定義為，</p>

\[\text{D}_u f(x_0, y_0)= \text{lim}_{h \to 0} \frac{f(x_0 + h u_1, y_0 + h u_2) - f(x_0, y_0)}{h} = \nabla f \boldsymbol{u}\]

<p>方向導數的意義在於，有助於計算出空間中某一場量 \(f(x, y, z)\) 在某一特定方向 \(\boldsymbol{u}\) 之變化率。</p>

<h3 id="梯度-gradient">梯度 Gradient</h3>

<p>梯度的定義就是對各個分量做偏微分。</p>

<p>例如，現在是兩變數函數 \(f(x, y)\)，其梯度 \(\nabla f(x, y)\) 定義為</p>

\[\nabla f(x, y) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)\]

<p>若是變數有三個，則 \(\nabla f(x, y, y)\) 定義為</p>

\[\nabla f(x, y, z) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)\]

<p>Gernal 來看，有 n 個變數的函數 \(f(x_1, x_2, \cdots, x_n)\)，我們可以寫成</p>

\[\nabla f(x_1, x_2, \cdots, x_n) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}\right)\]

<p>梯度 Gradient 的幾何意義，</p>
<ul>
  <li>\(\nabla f\) 表示曲面 \(f(x, y, z) = C\) 之垂直向量，如下圖所示。</li>
  <li>\(\nabla f(P)\) 表在 \(P\) 點增量最大的方向，也就是 the direction of steepest increase</li>
</ul>

<p><img src="http://localhost:4000/blog/img/posts/gradient.png" alt="" /></p>

<hr />

<h3 id="梯度下降法-gradient-descent">梯度下降法 Gradient descent</h3>

<p>Gradient descent 是一種最佳化方法。</p>

<p>從上一部分，我們知道 P 點 gradeint 表示該點增量最大的方向。現在我們要找 cost function 的最小值，也就是該 function 斜率為 0 的點，當我們今天在 P 點知道哪個方向是會增加最快速的點，那就只要往反方向就會是減少最快速的方向了。也就是 \(-\nabla f(P)\) 是找尋 minimum cost 的方向。</p>

<p>總的來說，梯度下降法 Gradient descent 就是不斷的在某個點 P 算它的 Gradient，然後往 Gradient 的反方向一定，直到找到 local minimum 為止（無法知道是否找到 glocal minimum，會根據 initial values 而定）。</p>

<p>在 neural netword 中，讓這個梯度計算更有效率的方法叫做 <strong>Backpropagation</strong>，而這正是 neural netword 的重點！下一篇繼續介紹 :)</p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/827/mod_resource/content/33/Summary_227.pdf">方向導數(Directional Derivative)之定義與意義</a>
<br />
<a href="https://ocw.chu.edu.tw/pluginfile.php/826/mod_resource/content/31/Summary_226.pdf">純量函數之梯度(Gradient)</a></p>

 -->
    
    <div class="post-excerpt"><p><a href="https://shihs.github.io/blog/neural%20network/2020/01/22/Neural-Network-Neural-Network/">上一篇</a>說明了神經網路的基本原理，這篇我們要講解神經網路是如何學習的 - Gradient descent。</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2020/01/Neural-Network-Neural-Network-2.html#disqus_thread';
        this.page.identifier = '/2020/01/Neural-Network-Neural-Network-2.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2020/01/Neural-Network-Neural-Network-2.html#disqus_thread" data-disqus-identifier="/2020/01/Neural-Network-Neural-Network-2.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2020/01/Neural-Network-Neural-Network.html">
        [Neural Network]Neural Network 1 - The idea
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on January 22, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">January 22, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Neural Network">Neural Network</a>
        
      
    </span>

    </div>
    <!-- <p>在了解 CNN (卷積神經網路, Convolutional neural network) 與 RNN (循環神經網絡, Recurrent neural network) 前，先來認識最簡單的 NN (神經網路, Neural network) Multilayer perceptron。</p>

<p>這篇是<a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">Neural networks</a>的課程筆記。</p>

<hr />

<p>Neural network 啟發自大腦神經，所謂的 Neural 就是 neurons (神經元)，而在 Neural network 裡，neurons 是個介於 0 和 1 之間的數字。</p>

<p>我們將以讓程式辨認手寫數字 0 到 9 為例解說 Neural network。下圖是一個 28*28 像素的手寫數字，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%201.png" alt="" /></p>

<p>每一個像素都是一個 0 到 1 的灰階值，而 Neural network 以這個圖的每個像素，對應到每個神經元作為輸入值，也就是說第一層輸入層共有 28*28 = 784 個神經元。灰階值 0 為黑色，1 為白色。這些在神經元中的數字稱為「激勵值」（activation）。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%202.png" alt="" /></p>

<p>這裡範例使用，第一層為輸入層，中間兩層 hidden layers，最後一層為輸出層。而最後輸出層就是 0 到 9 的數字，每個神經元都有個「激勵值」（activation），而這數字就代表著最後給定可能是這個數字的判定結果。</p>

<p>而中間的 hidden layers 正是神經網路的重點。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%203.png" alt="" /></p>

<p>我們可以想像，將整個數字分解成不同的部分，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%204.png" alt="" /></p>

<p>第一層 hidden layer 是比較小段的部分，而第二層 hidden layer 是由這些小區塊組成比較大塊的區塊。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%205.png" alt="" /></p>

<p>有了這些概念之後，那我們要怎麼從輸入層到第一層的 hidden layer 呢？</p>

<p>現在我們希望，第二層的其中一個神經元可以辨識這個圖有沒有一個邊，如下圖，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%206.png" alt="" /></p>

<p>我們會給這個神經元與輸入層的神經元之間每個連線一個權重（weight），然後將輸入層的神經元乘上每個權重再加總，權重可以是正或負值。</p>

<p>但因為，我們希望神經元的激勵值（activation）是在 0 和 1 之間，所以使用 sigmoid function \(\sigma(x) = \frac{1}{1+e^x}\) 。經過 <a href="https://zh.wikipedia.org/wiki/S函数">sigmoid function</a> 計算，越小的值會越接近 0，越大的值會越接近 1。</p>

<p>但也許並不是每次加總大於零我們就希望給正的灰階值，因此，權重最後再加上個 bias，來調整我們想要給灰階值的值。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%207.png" alt="" /></p>

<p>所有第一層 hidden layers 的神經元都根據這樣的概念，有自己關注的區塊，有自己的權重，再加上自己的 bias。第二層的 hidden layers 神經元也以同樣的概念計算，所以我們總共會有 13,002 個權重加上 bias。</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%208.png" alt="" /></p>

<p>而所謂的 deep learning 的 learning 就是在學習這些權重和 bias，讓電腦去找到正確的值。</p>

<p>上上圖的式子只是其中一個神經元，而我們可以用矩陣表達所有第一層 hidden layers 的神經元。如下圖，</p>

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%209.png" alt="" /></p>

<p>總的來說，我們可以將整個 neural network 看成一個 function。</p>

\[f(a_0, a_1,..., a_{783}) =  \begin{bmatrix}
   y_0 \\
   \vdots \\
   y_9
  \end{bmatrix}\]

<p>最後，其實現在已經不使用 sigmoid function 了，現在主流的 function 用的是 <strong>ReLU</strong>。</p>

\[ReLU(a) = max(0, a)\]

<p><img src="http://localhost:4000/blog/img/posts/neural%20network%2010.png" alt="" /></p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="https://www.youtube.com/watch?v=aircAruvnKk">究竟神經網路是什麼？ 第一章 深度學習</a></p>
 -->
    
    <div class="post-excerpt"><p>在了解 CNN (卷積神經網路, Convolutional neural network) 與 RNN (循環神經網絡, Recurrent neural network) 前，先來認識最簡單的 NN (神經網路, Neural network) Multilayer perceptron。</p>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2020/01/Neural-Network-Neural-Network.html#disqus_thread';
        this.page.identifier = '/2020/01/Neural-Network-Neural-Network.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2020/01/Neural-Network-Neural-Network.html#disqus_thread" data-disqus-identifier="/2020/01/Neural-Network-Neural-Network.html">0 Comments</a>
      </span>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/blog/2020/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96.html">
        [Database]ER Model 實體關聯模型
      </a>
    </h1>

    <div class="post-info">
      <!-- <span class="post-date">Posted by Shihs on January 1, 2020</span> -->
      <i class="fas fa-calendar-alt"></i>
      <span class="post-date">January 1, 2020</span>
      <span class="post-categories">
      <!-- <i class="fas fa-tag"></i> -->
      <!-- <i class="far fa-book"></i> -->
      <i class="fas fa-box"></i>
      
      
        <a target="_blank" href="/blog//category/#Database">Database</a>
        
      
    </span>

    </div>
    <!-- <h2 id="實體關聯模型entity-relationship-model">實體關聯模型（Entity-Relationship Model）</h2>

<p>是Conceptual model（概念塑模）的一環，用來描述<strong>實體</strong>與<strong>實體</strong>之間<strong>關係</strong>的工具。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/ER%20Model.png" alt="ER Model.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://www.csie.sju.edu.tw/cm/course/db/ch03.pdf">ER Model</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>實體（Entity）：是指用以描述真實世界的物件，在關聯式資料庫中為一個「資料表」
    <ul>
      <li>實體至少擁有一個不是鍵（主鍵）的屬性</li>
      <li>一個實體可以含有多個「屬性」(Attribute)用以描述該實體，在關聯式資料庫中，以資料表的「欄位」來表示</li>
      <li>例如：學生、員工、產品等等都是屬於實體</li>
    </ul>
  </li>
  <li>關係（Relationship）：指用來表示「一個實體」與「另一個實體」關聯的方式
    <ul>
      <li>例如：一對一關係、一對多關係、多對多關係</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="實體entity">實體（Entity）</h2>

<p><strong>定義</strong></p>
<ul>
  <li>a ”thing” in the real world with an independent existence</li>
  <li>用來描述實際存在的事物（如:學生），也可以是邏輯抽象的概念（如:課程）</li>
  <li>必須可以被識別，亦即能夠清楚分辨出兩個不同的實體(an independent existence)</li>
  <li>以「名詞」的來命名，不可以是「形容詞」或「動詞」</li>
</ul>

<p><strong>分類</strong></p>
<ul>
  <li>實體(entity)</li>
  <li>弱實體(weak entity)：必須依靠其他實體才能存在，若其依靠的實體消失，則該實體的存在也沒有意義了
    <ul>
      <li>例如：學生家長就是依附在學生實體的弱實體，若學生不存在，則家長也沒有存在的意義</li>
    </ul>
  </li>
</ul>

<p><strong>圖示</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/entity%20diagram.png" alt="entity diagram.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://images.app.goo.gl/auQ9MAQCKttsdFGEA">Entity diagram</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="屬性attribute">屬性(Attribute)</h2>

<p><strong>定義</strong></p>
<ul>
  <li>用來描述實體的性質（Property）（就是關聯式表格的行）
    <ul>
      <li>例如：學生的學號、名字、班級等等</li>
    </ul>
  </li>
</ul>

<p><strong>分類</strong></p>
<ul>
  <li>多值屬性 vs 單值屬性
    <ul>
      <li>多值屬性（Multivalued attributes）：屬性值不只一個時，我們稱該屬性為<em>多值屬性</em>
        <ul>
          <li>例如：學生有多個專長，珠算、程式…</li>
          <li>關係圖中以「雙橢圓形」來表示</li>
        </ul>
      </li>
      <li>單值屬性（Single-valued attribute）：非多值屬性稱為<em>單值屬性</em></li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Multivalued%20attributes.png" alt="Multivalued attributes.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://spaces.isu.edu.tw/upload/19225/0/news/postfile_308.pdf">多值屬性 Multivalued attributes diagram</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>複合屬性 vs 簡單屬性
    <ul>
      <li>複合屬性（Composite attributes）：屬性由數個屬性所組成時，我們稱該屬性為<em>複合屬性</em>
        <ul>
          <li>例如：姓名由姓、名組成</li>
          <li>關係圖如下</li>
        </ul>
      </li>
      <li>簡單屬性（Simple attribute）：非複合屬性則稱為<em>簡單屬性</em></li>
    </ul>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Composite%20attributes.png" alt="Composite attributes.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://spaces.isu.edu.tw/upload/19225/0/news/postfile_308.pdf">複合屬性 Composite attributes diagram</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>衍生屬性（Derived Attribute）
    <ul>
      <li>它的值可以由其它屬性之值經由某種方式的計算或推論而獲得</li>
      <li>例如：年齡和星座可由生日推算出來</li>
    </ul>
  </li>
</ul>

<h2 id="鍵屬性key-attribute">鍵屬性(Key attribute)</h2>

<p><strong>定義</strong></p>
<ul>
  <li>是指該屬性的值在某個環境下具有唯一性（primary key）</li>
</ul>

<p><strong>圖示</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/attribute.png" alt="attribute.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://images.app.goo.gl/auQ9MAQCKttsdFGEA">Attribute diagram</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="關係relationship">關係(Relationship)</h2>

<p><strong>定義</strong></p>
<ul>
  <li>是指用來表達兩個實體之間所隱含的關聯性</li>
  <li>使用足以說明關聯性質的「動詞」或「動詞片語」命名
    <ul>
      <li>例如：『學生』與『系所』兩個實體型態間存在著一種關係─「就讀於」</li>
    </ul>
  </li>
</ul>

<p><strong>分類</strong></p>
<ul>
  <li>關聯強度 Strength
    <ul>
      <li>強關聯（strong relationship）：a relationship where entity is existence-independent of other entities, and PK of Child doesn’t contain PK component of Parent Entity</li>
      <li>弱關聯（weak (identifying) relationship）：a relationship where Child entity is existence-dependent on parent, and PK of Child Entity contains PK component of Parent Entity，也就是說，此關聯是連結<em>實體</em>與<em>弱實體</em></li>
    </ul>
  </li>
  <li>Cardinality
    <ul>
      <li>1:1 一對一</li>
      <li>1:N 一對多</li>
      <li>M:1 多對一</li>
      <li>M:N 多對多</li>
    </ul>
  </li>
  <li>Participation constraints
    <ul>
      <li>全部參與（Total participation）：every entity in the set is involved in the relationship</li>
      <li>部分參與（Partial participation）：not all entities in the set are involved in the relationship</li>
    </ul>
  </li>
</ul>

<p><strong>圖示</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/Relationship.png" alt="Relationship.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://images.app.goo.gl/auQ9MAQCKttsdFGEA">Relationship diagram</a></td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>弱實體例子</strong></p>

<p>為了區分和一般實體的關係, 我們會以雙菱形來表示實體與弱實體之間的關係, 而弱實體和關係之間以雙直線連接</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="http://localhost:4000/blog/img/posts/books.png" alt="books.png" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://www.vertabelo.com/blog/chen-erd-notation/">Example</a></td>
    </tr>
  </tbody>
</table>

<p>以上圖為例，CHAPTER 必須依靠 BOOK 才能存在，所以 CHAPTER 是一個弱實體，兩者間用雙菱形（弱關聯）來連結。CHAPTER 用雙直線（Total participation）連接關係，因為 CHAPTER 是依靠 BOOK 而存在，表示每一個 CHAPTER 必定屬於某本 BOOK。而 CHAPTER 的 primary key 由 BOOK 的 primary key（BOOK ID） 與 CHAPTER ID 組成。</p>

<p>如果將上圖關係轉換成表格，CHAPTER 表格會有 {TITLE, CHAPTER ID, BOOK ID} 三個欄位，而 (CHAPTER ID, BOOK ID) 合併為 primary key。</p>

<hr />

<p><strong>Reference:</strong>
<br />
<a href="http://cc.cust.edu.tw/~ccchen/doc/db_03.pdf">ER Model 實體關係圖</a>
<br />
<a href="http://www.csie.sju.edu.tw/cm/course/db/ch04.pdf">第4章 資料庫設計與實體關聯模型</a>
<br />
<a href="http://spaces.isu.edu.tw/upload/19225/0/news/postfile_308.pdf">利用實體-關係模型 (E-R Model) 規劃資料庫</a>
<br />
<a href="https://www.vertabelo.com/blog/chen-erd-notation/">Chen notation</a>
<br />
<a href="http://snowlin.cmu.edu.tw/mis/MIS_Lec01.pdf">資料庫基礎概念</a></p>
 -->
    
    <div class="post-excerpt"><h2 id="實體關聯模型entity-relationship-model">實體關聯模型（Entity-Relationship Model）</h2>
</div>

    <div class="bookmark">
      <!-- <i class="far fa-comments"></i> -->
      <i class="fas fa-comments"></i>
      <span>
        


<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
var disqus_config = function () {
        // this.page.url = '/blog///2020/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96.html#disqus_thread';
        this.page.identifier = '/2020/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96.html';
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mcshihs.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



        <a href="/blog///2020/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96.html#disqus_thread" data-disqus-identifier="/2020/01/Database-ER-Model%E5%AF%A6%E9%AB%94%E9%97%9C%E4%BF%82%E5%9C%96.html">0 Comments</a>
      </span>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page5">Older</a>
  
  
    <!--  -->
      <a class="pagination-item newer" href="/blog/page3">Newer</a>
    <!--  -->
  
</div>

    </div>
    <script id="dsq-count-scr" src="//mcshihs.disqus.com/count.js" async></script>
  </body>
</html>
