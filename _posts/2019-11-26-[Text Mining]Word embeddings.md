---
layout: post
comments: true
title: "[Text Mining]Word embeddings"
date: 2019-11-26 19:40
author: "Shihs"
category: [Text Mining]
---


This post is based on the [732A92 Texting Mining](https://www.ida.liu.se/~732A92/index.en.shtml) course, given by Marco Kuhlmann at LiU in 2019.

***

## Word embeddings

å°æ–¼äººä¾†èªªï¼Œè¦ç†è§£æ–‡å­—ä¸¦ä¸æ˜¯ä»¶å›°é›£çš„äº‹ï¼Œä½†å°é›»è…¦ä¾†èªªï¼Œæ¯å€‹å­—ä¸éæ˜¯ä¸€ä¸² stringï¼Œæ‰€ä»¥ç•¶æˆ‘å€‘è¦åš text mining æ™‚å°±å¿…é ˆè¦å°‡é€™äº› string è½‰åŒ–æˆé›»è…¦å¯ä»¥ç†è§£çš„æ–¹å¼ã€‚
è€Œ word embedding(word vector) çš„æ¦‚å¿µå°±æ˜¯å°‡æ–‡å­—è½‰æ›æˆ vector ï¼Œå¥½è®“é›»è…¦å¯ä»¥è®€æ‡‚æ–‡å­—é–“çš„é—œä¿‚ã€‚

è­¬å¦‚èªªï¼Œäººé¡å¯ä»¥ç†è§£ pretty å’Œ beautiful æ˜¯ç›¸è¿‘è©ï¼Œä½†å¦‚æœåªæ˜¯çµ¦é›»è…¦é€™å…©å€‹å–®å­—ï¼Œå°æ–¼é›»è…¦ä¾†èªªï¼Œé€™åªæ˜¯å…©å€‹ä¸åŒé•·åº¦çš„ string ç½·äº†ã€‚word embedding æœƒå°‡é€™å…©å€‹å­—è½‰æ›æˆä¸åŒçš„ vector æ˜ å°„åˆ°ä¸€å€‹é«˜ç¶­ç©ºé–“ï¼Œç•¶é€™å…©å€‹ vector è¶Šæ¥è¿‘ï¼ˆå¯ä»¥ä½¿ç”¨ consine similarityï¼‰å°±è¡¨ç¤ºé€™å…©å€‹è©è¶Šç›¸è¿‘ã€‚é€™å°±æ˜¯ word embedding ä¸»è¦çš„æ¦‚å¿µã€‚

- A word embedding is a mapping of words to points in a vector space such that nearby words (points) are similar in terms of their distributional properties.

### The distributional principle

word embedding æ–¹æ³•å¯ä»¥ä½¿ç”¨æœ€é‡è¦çš„å°±æ˜¯å› ç‚ºæœ‰ distributional hypothesis é€™å€‹å‡è¨­ã€‚

- The distributional principle states that words that occur in similar contexts tend to have similar meanings.

é€™è£¡çš„æ¦‚å¿µæ˜¯èªªï¼Œè©(target words)å‡ºç¾åœ¨é¡ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­(context words)ï¼Œå‰‡å®ƒå€‘å¾ˆæœ‰å¯èƒ½æœ‰ç›¸ä¼¼çš„æ„æ€ã€‚
è­¬å¦‚èªªï¼Œ
<br>
-ã€Œé‚£éš»ã€è²“ã€å¥½å¯æ„›ã€
<br>
-ã€Œé‚£éš»ã€ç‹—ã€å¥½å¯æ„›ã€
<br>
é€™æ™‚å€™é™¤äº†ã€è²“ã€å’Œã€ç‹—ã€å¤–ï¼Œé€™å…©å¥è©±çš„ä¸Šä¸‹æ–‡æ˜¯ä¸€æ¨£çš„ï¼Œæ ¹æ“š distributional principleï¼Œé€™å…©å€‹è©æ‡‰è©²æ˜¯ç›¸ä¼¼çš„ã€‚




### Co-occurrence matrix

![]({{ "/img/posts/Co-occurrence matrix.png" |absolute_url}})

ä¸Šåœ–ä¸­ï¼Œcontext words å°±æ˜¯ä¸Šä¸‹æ–‡ï¼Œè€Œ target words å°±æ˜¯æˆ‘å€‘æƒ³è¦åˆ†æçš„å­—è©ã€‚
<br>
ç¾åœ¨ä¾†çœ‹ cheese é€™å€‹å­—ï¼Œå¯ä»¥çœ‹åˆ°å’Œ butter, cake, cow, deer é€™å¹¾å€‹å­—ä¸€èµ·å‡ºç¾çš„æ¬¡æ•¸åˆ†åˆ¥æ˜¯ï¼Œ12, 2, 1å’Œ0æ¬¡ã€‚çœ‹èµ·ä¾†å’Œ butter é‚„æœ‰ cake é€£çµæ€§æ¯”è¼ƒå¼·ã€‚
å†ä¾†çœ‹ bread é€™å€‹å­—ï¼ŒåŒæ¨£çš„åœ¨ butter å’Œ cake ä¸Šçš„é€£çµä¹Ÿæ¯”è¼ƒå¼·ã€‚å¦‚æœæˆ‘å€‘æŠŠé€™å…©å€‹å–®å­—ç”¨å‘é‡è¡¨ç¤ºå°±æœƒæ˜¯ï¼Œ(12, 2, 1, 0) å’Œ (5, 5, 0, 0)ï¼Œå¯ä»¥å»æ¯”è¼ƒå’Œå…¶ä»–å…©å€‹å–®å­—çš„ cosine similarityï¼Œé€™å…©å€‹çš„é—œä¿‚æ˜¯æ¯”è¼ƒå¼·çš„ã€‚

ç¾åœ¨æˆ‘å€‘æŠŠå®ƒå€‘ç•«å‡ºä¾†ï¼ˆå› ç‚ºå®ƒå€‘è¢«æ˜ å°„åˆ°çš„æ˜¯å››ç¶­ç©ºé–“ï¼Œæ‰€ä»¥è€å¸«çš„ slide åªçœ‹ cow å’Œ cake é€™å…©å€‹ context wordsï¼‰

![]({{ "/img/posts/From co-occurrences to word vectors.png" |absolute_url}})


å¦‚æœé‚„æ˜¯ä¸å¤ªæ˜ç™½ï¼Œ[é€™ç¯‡éƒ¨è½æ ¼](http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics)æ‡‰è©²å¯ä»¥çœ‹æ‡‚ã€‚

**è£œå……ï¼š**
å¾ä¸Šé¢çš„ matrix å¯èƒ½æœƒæƒ³åˆ°ä¸€ä»¶äº‹ï¼Œ*context words* ç­‰æ–¼æ˜¯æ±ºå®šäº† target words çš„å‘é‡å•Šï¼
æ›å¥è©±èªªï¼Œç•¶æˆ‘å€‘åˆ†æä¸åŒçš„æ–‡æœ¬çš„æ™‚å€™ï¼Œæœƒéœ€è¦ä¸åŒçš„ context wordsä¾†ç®—å‡º word embeddingsã€‚æƒ³åƒï¼Œå¦‚æœä»Šå¤©è¦åˆ†ææ–°èå ±å°å’Œ ptt å…§å®¹ï¼Œä¸å¤ªå¯èƒ½ä½¿ç”¨ä¸€æ¨£çš„ context wordsï¼Œç•¢ç«Ÿpttç”¨èªå’Œæ–°èç”¨èªæœƒæœ‰å¾ˆå¤§çš„ä¸åŒã€‚ï¼ˆè›¤ï¼Ÿä½ èªªè¨˜è€…éƒ½æŠ„ ptt å…§å®¹å—ï¼ŸXDDDï¼‰

***

### Simple applications of word embeddings

word embeddings çš„æ‡‰ç”¨

- finding similar words. æ‰¾åˆ°ç›¸ä¼¼çš„å­—ï¼Œåƒæ˜¯ä¸Šé¢çš„ä¾‹å­ï¼Œæ‰¾å‡ºå“ªä¸€å€‹å­—å’Œ cheese æ¯”è¼ƒç›¸ä¼¼ã€‚
- answering â€˜odd one outâ€™ questions. æ‰¾å‡ºä¸ä¸€æ¨£çš„è©ï¼Œè­¬å¦‚èªª  lunch, breakfast, dinner, car å“ªä¸€å€‹è©å±¬æ–¼ä¸åŒé¡ï¼Ÿ (æ ¹æ“šä¸Šé¢æåˆ°çš„æ¦‚å¿µï¼Œlunch, breakfast, dinner é€™ä¸‰å€‹çš„ vector æ‡‰è©²æœƒæ¯”è¼ƒæ¥è¿‘ï¼Œæœƒåœ¨æ¯”è¼ƒæ¥è¿‘çš„ä¸Šä¸‹æ–‡ä¸­å‡ºç¾)


### Limitations of word embeddings

- There are many different facets of â€˜similarityâ€™. Ex. Is a cat more similar to a dog or to a tiger? (åœ¨ä¸åŒæƒ…å¢ƒä¸‹ï¼Œcat å’Œ dog å¯èƒ½æ¯”è¼ƒç›¸ä¼¼ã€‚è­¬å¦‚èªªï¼Œè²“å’Œç‹—éƒ½æ˜¯å¯µç‰©ï¼Œä½†å¦‚æœä»¥ç”Ÿç‰©çš„è§’åº¦ä¾†çœ‹ï¼Œcat å’Œ tiger éƒ½å±¬æ–¼è²“ç§‘å‹•ç‰©ï¼Œé€™æ™‚å€™ cat å’Œ tiger æœƒæ¯”è¼ƒç›¸ä¼¼)

- Text data does not reflect many â€˜trivialâ€™ properties of words. Ex. more â€˜black sheepâ€™ than â€˜white sheepâ€™ (å¦‚æœåªåˆ†ææ–‡æœ¬ï¼Œå› ç‚ºå¤§éƒ¨åˆ†çš„ç¾Šéƒ½æ˜¯ç™½è‰²çš„ï¼Œæ‰€ä»¥åœ¨æåˆ°ç¾Šçš„æ™‚å€™ä¸¦ä¸æœƒç‰¹åˆ¥æåˆ°é¡è‰²ï¼Œä½†ç•¶æåˆ°æ¯”è¼ƒç¨€å°‘çš„é»‘ç¾Šæ™‚ï¼Œåè€Œæœƒç‰¹åˆ¥èªªåˆ° blackï¼Œé€™æœƒå°è‡´åœ¨åˆ†ææ™‚å¥½åƒé»‘ç¾Šå‡ºç¾çš„é »ç‡æ¯”ç™½ç¾Šå‡ºç¾çš„é »ç‡é«˜)

- Word vectors reflect social biases in the data used to train them. Ex. including gender and ethnic stereotypes ([è«–æ–‡](https://www.pnas.org/content/115/16/E3635)åƒè€ƒ) å¾ˆå¤šè©èªä¸Šçš„ç”¨æ³•å…¶å¯¦å¸¶æœ‰éå¸¸å¤šçš„ç¤¾æœƒåè¦‹å’Œåˆ»æ¿å°è±¡ï¼Œè€Œé€™ä¹Ÿæœƒå°è‡´åˆ†æå‡ºçš„çµæœæœ‰æ‰€åå·®ã€‚


***

**é‚„æœ‰ä»€éº¼å•é¡Œï¼Ÿ**

åˆ°ç›®å‰ç‚ºæ­¢ï¼Œçœ‹èµ·ä¾†éƒ½éå¸¸åˆç†ï¼Œé‚£é‚„æœƒæœ‰ä»€éº¼å•é¡Œå‘¢ï¼Ÿ

é€™è£¡æœƒç¢°åˆ°å’Œä¹‹å‰æåˆ°éçš„ï¼ŒçŸ©é™£*ç¨€ç–æ€§*çš„å•é¡Œã€‚å¦‚æœä»Šå¤© context words æœ‰åè¬å€‹å­—ï¼Œé‚£éº¼ target words å°±æœƒæ˜¯åœ¨åè¬ç¶­åº¦çš„ç©ºé–“çš„ vectorsï¼Œè€Œä¸”å¯èƒ½æœƒæœ‰å¾ˆå¤šçš„å€¼éƒ½æ˜¯ 0 çš„ç‹€æ³ç™¼ç”Ÿã€‚é‚£é€™æ¨£è¦ç”¨ä»€éº¼æ–¹æ³•è§£æ±ºçŸ©é™£çš„ç¨€ç–æ€§ä¸¦ç”¢ç”Ÿ word embeddingsï¼ˆä¹Ÿå°±æ˜¯æ¯å€‹è©çš„å‘é‡ï¼‰ å‘¢ï¼Ÿ

å¾ä¸åŒçš„é¢å‘ä¾†çœ‹å¹¾å€‹å¸¸è¦‹çš„ word embedding æ–¹æ³•ï¼Œ
- Learning word embeddings via *matrix factorization*
1. Singular Value Decomposition(SVD)
2. Positive Pointwise mutual information(PPMI)
- Learning word embeddings via *language models*
1. N-gram
2. Neural language models(Ex. word2vec)

ä»¥ä¸‹å°±è¦ä¾†ä»‹ç´¹é€™å¹¾ç¨®æ–¹æ³•ã€‚

***

## Singular Value Decomposition(SVD)

- The rows of co-occurrence matrices are long and sparse. Instead, we would like to have word vectors that are short and dense. ç°¡å–®ä¾†èªªï¼Œco-occurrence matrices æœƒæœ‰ç¨€ç–æ€§çš„å•é¡Œã€‚ 
- One idea is to approximate the co-occurrence matrix by another matrix with fewer columns. Singular Value Decomposition çš„æƒ³æ³•æ˜¯ï¼Œå°‡é€™å€‹åˆé•·åˆè‡­çš„ co-occurrence matrix ç”¨å¦æ¯”è¼ƒå°‘ columns çš„ matrix å–ä»£ã€‚


### ä»€éº¼æ˜¯ Singular value decompositionï¼ˆå¥‡ç•°å€¼åˆ†è§£ï¼‰?

æ¨è–¦[æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸](https://www.youtube.com/watch?v=OEJ0wxxLO7M)
- *Singular value decomposition(SVD)* can be applied on any matrix. (ä¸éœ€è¦æ˜¯æ–¹é™£ã€‚æ¯”è¼ƒï¼šPCA ä¹Ÿæ˜¯ä¸€å€‹å¯é™ç¶­çš„æ–¹æ³•ï¼Œä½†å®ƒçš„çŸ©é™£å°±å¿…é ˆè¦æ˜¯æ–¹é™£ã€‚)

SVD çš„æ¦‚å¿µå°±æ˜¯ï¼Œä»»ä¸€ä¸€å€‹çŸ©é™£ $$A_{m \times n}$$ï¼Œå®ƒéƒ½å¯ä»¥æ‹†è§£æˆä¸‰å€‹çŸ©é™£ï¼ˆ$$U_{m \times n}, \Sigma_{m \times n}, V^T_{n \times n}$$ï¼‰çš„ç›¸ä¹˜ã€‚

å…¶ä¸­ï¼Œ$$U_{m \times n}$$ çš„ columns æ˜¯ *Orthonormal*ï¼Œè€Œ $$V^T_{n \times n}$$ çš„ rows æ˜¯ *Orthonormal*ï¼Œ$$\Sigma_{m \times n}$$ æ˜¯ *Diagonal*(åªæœ‰å°è§’ç·šæœ‰éè² çš„å€¼ï¼Œä¸”ç”±å¤§åˆ°å°)ã€‚

![]({{ "/img/posts/SVD.png" |absolute_url}})

- $$\Sigma_{m \times n}$$ æœƒæ˜¯ä¸€å€‹é•·å¾—åƒé€™æ¨£çš„çŸ©é™£ï¼Œä¸” $$\sigma_1 \ge \sigma_1 \ge \ldots \ge \sigma_k$$

![]({{ "/img/posts/Sigma.png" |absolute_url}})

è€Œ $$\sigma_r, ~~where~~1 \le r \le k$$ æ˜¯å¥‡ç•°å€¼ï¼ˆsingular valueï¼‰ï¼Œè€Œ r è¶Šå°ä¹Ÿä»£è¡¨äº†è©²å€¼è¶Šé‡è¦ï¼Œæ›å¥è©±èªªï¼Œå«æœ‰è¶Šå¤šè¨Šæ¯ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥åªä¿ç•™ $$\Sigma$$ è¼ƒé‡è¦çš„å‰é¢å¹¾è¡Œå¾—åˆ°ä¸€å€‹ç›¸ä¼¼çš„çŸ©é™£ $$A$$ã€‚

$$A_{m \times n} = U_{m \times r} \times \Sigma_{r \times r} \times V^T_{r \times n} $$

åƒè€ƒ[ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)](https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/)çš„åœ–ï¼Œ

![]({{ "/img/posts/svd2.jpg" |absolute_url}})


å›åˆ°æˆ‘å€‘çš„ word-embeddingã€‚ä¹Ÿå°±æ˜¯èªªï¼Œä½¿ç”¨ SVD å¯ä»¥åˆ©ç”¨æ¸›å°‘ $$\Sigma$$ çš„ç¶­åº¦ä¾†è™•ç†ç¨€ç–æ€§çš„å•é¡Œï¼Œé›–ç„¶åˆªé™¤äº†ä¸€äº›è©ä»èˆŠä¿ç•™é‡è¦çš„è©ã€‚

- Each row of the (truncated) matrix ğ‘¼ is a k-dimensional vector that represents the â€˜most importantâ€™ information about a word.
- A practical problem is that computing the singular value decomposition for large matrices is expensive.

***

## Positive Pointwise mutual information(PPMI)

### Pointwise mutual information(PMI)

- Raw counts favour pairs that involve very common contexts. Ex.the cat, a cat will receive higher weight than cute cat, small cat.
- We want a measure that favours contexts in which the target word occurs more often than other words.
- A suitable measure is pointwise mutual information (PMI):

$$PMI(x, y) = log \frac{P(x, y)}{P(x) \times P(y)}$$

ç°¡å–®ä¾†èªªï¼Œæˆ‘å€‘å¯ä»¥ç”¨ PMI å…¬å¼ä¾†çœ‹å…©å€‹å­—ä¹‹é–“çš„é—œä¿‚ã€‚


ç¾åœ¨æˆ‘å€‘æŠŠ $$x$$ çœ‹æˆæˆ‘å€‘çš„ target wordï¼Œ$$y$$ çœ‹æˆæˆ‘å€‘çš„ context wordï¼Œ

- We want to use PMI to measure the associative strength between a word $$w$$ and a context $$c$$ in a data set $$D$$:

$$PMI(w, c) = log \frac{P(w, c)}{P(w) \times P(c)} = log \frac{\#(w, c)/|D|}{\#(w)/|D| \cdot \#(c)/|D|} = log \frac{\#(w,c) \cdot |D|}{\#(w) \cdot \#(c)}$$


ä½†æ ¹æ“šä¸Šé¢çš„å…¬å¼ï¼Œæœƒç™¼ç¾ä¸€å€‹å•é¡Œï¼ŒPMI is infinitely small for unseen wordâ€“context pairs, and undefined for unseen target words. (å¦‚æœ $$w$$ å’Œ $$c$$ ä¸¦æ²’æœ‰å…±åŒå‡ºç¾éï¼Œå†å– logï¼Œæ•´å€‹å€¼æœƒè®Šæˆ -Inf)

æ‰€ä»¥é€™æ™‚å€™å°±æœ‰äº† **Positive Pointwise mutual information(PPMI)**ã€‚

- In positive pointwise mutual information (PPMI), all negative and undefined values are replaced by zero:

$$ô°œô°œô°ô°ô°ô°‚ô° PPMI(w, c) = max(PMI(w, c), 0)$$

- PPMI assigns high values to rare events, it is advisable to apply a count threshold or smooth the probabilities.


![]({{ "/img/posts/Computing PPMI on a wordâ€“context matrix.png" |absolute_url}})

***

## Language models

- A *probabilistic language model* is a probability distribution over sequences of words in some language.
- Recent years have seen the rise of *neural language models*, which are based on distributed representations of words.
- By the chain rule, the probability of a sequence of ğ‘ words can be computed using conditional probabilities as

$$p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_1 \ldots w_{k-1})$$

- To make probability estimates more robust, we can approximate the full history $$w_1 \ldots w_N$$ by the last few words(é¦¬å¯å¤«éŠ):

$$p(w_1, w_2,\ldots, w_N) = \prod_{k=1}^N P(w_k|w_{k-n+1} \ldots w_{k-1})$$


## N-gram models

- An n-gram is a contiguous sequence of n words or characters. Ex. unigram (Text), bigram (Text Mining), trigram (Text Mining course)
- An n-gram model is a language model defined on n-grams â€“ â€¨a probability distribution over sequences of n words.
- n-gram æ˜¯ä¸€ç¨®èªè¨€æ©Ÿç‡æ¨¡å‹ã€‚ä¸€å¥è©±å‡ºç¾çš„æ©Ÿç‡æ˜¯ä¸€å€‹è¯åˆæ¨¡å‹ã€‚å¦‚æœä¸€å€‹è©çš„å‡ºç¾åªè€ƒæ…®å‰é¢ä¸€å€‹å­—ï¼Œé‚£å°±æ˜¯ bi-gramï¼›å¦‚æœä¸€å€‹è©çš„å‡ºç¾è€ƒæ…®å‰é¢å…©å€‹å­—ï¼Œé‚£å°±æ˜¯ tri-gramã€‚


**Formal definition of an n-gram model**

- $$n$$: the model's order (1 = unigram, 2 = bigram, ...)
- $$V$$: a set of possible words (character); the vocabulary
- $$P(w\mid u)$$: a probability that specifies how likely it is to observe â€¨the word $$w$$ after the context 
<br>(n âˆ’ 1)-gram $$u$$

***

**Unigram model**

*n = 1* ä¸è€ƒæ…®å‰é¢å‡ºç¾çš„å­—ã€‚
![]({{ "/img/posts/Unigram model.png" |absolute_url}})

Thus contexts are empty.

**MLE of unigram probabilities**

![]({{ "/img/posts/MLE of unigram probabilities.png" |absolute_url}})

***

**Bigram models**

*n = 2* è€ƒæ…®å‰é¢å‡ºç¾çš„ä¸€å€‹å­—ã€‚

![]({{ "/img/posts/Bigram models.png" |absolute_url}})

Thus contexts are unigrams.

**Estimating bigram probabilities**

![]({{ "/img/posts/Estimating bigram probabilities.png" |absolute_url}})


**Example**
([source](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf))
![]({{ "/img/posts/bigram example.png" |absolute_url}})

***

**Smoothing**


ç•¶åœ¨è¨ˆç®— bigram æ™‚å¯èƒ½æœƒç¢°åˆ°å…©å€‹å­—å®Œå…¨æ²’æœ‰ç›¸é„°çš„ç‹€æ³ï¼Œé€™æœƒå°è‡´ç®—å‡ºä¾†çš„æ©Ÿç‡ç­‰æ–¼ 0ã€‚ï¼ˆå¦‚ä¸‹åœ–ï¼‰

![]({{ "/img/posts/bigram smoothing.png" |absolute_url}})


é€™ç¨®æ™‚å€™å°±éœ€è¦ç”¨åˆ°ï¼Œsmoothingã€‚

**Smoothing methods**
- Additive smoothing
- Good-Turing estimate
- Jelinek-Mercer smoothing (interpolation) â€¢ Katz smoothing (backoff)
- Witten-Bell smoothing
- Absolute discounting
- Kneser-Ney smoothing

***

ä¸Šé¢çš„ç‹€æ³ç¢°åˆ°çš„æ˜¯ï¼Œ"CHER" å¾Œé¢æ²’æœ‰å‡ºç¾ "READ" çš„ç‹€æ³ï¼Œè€Œå°è‡´æ©Ÿç‡ç­‰æ–¼0ï¼Œä½†å¦‚æœç¾åœ¨æ˜¯ "CHER" é€™å€‹å­—å¾æœªå‡ºç¾åœ¨è³‡æ–™é›†ä¸­å‘¢ï¼Ÿé€™ç¨®ç‹€æ³æ™‚ï¼Œsmoothing ä¾¿æ´¾ä¸ä¸Šç”¨å ´äº†ã€‚

- A simple way to deal with this is to introduce a special word type UNK, and to smooth it like any other word type in the vocabulary.
- When we compute the probability of a document, then we first replace every unknown word with UNK.

***

## Neural networks as language models

### Advantages of neural language models
- Neural models can achieve better perplexity than probabilistic models, and scale to much larger values of n.
- Words in different positions share parameters, making them share statistical strength. (Everything must pass through the hidden layer.)
- The network can learn that in some contexts, only parts of the â€¨n-gram are informative. (implicit smoothing, helps with unknown words)

### word2vec






***

**Reference:**
<br>
[732A92 Texting Mining](https://www.ida.liu.se/~732A92/index.en.shtml)
<br>
[è©å‘é‡ä»‹ç´¹](https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1)
<br>
[è‡ªç„¶èªè¨€è™•ç† -- Vector Space of Semantics](http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics)
<br>
[[NLP] ç§’æ‡‚è¯å‘é‡Word2vecçš„æœ¬è´¨](https://zhuanlan.zhihu.com/p/26306795)
<br>
[æå®æ¯…è€å¸«çš„ç·šæ€§ä»£æ•¸ - SVD](https://www.youtube.com/watch?v=OEJ0wxxLO7M)
<br>
[NLP ç¬”è®° - å†è°ˆè¯å‘é‡](http://www.shuang0420.com/2017/03/21/NLP%20ç¬”è®°%20-%20å†è°ˆè¯å‘é‡/)
<br>
[æ©Ÿå™¨å­¸ç¿’ç­†è¨˜ä¹‹äºŒåäºŒâ€”â€”PCAèˆ‡SVD](https://www.twblogs.net/a/5c7bdd70bd9eee339918b4da)
<br>
[ç·šä»£å•Ÿç¤ºéŒ„-å¥‡ç•°å€¼åˆ†è§£ (SVD)](https://ccjou.wordpress.com/2009/09/01/å¥‡ç•°å€¼åˆ†è§£-svd/)
<br>
[è‡ªç„¶èªè¨€è™•ç† -- Pointwise Mutual Information](http://cpmarkchang.logdown.com/posts/195584-natural-language-processing-pointwise-mutual-information)
<br>
[NLP Lunch Tutorial: Smoothing](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)
<br>
[æ©Ÿå™¨å­¸ç¿’äº”åˆ†é˜ï¼šè‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰çš„N-gramæ¨¡å‹æ˜¯ä»€éº¼ï¼Ÿ](https://kknews.cc/tech/83yx3qn.html)